{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ea0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c89e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense , Activation , Dropout, Embedding, SpatialDropout1D, LSTM , TextVectorization, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb1349c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50137a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'Corpus\\\\corpus_morphological_analysis'\n",
    "file_paths = []\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    file_paths.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c5f5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file_paths = file_paths[:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2602e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "#i = 0\n",
    "for filepath in temp_file_paths :\n",
    "    #print(i)\n",
    "    with open(filepath, encoding='utf-8') as f :\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()  \n",
    "    text = soup.get_text()\n",
    "    content.append(text)\n",
    "    #i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "104b5ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35a75b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_list = []\n",
    "for item in content : \n",
    "    tmp = item.splitlines()\n",
    "    split_list.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3544ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_list = []\n",
    "for k in split_list :\n",
    "    l = [item for item in k if 'لا توجد نتائج لتحليل هذه الكلمة' not in item]\n",
    "    tmp_l = [item.replace(\"#\",'') for item in l]\n",
    "    work_list.append(tmp_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "429730b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_list = []\n",
    "for k in work_list :\n",
    "    tst = [item.split(':') for item in k]\n",
    "    final_list.append(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9135fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifie le prefixe, racine et le suffixe et les placent dans un dictionnaire\n",
    "def identify(word_l):\n",
    "    dictt = {}\n",
    "    dictt['word'] = word_l[1]\n",
    "    # le cas s'il existe un préfixe\n",
    "    if word_l[2] != '' and word_l[2] != ' ' :   \n",
    "        if word_l[4] not in word_l[0]: \n",
    "            if word_l[5] in word_l[0] and word_l[5] != '': \n",
    "                dictt['prefixe'] = word_l[2]\n",
    "                dictt['root'] = word_l[8]\n",
    "                dictt['suffixe'] = word_l[9]\n",
    "            elif word_l[3] in word_l[0] and word_l[3] != '':\n",
    "                dictt['prefixe'] = word_l[2]\n",
    "                dictt['root'] = word_l[3]\n",
    "                dictt['suffixe'] = ''\n",
    "        else :\n",
    "            dictt['prefixe'] = word_l[2]\n",
    "            dictt['root'] = word_l[7]\n",
    "            dictt['suffixe'] = word_l[8]\n",
    "    # s'il n'existe pas un préfixe\n",
    "    else : \n",
    "        if word_l[2] == '' : \n",
    "            dictt['prefixe'] = word_l[2]\n",
    "            dictt['root'] = word_l[6]\n",
    "            dictt['suffixe'] = word_l[7]\n",
    "        elif  word_l[2] == ' ' :\n",
    "            dictt['prefixe'] = ''\n",
    "            dictt['root'] = word_l[3]\n",
    "            dictt['suffixe'] = ''    \n",
    "    return dictt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69a52da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtre la liste de mots en liste de dictionnaires\n",
    "def word_to_dict_list(wordlist):\n",
    "    dictlist = []\n",
    "    for k in wordlist : \n",
    "        dictlist.append(identify(k))\n",
    "    return dictlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "897b24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for k in final_list: \n",
    "    for j in k :\n",
    "        final.append(identify(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cce4d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'الْجِمْعَةَ', 'prefixe': 'ال', 'root': 'جمع', 'suffixe': 'ة'}\n"
     ]
    }
   ],
   "source": [
    "print(final[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0fe335f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dic_to_list(listt):\n",
    "    L = []\n",
    "    for k in listt : \n",
    "        tmp = []\n",
    "        #print(k)\n",
    "        if len(k) == 4 : \n",
    "            tmp.append(k['word'])\n",
    "            tmp.append(k['prefixe'])\n",
    "            tmp.append(k['root'])\n",
    "            tmp.append(k['suffixe'])\n",
    "            L.append(tmp)\n",
    "    return L\n",
    "data = dic_to_list(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f81f4b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['كَتِّبْ', '', 'كتب', '']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_l = dic_to_list(final)\n",
    "final_l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22118e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimenting with our first neural network to predict the root of a word.\n",
    "\n",
    "data_1 = []\n",
    "\n",
    "for word in final_l:\n",
    "    tmp = []\n",
    "    word[0] = word[0].replace('+', ' ')\n",
    "    word[2] = word[2].replace('+', ' ')\n",
    "    tmp.append(word[0])\n",
    "    tmp.append(word[2])\n",
    "    data_1.append(tmp)\n",
    "\n",
    "data_1 = np.array(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f8ebb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we extract our dictionnary from the our dataset \n",
    "def extract_dict(listt) :\n",
    "    dictt = []\n",
    "    for word in listt :\n",
    "        for item in word : \n",
    "            tmp = set(item)\n",
    "            for k in tmp : \n",
    "                if k not in dictt : \n",
    "                    dictt.append(k)\n",
    "    return dictt       \n",
    "dic = extract_dict(data_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c27d12",
   "metadata": {},
   "source": [
    "### Let's create a data structure for each word of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ef987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = work_list[2]\n",
    "tmp.split(\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b25293",
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel_pattern = r'[\\u064b\\u064e\\u064f\\u0650\\u0651\\u0652]'\n",
    "word = \"مَرْحَبًا\"\n",
    "vowels = re.findall(vowel_pattern, word)\n",
    "print(f\"Vowels in '{word}': {vowels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f63a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''filtered_split_list = []\n",
    "for item in split_list : \n",
    "    temp_item_list = []\n",
    "    for subitem in item : \n",
    "        if 'لا توجد نتائج لتحليل هذه الكلمة' not in item : \n",
    "            temp_item_list.append(item)\n",
    "    filtered_split_list.append(temp_item_list)\n",
    "filtered_split_list[0]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a7be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Check the data type of each element in texts\n",
    "texts = pd.DataFrame(data_1[:, -1])\n",
    "texts = texts.iloc[:, 1:]\n",
    "for i, text in enumerate(texts):\n",
    "    if not isinstance(text, str):\n",
    "        print(f\"Element {i} is not a string: {text}\")'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
