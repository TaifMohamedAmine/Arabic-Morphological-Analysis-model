{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ea0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89e9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f7603ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb1349c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50137a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'corpus_morphological_analysis'\n",
    "file_paths = []\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    file_paths.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c5f5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file_paths = file_paths[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2602e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "#i = 0\n",
    "for filepath in temp_file_paths :\n",
    "    #print(i)\n",
    "    with open(filepath, encoding='utf-8') as f :\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()  \n",
    "    text = soup.get_text()\n",
    "    content.append(text)\n",
    "    #i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d7aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b5ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a75b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_list = []\n",
    "for item in content : \n",
    "    tmp = item.splitlines()\n",
    "    split_list.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3544ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_list = []\n",
    "for k in split_list :\n",
    "    l = [item for item in k if 'لا توجد نتائج لتحليل هذه الكلمة' not in item]\n",
    "    tmp_l = [item.replace(\"#\",'') for item in l]\n",
    "    work_list.append(tmp_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "429730b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_list = []\n",
    "for k in work_list :\n",
    "    tst = [item.split(':') for item in k]\n",
    "    final_list.append(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9135fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifie le prefixe, racine et le suffixe et les placent dans un dictionnaire\n",
    "def identify(word_l):\n",
    "    if len(word_l) < 4 :\n",
    "        return None\n",
    "    dictt = {}\n",
    "    dictt['word'] = word_l[0]\n",
    "    # le cas s'il existe un préfixe\n",
    "    if word_l[2] != '' and word_l[2] != ' ' :   \n",
    "        if word_l[4] not in word_l[0]: \n",
    "            if word_l[5] in word_l[0] and word_l[5] != '': \n",
    "                dictt['prefixe'] = word_l[2]\n",
    "                dictt['root'] = word_l[8]\n",
    "                dictt['suffixe'] = word_l[9]\n",
    "            elif word_l[3] in word_l[0] and word_l[3] != '':\n",
    "                dictt['prefixe'] = word_l[2]\n",
    "                dictt['root'] = word_l[3]\n",
    "                dictt['suffixe'] = ''\n",
    "        else :\n",
    "            dictt['prefixe'] = word_l[2]\n",
    "            dictt['root'] = word_l[7]\n",
    "            dictt['suffixe'] = word_l[8]\n",
    "    # s'il n'existe pas un préfixe\n",
    "    else : \n",
    "        if word_l[2] == '' : \n",
    "            dictt['prefixe'] = word_l[2]\n",
    "            dictt['root'] = word_l[6]\n",
    "            dictt['suffixe'] = word_l[7]\n",
    "        elif  word_l[2] == ' ' :\n",
    "            dictt['prefixe'] = ''\n",
    "            dictt['root'] = word_l[3]\n",
    "            dictt['suffixe'] = ''    \n",
    "    return dictt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807dc08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69a52da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtre la liste de mots en liste de dictionnaires\n",
    "def word_to_dict_list(wordlist):\n",
    "    dictlist = []\n",
    "    for k in wordlist : \n",
    "        dictlist.append(identify(k))\n",
    "    return dictlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "897b24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for k in final_list: \n",
    "    for j in k :\n",
    "        s = identify(j)\n",
    "        if s == None :\n",
    "            continue\n",
    "        final.append(identify(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce4d9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0fe335f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dic_to_list(listt):\n",
    "    L = []\n",
    "    for k in listt : \n",
    "        tmp = []\n",
    "        #print(k)\n",
    "        if len(k) == 4 : \n",
    "            tmp.append(k['word'])\n",
    "            tmp.append(k['prefixe'])\n",
    "            tmp.append(k['root'])\n",
    "            tmp.append(k['suffixe'])\n",
    "            L.append(tmp)\n",
    "    return L\n",
    "data = dic_to_list(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad205283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f81f4b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['يقدمها', 'ي', 'قدم', 'هَا']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_l = dic_to_list(final)\n",
    "final_l[5990]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22118e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimenting with our first neural network to predict the root of a word.\n",
    "\n",
    "data_1 = []\n",
    "\n",
    "for word in final_l:\n",
    "    tmp = []\n",
    "    word[0] = word[0].replace('+', ' ')\n",
    "    word[2] = word[2].replace('+', ' ')\n",
    "    tmp.append(word[0])\n",
    "    tmp.append(word[2])\n",
    "    data_1.append(tmp)\n",
    "    #print(tmp)\n",
    "#data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f8ebb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we extract our dictionnary from the our dataset \n",
    "def extract_dict(listt) :\n",
    "    dictt = []\n",
    "    for word in listt :\n",
    "        for item in word : \n",
    "            tmp = set(item)\n",
    "            for k in tmp : \n",
    "                if k not in dictt : \n",
    "                    dictt.append(k)\n",
    "    return dictt       \n",
    "dic = extract_dict(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247cf020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc451ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data = []\n",
    "for word in data : \n",
    "    tmp =[]\n",
    "    tmp.append(word[0])\n",
    "    tmp.append(word[2])\n",
    "    root_data.append(tmp)\n",
    "#root_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51d4a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encode :\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def code_sequence(self):\n",
    "        L = []\n",
    "        for word in self.data : \n",
    "            tmp_string = code_word(word)\n",
    "            L.append(tmp_string)\n",
    "        final_string = '#'.join(map(str,L))\n",
    "        return final_string \n",
    "    \n",
    "    def code_normal_text(self):\n",
    "        L = []\n",
    "        for word in self.data : \n",
    "            L.append(word[1])\n",
    "        final_string = ' '.join(map(str,L))\n",
    "        return final_string\n",
    "        \n",
    "final_str = encode(root_data).code_normal_text()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d08082",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3dbc01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90b091db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = []\n",
    "for item in root_data : \n",
    "    tmp = []\n",
    "    if len(item[1]) <= 3 and len(item[1]) != 0:\n",
    "        tmp.append('$'+item[0]+'£')\n",
    "        tmp.append('$'+item[1]+'£')\n",
    "        data_root.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d85cf4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32e29629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_padding(dat, padding_char):\n",
    "    #Le'ts create a padding character : \n",
    "    pad_char = padding_char\n",
    "    padded_data = []\n",
    "    \n",
    "    ls_words = []\n",
    "    ls_roots = []\n",
    "\n",
    "    for instance in dat : \n",
    "        ls_words.append(instance[0])\n",
    "        ls_roots.append(instance[1])\n",
    "    max_len_words = max([len(item) for item in ls_words])\n",
    "    max_len_roots = max([len(item) for item in ls_roots])\n",
    "    \n",
    "    for instance in dat: \n",
    "        tmp = []\n",
    "        word,root = instance[0], instance[1]\n",
    "        while(len(word) != max_len_words):\n",
    "            word += pad_char\n",
    "        tmp.append(word)\n",
    "        while(len(root) != max_len_roots):\n",
    "            root += pad_char\n",
    "        tmp.append(root)\n",
    "        padded_data.append(tmp)\n",
    "        \n",
    "    return padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11f4f468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['$مسقط£%%%%%%%%%', '$سقط£'],\n",
       " ['$يقام£%%%%%%%%%', '$قمي£'],\n",
       " ['$الرابع£%%%%%%%', '$ربع£'],\n",
       " ['$والعشرين£%%%%%', '$عشر£'],\n",
       " ['$من£%%%%%%%%%%%', '$من£%'],\n",
       " ['$شهر£%%%%%%%%%%', '$شهر£'],\n",
       " ['$فبراير£%%%%%%%', '$رير£'],\n",
       " ['$المقبل£%%%%%%%', '$قبل£'],\n",
       " ['$قصر£%%%%%%%%%%', '$قصر£'],\n",
       " ['$البستان£%%%%%%', '$بسس£']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_padding(data_root,'%')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7246f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    def train_batch(self, batch):\n",
    "                \n",
    "        self.optimizer.zero_grad()\n",
    "        for instance in batch: \n",
    "            '''\n",
    "            word = instance[0] : the word to extract root from\n",
    "            target_word = instance[1] : the root of the word\n",
    "            '''\n",
    "            word = instance[0]\n",
    "            target_root = instance[1]\n",
    "            \n",
    "            res1 = self.encode_word(word)\n",
    "            predicted_root = []\n",
    "            original_root_seq = self.word_to_idx_seq(target_root)\n",
    "            predicted_root_seq = []\n",
    "            res_char = target_root[0]\n",
    "            \n",
    "            i = 0\n",
    "            for char in target_root : \n",
    "                if len(predicted_root) == len(target_root):\n",
    "                    break\n",
    "                \n",
    "                \n",
    "                if random.random() < self.teacher_forcing_ratio and i!=0 : \n",
    "                    res_char = char\n",
    "                    i+= 1\n",
    "                \n",
    "                res_char , hidd , idx = self.decode_word(res1, res_char)\n",
    "                predicted_root_seq.append(idx)\n",
    "                test = hidd\n",
    "                predicted_root.append(res_char)\n",
    "\n",
    "            predicted_root = ''.join(predicted_root)\n",
    "\n",
    "            predicted_root_seq = torch.tensor(predicted_root_seq)\n",
    "\n",
    "            org = self.embedding(original_root_seq)\n",
    "            pred = self.embedding(predicted_root_seq)\n",
    "            \n",
    "            \n",
    "            loss = self.criterion(org, pred)\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            print('target root :', target_root)\n",
    "            print('predicted root : ', predicted_root)\n",
    "\n",
    "            print('the loss : ', loss.item())  \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return\n",
    "    \n",
    "            \n",
    "            \n",
    "        #print(em1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95150815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(self):\n",
    "    \n",
    "    #Le'ts create a padding for ouriinstances : \n",
    "    \n",
    "    pad_char = ''\n",
    "    padded_data = []\n",
    "    ls_words = []\n",
    "    ls_roots = []\n",
    "    for instance in self.data : \n",
    "        ls_words.append(instance[0])\n",
    "        ls_roots.append(instance[1])\n",
    "    max_len_words = max([len(item) for item in ls_words])\n",
    "    max_len_roots = max([len(item) for item in ls_roots])\n",
    "    \n",
    "    for instance in self.data: \n",
    "        tmp = []\n",
    "        word,root = instance[0], instance[1]\n",
    "        while(len(word) != max_len_words):\n",
    "            word += pad_char\n",
    "        tmp.append(word)\n",
    "        while(len(root) != max_len_roots):\n",
    "            root += pad_char\n",
    "        tmp.append(root)\n",
    "        padded_data.append(tmp)\n",
    "    \n",
    "\n",
    "    # let's create our vocab : \n",
    "    \n",
    "    vocab = []\n",
    "    for word in padded_data :\n",
    "        for item in word : \n",
    "            tmp = set(item)\n",
    "            for k in tmp : \n",
    "                if k not in vocab : \n",
    "                    vocab.append(k)\n",
    "    \n",
    "    \n",
    "    # Let's create our dictionnary with unique indexes\n",
    "    \n",
    "    char_to_idx_map = {char: idx for idx, char in enumerate(dictt)}\n",
    "    \n",
    "    # Let's now split our data to batches\n",
    "   \n",
    "    final_data = []\n",
    "    for instance in padded_data : \n",
    "        tmp = []\n",
    "        word = self.word_to_seq(instance[0])\n",
    "        root = self.word_to_seq(instance[1])\n",
    "        tmp.append(word)\n",
    "        tmp.append(root)\n",
    "        final_data.append(tmp)\n",
    "        \n",
    "    size= self.batch_size \n",
    "    batches = [final_data[i:i + size] for i in range(0, len(final_data), size)]\n",
    "    \n",
    "    return batches , vocab , char_to_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d695d5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(item[0]) for item in data_root])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa595ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794906\n",
      "794906\n"
     ]
    }
   ],
   "source": [
    "print(len(data_root))\n",
    "for item in data_root :\n",
    "    if len(item[0])==15:\n",
    "        data_root.pop(data_root.index(item))\n",
    "print(len(data_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549d514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c250db0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# implementation of a paper : \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch \n",
    "import string\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "\n",
    "class model(nn.Module): \n",
    "\n",
    "    def __init__(self, data, batch_size ,embedding_size, hidden_size,num_layers ,dropout, teacher_forcing_ratio, learning_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''\n",
    "        our input data are arabic words with their roots, with the hypothesis that each word has it's own root.\n",
    "        \n",
    "        (our dataset for this is named root_data)\n",
    "        '''\n",
    "        self.sow = '$'\n",
    "        self.eow = '£'\n",
    "        self.lr = learning_rate\n",
    "        self.ratio = 0.8\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.batches, self.vocab, self.char_index_dic = self.prepare_data(self.data)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embedding(num_embeddings = len(self.vocab), embedding_dim = self.embedding_size, padding_idx = self.char_index_dic['%']) \n",
    "        \n",
    "        \n",
    "        #self.bigru = nn.GRU(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=self.num_layers, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.BILSTM = nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=self.num_layers, bidirectional=True, batch_first=True, dropout = self.dropout)\n",
    "\n",
    "        \n",
    "        #self.gru = nn.GRU(input_size= self.embedding_size ,hidden_size = self.hidden_size * 2, num_layers = self.num_layers, batch_first = True)\n",
    "        self.LSTM = nn.LSTM(input_size= self.embedding_size ,hidden_size = self.hidden_size*2, num_layers = self.num_layers, batch_first = True, dropout = self.dropout)\n",
    "\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index =self.char_index_dic['%'])\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.Linear = nn.Linear(self.hidden_size * 2,len(self.vocab))\n",
    "        #self.Linear = nn.Linear(self.hidden_size * 2,1)\n",
    "        \n",
    "        #self.optimizer = optim.Adam([*self.bigru.parameters(), *self.gru.parameters(), *self.Linear.parameters()], lr = 0.001)\n",
    "        #self.optimizer = optim.Adam([*self.BILSTM.parameters(), *self.LSTM.parameters()], lr = 0.1)\n",
    "        #self.optimizer = optim.Adam(self.parameters(), lr = self.lr)\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr = self.lr)\n",
    "    \n",
    "    def prepare_data(self, data):\n",
    "    \n",
    "        #Le'ts create a padding for ouriinstances : \n",
    "\n",
    "        pad_char = '%'\n",
    "        padded_data = []\n",
    "        ls_words = []\n",
    "        ls_roots = []\n",
    "        for instance in data : \n",
    "            ls_words.append(instance[0])\n",
    "            ls_roots.append(instance[1])\n",
    "        \n",
    "        # Let's calculate the biggest length\n",
    "        max_len_words = max([len(item) for item in ls_words])\n",
    "        max_len_roots = max([len(item) for item in ls_roots])\n",
    "\n",
    "        # Now we pad the word until we reach the max length\n",
    "        for instance in data: \n",
    "            tmp = []\n",
    "            word,root = instance[0], instance[1]\n",
    "            while(len(word) != max_len_words):\n",
    "                word += pad_char\n",
    "            tmp.append(word)\n",
    "            while(len(root) != max_len_roots):\n",
    "                root += pad_char\n",
    "            tmp.append(root)\n",
    "            padded_data.append(tmp)\n",
    "\n",
    "        # let's create our vocab : \n",
    "\n",
    "        vocab = []\n",
    "        for word in padded_data :\n",
    "            for item in word : \n",
    "                tmp = set(item)\n",
    "                for k in tmp : \n",
    "                    if k not in vocab : \n",
    "                        vocab.append(k)\n",
    "\n",
    "        # Let's create our dictionnary with unique indexes\n",
    "\n",
    "        char_to_idx_map = {char: idx for idx, char in enumerate(vocab)}\n",
    "\n",
    "        # Let's now split our data to batches\n",
    "\n",
    "        final_data = []\n",
    "        for instance in padded_data : \n",
    "            tmp = []\n",
    "            word = [char_to_idx_map[char] for char in instance[0]]\n",
    "            root = [char_to_idx_map[char] for char in instance[1]]\n",
    "            tmp.append(word)\n",
    "            tmp.append(root)\n",
    "            final_data.append(tmp)\n",
    "\n",
    "        size= self.batch_size \n",
    "        batches = [final_data[i:i + size] for i in range(0, len(final_data), size)]\n",
    "        \n",
    "        return batches , vocab , char_to_idx_map\n",
    "    \n",
    "    \n",
    "    def word_to_seq(self, word):\n",
    "        '''\n",
    "        this function returns a sequence of the unique indexes for the given word \n",
    "        (sequence is tensor that can be changed using a .tolist() )\n",
    "        '''\n",
    "        word_char_idx_seq =[self.char_index_dic[char] for char in word]    \n",
    "        return word_char_idx_seq # word sequence\n",
    "    \n",
    "    \n",
    "    # Let's now construct our model : \n",
    "    \n",
    "    # we should think about character embeddings in order to create an embeded matrix for each word\n",
    "        \n",
    "    \n",
    "    \n",
    "    def encode(self, batch):    \n",
    "        '''\n",
    "        input : a batch of sequences of instances : [word_seq , root_seq] * batch_size\n",
    "                input_size : (input_size,2)\n",
    "        '''\n",
    "        \n",
    "        word_batch = [] # list of words in the batch\n",
    "        root_batch = [] # list of roots in the batch\n",
    "        \n",
    "        for instance in batch : \n",
    "            word_batch.append(instance[0])\n",
    "            root_batch.append(instance[1])\n",
    "            \n",
    "        word_batch = torch.tensor(word_batch)\n",
    "        root_batch = torch.tensor(root_batch)\n",
    "        \n",
    "        # we create embedding of the word batch : \n",
    "        \n",
    "        embedded_word_batch = self.embedding(word_batch)\n",
    "                \n",
    "        outputs, (hidden, cell) = self.BILSTM(embedded_word_batch) # we pass the emebedded vector through the bi-GRU \n",
    "        \n",
    "        \n",
    "        # hidden size : [2 * num_layers, batch_size , hidden_size]\n",
    "        \n",
    "        # we want hidden size : [num_layers , batch_size  , 2 * hidden_size]\n",
    "        \n",
    "        # we return an adequate layer for the decoder : \n",
    "        \n",
    "        final_hid, final_ce = [], []\n",
    "        for k in range(0,hidden.size(0), 2):\n",
    "            \n",
    "            tmp_hid = hidden[k:k+2 , :, :]\n",
    "            tmp_ce = cell[k:k+2, :, :]\n",
    "            \n",
    "            \n",
    "            cct_hid = torch.cat((tmp_hid[0], tmp_hid[1]), dim  = 1).tolist()\n",
    "            cct_ce = torch.cat((tmp_ce[0], tmp_ce[1]), dim  = 1).tolist()\n",
    "            \n",
    "            final_hid.append(cct_hid)\n",
    "            final_ce.append(cct_ce)\n",
    "        \n",
    "        final_hid, final_ce = torch.tensor(final_hid), torch.tensor(final_ce)\n",
    "        \n",
    "        #print(final_hid.size(), final_ce.size())\n",
    "            \n",
    "        return root_batch , (final_hid, final_ce)\n",
    "    \n",
    "    \n",
    "    def decode(self, encoder_hidden_cell , batch, teacher_forcing_bool):\n",
    "        \n",
    "        '''\n",
    "        input : encoding_hidden_layer => corresponds to the concatenation of the final hidden layers \n",
    "                                        of the bidirectionnal gru in our encoder\n",
    "                \n",
    "                batch : subset of data that contains the roots of the words we encoded.\n",
    "                \n",
    "        output : we'll see :) \n",
    "        \n",
    "        '''\n",
    "\n",
    "        (hidden_layer , cell) , root_batch = encoder_hidden_cell , batch \n",
    "                        \n",
    "        embedded_char = self.embedding(torch.unsqueeze(root_batch[:, 0], 1))\n",
    "            \n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(root_batch.size(1)): \n",
    "            \n",
    "            decoder_output , (hidden_layer, cell) = self.LSTM(embedded_char, (hidden_layer, cell))\n",
    "                        \n",
    "            input_dense = nn.Linear(self.hidden_size * 2,self.embedding_size)\n",
    "            input_decoder_output = input_dense(decoder_output)\n",
    "            \n",
    "            embedded_char = input_decoder_output\n",
    "    \n",
    "            mask = np.where([random.random() < self.teacher_forcing_ratio for i in range(root_batch.size(0))])[0]\n",
    "            \n",
    "            teacher_forcing_input = self.embedding(torch.unsqueeze(torch.clone(root_batch[:, i]), 1))\n",
    "            \n",
    "            if teacher_forcing_bool : \n",
    "\n",
    "                embedded_char[mask] = teacher_forcing_input[mask] \n",
    "                \n",
    "            Dense_decoded_output = self.Linear(decoder_output)\n",
    "            \n",
    "            soft = nn.Softmax(dim = 2)\n",
    "            \n",
    "            soft_out = soft(Dense_decoded_output)\n",
    "\n",
    "            outputs.append(soft_out)\n",
    "\n",
    "        return outputs \n",
    "                            \n",
    "        \n",
    "    \n",
    "    def train_model(self, batches, teacher_forcing_bool):\n",
    "                \n",
    "        train_batches = batches        \n",
    "         \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        n = 0            \n",
    "        \n",
    "        for batch in train_batches :\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            root_batch, encoder_states = self.encode(batch)\n",
    "\n",
    "            outputs = self.decode(encoder_states, root_batch, teacher_forcing_bool)\n",
    "\n",
    "            a = [torch.squeeze(item, 1) for item in outputs]\n",
    "            a = [torch.unsqueeze(item, 0) for item in a]\n",
    "\n",
    "            output = torch.cat(a, dim = 0)\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output.view(-1, output_dim)\n",
    "\n",
    "            trg = root_batch.transpose(0, 1)\n",
    "\n",
    "            trg = trg.reshape(-1)\n",
    "\n",
    "            #print(output.size(),trg.size())\n",
    "            \n",
    "            loss = self.criterion(output, trg)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_loss+=loss.item()\n",
    "\n",
    "            n+=1\n",
    "\n",
    "            print('the loss of the train batch ', n ,' is : ', loss.item())\n",
    "    \n",
    "        return epoch_loss/n\n",
    "\n",
    "    def evaluate_model(self, batches, teacher_forcing_bool):\n",
    "        '''\n",
    "        this method evaluates our model :=)\n",
    "        will be similar to train but without the teacher forcing/ using an optimizer \n",
    "        '''          \n",
    "        self.eval()\n",
    "\n",
    "        val_batches = batches\n",
    "\n",
    "        n = 0\n",
    "\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        with torch.no_grad() :\n",
    "\n",
    "            for batch in val_batches :\n",
    "\n",
    "                root_batch, encoder_states = self.encode(batch)\n",
    "\n",
    "                outputs = self.decode(encoder_states, root_batch, teacher_forcing_bool)\n",
    "\n",
    "                a = [torch.squeeze(item, 1) for item in outputs]\n",
    "                a = [torch.unsqueeze(item, 0) for item in a]\n",
    "\n",
    "                output = torch.cat(a, dim = 0)\n",
    "\n",
    "                output_dim = output.shape[-1]\n",
    "\n",
    "                output = output.view(-1, output_dim)\n",
    "\n",
    "                trg = root_batch.transpose(0, 1)\n",
    "\n",
    "                trg = trg.reshape(-1)\n",
    "                \n",
    "                #print(output.size(), trg.size())\n",
    "                \n",
    "                loss = self.criterion(output, trg)\n",
    "\n",
    "                epoch_loss+=loss.item()\n",
    "\n",
    "                n+=1\n",
    "\n",
    "                print('the loss of the val batch ', n ,' is : ', loss.item())\n",
    "\n",
    "        return epoch_loss / n\n",
    "    \n",
    "    def predict(self, word):\n",
    "        '''\n",
    "        this is the adaptation of encoder-decoder network on a single word w/o optimization\n",
    "        '''\n",
    "\n",
    "        # Let's turn the word into a sequence of word indexes \n",
    "        word_seq = self.word_to_seq(word)\n",
    "\n",
    "        # Let's create an embedding of the word seq\n",
    "        embedded_word = self.embedding(torch.tensor(word_seq))\n",
    "\n",
    "        # Let's feed our word embedding to the encoder network\n",
    "        outputs, (hidden, cell) = self.BILSTM(embedded_word)\n",
    "        \n",
    "        #print(hidden.size())\n",
    "        \n",
    "        final_hid, final_ce = [], []\n",
    "        for k in range(0,hidden.size(0), 2):\n",
    "            \n",
    "            tmp_hid = hidden[k:k+2 ,:]\n",
    "            tmp_ce = cell[k:k+2, :]\n",
    "\n",
    "            cct_hid = torch.cat((tmp_hid[0], tmp_hid[1]), dim  = -1).tolist()\n",
    "            cct_ce = torch.cat((tmp_ce[0], tmp_ce[1]), dim  = -1).tolist()\n",
    "\n",
    "            final_hid.append(cct_hid)\n",
    "            final_ce.append(cct_ce)\n",
    "        \n",
    "        final_hidden, final_cell = torch.tensor(final_hid), torch.tensor(final_ce)\n",
    "\n",
    "        #initialize the input of the decoder\n",
    "\n",
    "        embedded_char = torch.unsqueeze(self.embedding(torch.tensor(self.char_index_dic[self.sow])), 0)\n",
    "\n",
    "        prediction_output = [] # a list of the outputs of the decoder \n",
    "     \n",
    "        # we create a softmax layer : \n",
    "\n",
    "        soft = nn.Softmax(dim = 1)\n",
    "        \n",
    "        key_list = list(self.char_index_dic.keys())\n",
    "        val_list = list(self.char_index_dic.values())\n",
    "        \n",
    "        for i in range(5):\n",
    "                        \n",
    "            decoder_output , (final_hidden, final_cell) = self.LSTM(embedded_char, (final_hidden, final_cell))\n",
    "\n",
    "            input_dense = nn.Linear(self.hidden_size * 2,self.embedding_size)\n",
    "            input_decoder_output = input_dense(decoder_output)\n",
    "\n",
    "            embedded_char = input_decoder_output\n",
    "\n",
    "            Dense_decoded_output = self.Linear(decoder_output)\n",
    "            prediction_output.append(soft(Dense_decoded_output))\n",
    "\n",
    "                \n",
    "        best_char_indexes = [torch.argmax(item).item() for item in prediction_output]\n",
    "\n",
    "\n",
    " \n",
    "        position = [val_list.index(item) for item in best_char_indexes]\n",
    "        result_char = [key_list[pos] for pos in position]\n",
    "        predicted_root = ''.join(result_char)\n",
    "           \n",
    "        print(predicted_root)\n",
    "    \n",
    "        return predicted_root\n",
    "\n",
    "    \n",
    "\n",
    "    def fit(self, num_epochs):\n",
    "        \n",
    "        \"\"\"\n",
    "        let's first prepare our data\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        print(f'The model has {self.count_parameters():,} trainable parameters')\n",
    "        \n",
    "        data = self.data\n",
    "        \n",
    "        data = random.sample(data, len(data))\n",
    "        data_size = len(data)\n",
    "        middle_index = int(data_size * self.ratio)        \n",
    "        train_data , val_data = data[:middle_index], data[middle_index:]\n",
    "        \n",
    "        train_batches, voc, dic = self.prepare_data(train_data)\n",
    "        val_batches ,voc , dic = self.prepare_data(val_data)\n",
    "        \n",
    "        epochs = list(range(num_epochs))\n",
    "        \n",
    "        best_val_loss = 1000\n",
    "        best_model_par = 0\n",
    "        \n",
    "        losses =[]\n",
    "        predicted_roots = []\n",
    "        test_word = '$' + 'تحليل' + '£'\n",
    "        \n",
    "        \n",
    "        for epoch in epochs : \n",
    "            \n",
    "            print('epoch num : ', epoch) \n",
    "            \n",
    "            t1 = time.time()\n",
    "            \n",
    "            train_batches = random.sample(train_batches , len(train_batches))\n",
    "            #val_batches = random.sample(val_batches, len(val_batches))\n",
    "                        \n",
    "            train_loss = self.train_model(train_batches, 1)\n",
    "            val_loss = self.evaluate_model(val_batches, 0) # we set the teacher forcing to false            \n",
    "            t2 = time.time()\n",
    "            \n",
    "            \n",
    "            predicted_root = self.predict(test_word)\n",
    "            print(predicted_root)\n",
    "            predicted_roots.append(predicted_root)\n",
    "            \n",
    "            \n",
    "            \n",
    "            tmp = [train_loss, val_loss]\n",
    "            losses.append(tmp)\n",
    "            \n",
    "            print('the training loss : ', train_loss , 'the val loss :', val_loss)\n",
    "            print('epoch num : ' ,epoch , ' lasted : ', t2 - t1 , 'seconds')\n",
    "            \n",
    "            if val_loss < best_val_loss :\n",
    "                \n",
    "                best_val_loss = val_loss \n",
    "                best_model_par = self.state_dict()\n",
    "            \n",
    "            torch.save(best_model_par, 'best_model.pt')\n",
    "            \n",
    "        return losses\n",
    "    \n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(torch.numel(p) for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "046db112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstuff to test in order to reduce overfitting :/\\n\\n==> shuffling the dataset before each epoch  \\n==> reclean my dataset and check for outliers\\n==> recheck the structure of my code for the model\\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "stuff to test in order to reduce overfitting :/\n",
    "\n",
    "==> shuffling the dataset before each epoch -- Done \n",
    "==> initialize the encoder with random values instead of zeros. \n",
    "==> reclean my dataset and check for outliers \n",
    "==> recheck the structure of my code for the model \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63eeff97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "794906"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "68364263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549072\n"
     ]
    }
   ],
   "source": [
    "d = []\n",
    "for item in data_root:\n",
    "    if len(item[0]) > 5 :\n",
    "        d.append(item)\n",
    "print(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40568a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918acfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951a03e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633fcc13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c9388d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 20,256,743 trainable parameters\n",
      "epoch num :  0\n",
      "the loss of the train batch  1  is :  3.6634202003479004\n",
      "the loss of the train batch  2  is :  3.6625521183013916\n",
      "the loss of the train batch  3  is :  3.6615493297576904\n",
      "the loss of the train batch  4  is :  3.6597795486450195\n",
      "the loss of the train batch  5  is :  3.653594493865967\n",
      "the loss of the train batch  6  is :  3.585444211959839\n",
      "the loss of the train batch  7  is :  3.482663154602051\n",
      "the loss of the train batch  8  is :  3.4820497035980225\n",
      "the loss of the train batch  9  is :  3.4808826446533203\n",
      "the loss of the train batch  10  is :  3.480748414993286\n",
      "the loss of the train batch  11  is :  3.4803426265716553\n",
      "the loss of the train batch  12  is :  3.4795522689819336\n",
      "the loss of the train batch  13  is :  3.4793527126312256\n",
      "the loss of the train batch  14  is :  3.4790451526641846\n",
      "the loss of the train batch  15  is :  3.4781334400177\n",
      "the loss of the train batch  16  is :  3.4775454998016357\n",
      "the loss of the train batch  17  is :  3.4776127338409424\n",
      "the loss of the train batch  18  is :  3.4765119552612305\n",
      "the loss of the train batch  19  is :  3.4766833782196045\n",
      "the loss of the train batch  20  is :  3.4756171703338623\n",
      "the loss of the train batch  21  is :  3.476274013519287\n",
      "the loss of the train batch  22  is :  3.4755423069000244\n",
      "the loss of the train batch  23  is :  3.4747707843780518\n",
      "the loss of the train batch  24  is :  3.474327802658081\n",
      "the loss of the train batch  25  is :  3.4736037254333496\n",
      "the loss of the train batch  26  is :  3.473285675048828\n",
      "the loss of the train batch  27  is :  3.472811460494995\n",
      "the loss of the train batch  28  is :  3.4716715812683105\n",
      "the loss of the train batch  29  is :  3.4714624881744385\n",
      "the loss of the train batch  30  is :  3.47039532661438\n",
      "the loss of the train batch  31  is :  3.4698598384857178\n",
      "the loss of the train batch  32  is :  3.4688730239868164\n",
      "the loss of the train batch  33  is :  3.4698760509490967\n",
      "the loss of the train batch  34  is :  3.4689385890960693\n",
      "the loss of the train batch  35  is :  3.4642493724823\n",
      "the loss of the train batch  36  is :  3.4620609283447266\n",
      "the loss of the train batch  37  is :  3.4841887950897217\n",
      "the loss of the train batch  38  is :  3.476353645324707\n",
      "the loss of the train batch  39  is :  3.4741644859313965\n",
      "the loss of the train batch  40  is :  3.4727389812469482\n",
      "the loss of the train batch  41  is :  3.469517946243286\n",
      "the loss of the train batch  42  is :  3.4692087173461914\n",
      "the loss of the train batch  43  is :  3.4680237770080566\n",
      "the loss of the train batch  44  is :  3.4671483039855957\n",
      "the loss of the train batch  45  is :  3.46471905708313\n",
      "the loss of the train batch  46  is :  3.462719678878784\n",
      "the loss of the train batch  47  is :  3.461305618286133\n",
      "the loss of the train batch  48  is :  3.4578325748443604\n",
      "the loss of the train batch  49  is :  3.456998825073242\n",
      "the loss of the train batch  50  is :  3.4560742378234863\n",
      "the loss of the train batch  51  is :  3.4528086185455322\n",
      "the loss of the train batch  52  is :  3.4572768211364746\n",
      "the loss of the train batch  53  is :  3.452371835708618\n",
      "the loss of the train batch  54  is :  3.453394889831543\n",
      "the loss of the train batch  55  is :  3.4542033672332764\n",
      "the loss of the train batch  56  is :  3.45263409614563\n",
      "the loss of the train batch  57  is :  3.448634147644043\n",
      "the loss of the train batch  58  is :  3.4482038021087646\n",
      "the loss of the train batch  59  is :  3.449606418609619\n",
      "the loss of the train batch  60  is :  3.4498989582061768\n",
      "the loss of the train batch  61  is :  3.452363967895508\n",
      "the loss of the train batch  62  is :  3.458731174468994\n",
      "the loss of the train batch  63  is :  3.456821918487549\n",
      "the loss of the train batch  64  is :  3.4552714824676514\n",
      "the loss of the train batch  65  is :  3.4512946605682373\n",
      "the loss of the train batch  66  is :  3.4466426372528076\n",
      "the loss of the train batch  67  is :  3.442173719406128\n",
      "the loss of the train batch  68  is :  3.4471640586853027\n",
      "the loss of the train batch  69  is :  3.440370559692383\n",
      "the loss of the train batch  70  is :  3.4365739822387695\n",
      "the loss of the train batch  71  is :  3.443540334701538\n",
      "the loss of the train batch  72  is :  3.453648090362549\n",
      "the loss of the train batch  73  is :  3.452225685119629\n",
      "the loss of the train batch  74  is :  3.4475018978118896\n",
      "the loss of the train batch  75  is :  3.4339382648468018\n",
      "the loss of the train batch  76  is :  3.4450042247772217\n",
      "the loss of the train batch  77  is :  3.4509286880493164\n",
      "the loss of the train batch  78  is :  3.445258855819702\n",
      "the loss of the train batch  79  is :  3.4491841793060303\n",
      "the loss of the train batch  80  is :  3.442600727081299\n",
      "the loss of the train batch  81  is :  3.434835433959961\n",
      "the loss of the train batch  82  is :  3.429988145828247\n",
      "the loss of the train batch  83  is :  3.434156656265259\n",
      "the loss of the train batch  84  is :  3.4362311363220215\n",
      "the loss of the train batch  85  is :  3.433422565460205\n",
      "the loss of the train batch  86  is :  3.425098180770874\n",
      "the loss of the train batch  87  is :  3.4027910232543945\n",
      "the loss of the train batch  88  is :  3.370223045349121\n",
      "the loss of the train batch  89  is :  3.422492504119873\n",
      "the loss of the train batch  90  is :  3.363452672958374\n",
      "the loss of the train batch  91  is :  3.335451126098633\n",
      "the loss of the train batch  92  is :  3.319366693496704\n",
      "the loss of the train batch  93  is :  3.3052473068237305\n",
      "the loss of the train batch  94  is :  3.291501522064209\n",
      "the loss of the train batch  95  is :  3.291055679321289\n",
      "the loss of the train batch  96  is :  3.2856361865997314\n",
      "the loss of the train batch  97  is :  3.2760698795318604\n",
      "the loss of the train batch  98  is :  3.27760648727417\n",
      "the loss of the train batch  99  is :  3.2751104831695557\n",
      "the loss of the train batch  100  is :  3.2802348136901855\n",
      "the loss of the train batch  101  is :  3.2758069038391113\n",
      "the loss of the train batch  102  is :  3.273639440536499\n",
      "the loss of the train batch  103  is :  3.267287492752075\n",
      "the loss of the train batch  104  is :  3.2752296924591064\n",
      "the loss of the train batch  105  is :  3.2688260078430176\n",
      "the loss of the train batch  106  is :  3.2733163833618164\n",
      "the loss of the train batch  107  is :  3.2722675800323486\n",
      "the loss of the train batch  108  is :  3.2759461402893066\n",
      "the loss of the train batch  109  is :  3.2700228691101074\n",
      "the loss of the train batch  110  is :  3.2794365882873535\n",
      "the loss of the train batch  111  is :  3.2802042961120605\n",
      "the loss of the train batch  112  is :  3.2696046829223633\n",
      "the loss of the train batch  113  is :  3.263320207595825\n",
      "the loss of the train batch  114  is :  3.2757139205932617\n",
      "the loss of the train batch  115  is :  3.265315294265747\n",
      "the loss of the train batch  116  is :  3.2775163650512695\n",
      "the loss of the train batch  117  is :  3.279428720474243\n",
      "the loss of the train batch  118  is :  3.2757019996643066\n",
      "the loss of the train batch  119  is :  3.2791197299957275\n",
      "the loss of the train batch  120  is :  3.273972749710083\n",
      "the loss of the train batch  121  is :  3.2592930793762207\n",
      "the loss of the train batch  122  is :  3.26358962059021\n",
      "the loss of the train batch  123  is :  3.2645504474639893\n",
      "the loss of the train batch  124  is :  3.2643418312072754\n",
      "the loss of the train batch  125  is :  3.257035255432129\n",
      "the loss of the train batch  126  is :  3.259443998336792\n",
      "the loss of the train batch  127  is :  3.2629776000976562\n",
      "the loss of the train batch  128  is :  3.258706569671631\n",
      "the loss of the train batch  129  is :  3.2652664184570312\n",
      "the loss of the train batch  130  is :  3.267739772796631\n",
      "the loss of the train batch  131  is :  3.2563297748565674\n",
      "the loss of the train batch  132  is :  3.259042978286743\n",
      "the loss of the train batch  133  is :  3.269068956375122\n",
      "the loss of the train batch  134  is :  3.275592803955078\n",
      "the loss of the train batch  135  is :  3.2813644409179688\n",
      "the loss of the train batch  136  is :  3.274730920791626\n",
      "the loss of the train batch  137  is :  3.274946689605713\n",
      "the loss of the train batch  138  is :  3.2722909450531006\n",
      "the loss of the train batch  139  is :  3.279855966567993\n",
      "the loss of the train batch  140  is :  3.27384090423584\n",
      "the loss of the train batch  141  is :  3.2708523273468018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss of the train batch  142  is :  3.264816999435425\n",
      "the loss of the train batch  143  is :  3.257237434387207\n",
      "the loss of the train batch  144  is :  3.255398750305176\n",
      "the loss of the train batch  145  is :  3.2619471549987793\n",
      "the loss of the train batch  146  is :  3.254399061203003\n",
      "the loss of the train batch  147  is :  3.264723539352417\n",
      "the loss of the train batch  148  is :  3.255960464477539\n",
      "the loss of the train batch  149  is :  3.254455327987671\n",
      "the loss of the train batch  150  is :  3.253262758255005\n",
      "the loss of the train batch  151  is :  3.258193254470825\n",
      "the loss of the train batch  152  is :  3.2616536617279053\n",
      "the loss of the train batch  153  is :  3.262356996536255\n",
      "the loss of the train batch  154  is :  3.257763624191284\n",
      "the loss of the train batch  155  is :  3.250488758087158\n",
      "the loss of the train batch  156  is :  3.25148868560791\n",
      "the loss of the train batch  157  is :  3.263176679611206\n",
      "the loss of the train batch  158  is :  3.2521653175354004\n",
      "the loss of the train batch  159  is :  3.2547974586486816\n",
      "the loss of the train batch  160  is :  3.252194404602051\n",
      "the loss of the train batch  161  is :  3.251254081726074\n",
      "the loss of the train batch  162  is :  3.256171703338623\n",
      "the loss of the train batch  163  is :  3.2538623809814453\n",
      "the loss of the train batch  164  is :  3.260371685028076\n",
      "the loss of the train batch  165  is :  3.2551705837249756\n",
      "the loss of the train batch  166  is :  3.2520620822906494\n",
      "the loss of the train batch  167  is :  3.253460645675659\n",
      "the loss of the train batch  168  is :  3.251601457595825\n",
      "the loss of the train batch  169  is :  3.2539713382720947\n",
      "the loss of the train batch  170  is :  3.250457763671875\n",
      "the loss of the train batch  171  is :  3.2561542987823486\n",
      "the loss of the train batch  172  is :  3.2582249641418457\n",
      "the loss of the train batch  173  is :  3.2488749027252197\n",
      "the loss of the train batch  174  is :  3.2561662197113037\n",
      "the loss of the train batch  175  is :  3.254925012588501\n",
      "the loss of the train batch  176  is :  3.2575268745422363\n",
      "the loss of the train batch  177  is :  3.2512364387512207\n",
      "the loss of the train batch  178  is :  3.2554025650024414\n",
      "the loss of the train batch  179  is :  3.2520551681518555\n",
      "the loss of the train batch  180  is :  3.2581653594970703\n",
      "the loss of the train batch  181  is :  3.2612950801849365\n",
      "the loss of the train batch  182  is :  3.2580041885375977\n",
      "the loss of the train batch  183  is :  3.256181240081787\n",
      "the loss of the train batch  184  is :  3.250648021697998\n",
      "the loss of the train batch  185  is :  3.2511141300201416\n",
      "the loss of the train batch  186  is :  3.25205659866333\n",
      "the loss of the train batch  187  is :  3.2501285076141357\n",
      "the loss of the train batch  188  is :  3.251197099685669\n",
      "the loss of the train batch  189  is :  3.2511231899261475\n",
      "the loss of the train batch  190  is :  3.2478976249694824\n",
      "the loss of the train batch  191  is :  3.2557666301727295\n",
      "the loss of the train batch  192  is :  3.255934715270996\n",
      "the loss of the train batch  193  is :  3.2502200603485107\n",
      "the loss of the train batch  194  is :  3.2530038356781006\n",
      "the loss of the train batch  195  is :  3.2554709911346436\n",
      "the loss of the train batch  196  is :  3.2577638626098633\n",
      "the loss of the train batch  197  is :  3.2480628490448\n",
      "the loss of the train batch  198  is :  3.253495693206787\n",
      "the loss of the train batch  199  is :  3.245999813079834\n",
      "the loss of the train batch  200  is :  3.2538161277770996\n",
      "the loss of the train batch  201  is :  3.2644996643066406\n",
      "the loss of the train batch  202  is :  3.254873275756836\n",
      "the loss of the train batch  203  is :  3.2518739700317383\n",
      "the loss of the train batch  204  is :  3.257464647293091\n",
      "the loss of the train batch  205  is :  3.254406452178955\n",
      "the loss of the train batch  206  is :  3.2508533000946045\n",
      "the loss of the train batch  207  is :  3.2614386081695557\n",
      "the loss of the train batch  208  is :  3.2481656074523926\n",
      "the loss of the train batch  209  is :  3.2531354427337646\n",
      "the loss of the train batch  210  is :  3.256235122680664\n",
      "the loss of the train batch  211  is :  3.2510557174682617\n",
      "the loss of the train batch  212  is :  3.255115032196045\n",
      "the loss of the train batch  213  is :  3.2480924129486084\n",
      "the loss of the train batch  214  is :  3.2502548694610596\n",
      "the loss of the train batch  215  is :  3.249746799468994\n",
      "the loss of the train batch  216  is :  3.257434844970703\n",
      "the loss of the train batch  217  is :  3.242283344268799\n",
      "the loss of the train batch  218  is :  3.253467082977295\n",
      "the loss of the train batch  219  is :  3.252168655395508\n",
      "the loss of the train batch  220  is :  3.253835916519165\n",
      "the loss of the train batch  221  is :  3.249943494796753\n",
      "the loss of the train batch  222  is :  3.2576937675476074\n",
      "the loss of the train batch  223  is :  3.2501916885375977\n",
      "the loss of the train batch  224  is :  3.2465405464172363\n",
      "the loss of the train batch  225  is :  3.253206491470337\n",
      "the loss of the train batch  226  is :  3.2570431232452393\n",
      "the loss of the train batch  227  is :  3.2549796104431152\n",
      "the loss of the train batch  228  is :  3.2606289386749268\n",
      "the loss of the train batch  229  is :  3.2522566318511963\n",
      "the loss of the train batch  230  is :  3.2424020767211914\n",
      "the loss of the train batch  231  is :  3.255253791809082\n",
      "the loss of the train batch  232  is :  3.2520031929016113\n",
      "the loss of the train batch  233  is :  3.249375104904175\n",
      "the loss of the train batch  234  is :  3.258253574371338\n",
      "the loss of the train batch  235  is :  3.2571122646331787\n",
      "the loss of the train batch  236  is :  3.2518019676208496\n",
      "the loss of the train batch  237  is :  3.2562127113342285\n",
      "the loss of the train batch  238  is :  3.2545645236968994\n",
      "the loss of the train batch  239  is :  3.254164218902588\n",
      "the loss of the train batch  240  is :  3.2464258670806885\n",
      "the loss of the train batch  241  is :  3.2512638568878174\n",
      "the loss of the train batch  242  is :  3.255709648132324\n",
      "the loss of the train batch  243  is :  3.254793405532837\n",
      "the loss of the train batch  244  is :  3.247393846511841\n",
      "the loss of the train batch  245  is :  3.2451140880584717\n",
      "the loss of the train batch  246  is :  3.2482895851135254\n",
      "the loss of the train batch  247  is :  3.248694658279419\n",
      "the loss of the train batch  248  is :  3.2473561763763428\n",
      "the loss of the train batch  249  is :  3.2523677349090576\n",
      "the loss of the train batch  250  is :  3.24572491645813\n",
      "the loss of the train batch  251  is :  3.2498083114624023\n",
      "the loss of the train batch  252  is :  3.247159957885742\n",
      "the loss of the train batch  253  is :  3.248502254486084\n",
      "the loss of the train batch  254  is :  3.2509896755218506\n",
      "the loss of the train batch  255  is :  3.245725154876709\n",
      "the loss of the train batch  256  is :  3.2484934329986572\n",
      "the loss of the train batch  257  is :  3.2406578063964844\n",
      "the loss of the train batch  258  is :  3.2492499351501465\n",
      "the loss of the train batch  259  is :  3.2522811889648438\n",
      "the loss of the train batch  260  is :  3.2454299926757812\n",
      "the loss of the train batch  261  is :  3.254452705383301\n",
      "the loss of the train batch  262  is :  3.2473840713500977\n",
      "the loss of the train batch  263  is :  3.2481467723846436\n",
      "the loss of the train batch  264  is :  3.2544937133789062\n",
      "the loss of the train batch  265  is :  3.246835231781006\n",
      "the loss of the train batch  266  is :  3.248183250427246\n",
      "the loss of the train batch  267  is :  3.2502615451812744\n",
      "the loss of the train batch  268  is :  3.260509490966797\n",
      "the loss of the train batch  269  is :  3.26228666305542\n",
      "the loss of the train batch  270  is :  3.2524704933166504\n",
      "the loss of the train batch  271  is :  3.249356985092163\n",
      "the loss of the train batch  272  is :  3.256675958633423\n",
      "the loss of the train batch  273  is :  3.2493722438812256\n",
      "the loss of the train batch  274  is :  3.2527577877044678\n",
      "the loss of the train batch  275  is :  3.252375602722168\n",
      "the loss of the train batch  276  is :  3.2482409477233887\n",
      "the loss of the train batch  277  is :  3.248518228530884\n",
      "the loss of the train batch  278  is :  3.2522830963134766\n",
      "the loss of the train batch  279  is :  3.2508952617645264\n",
      "the loss of the train batch  280  is :  3.2502920627593994\n",
      "the loss of the train batch  281  is :  3.2459914684295654\n",
      "the loss of the train batch  282  is :  3.23905086517334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss of the train batch  283  is :  3.251084327697754\n",
      "the loss of the train batch  284  is :  3.2478246688842773\n",
      "the loss of the train batch  285  is :  3.251664638519287\n",
      "the loss of the train batch  286  is :  3.2462611198425293\n",
      "the loss of the train batch  287  is :  3.2506563663482666\n",
      "the loss of the train batch  288  is :  3.2489469051361084\n",
      "the loss of the train batch  289  is :  3.2501208782196045\n",
      "the loss of the train batch  290  is :  3.249495267868042\n",
      "the loss of the train batch  291  is :  3.2490742206573486\n",
      "the loss of the train batch  292  is :  3.2417709827423096\n",
      "the loss of the train batch  293  is :  3.2462568283081055\n",
      "the loss of the train batch  294  is :  3.25225567817688\n",
      "the loss of the train batch  295  is :  3.2467100620269775\n",
      "the loss of the train batch  296  is :  3.2563345432281494\n",
      "the loss of the train batch  297  is :  3.249711275100708\n",
      "the loss of the train batch  298  is :  3.2452938556671143\n",
      "the loss of the train batch  299  is :  3.243373155593872\n",
      "the loss of the train batch  300  is :  3.251847505569458\n",
      "the loss of the train batch  301  is :  3.2455644607543945\n",
      "the loss of the train batch  302  is :  3.245748996734619\n",
      "the loss of the train batch  303  is :  3.2445592880249023\n",
      "the loss of the train batch  304  is :  3.248436212539673\n",
      "the loss of the train batch  305  is :  3.2435805797576904\n",
      "the loss of the train batch  306  is :  3.2534053325653076\n",
      "the loss of the train batch  307  is :  3.2513113021850586\n",
      "the loss of the train batch  308  is :  3.2433135509490967\n",
      "the loss of the train batch  309  is :  3.2428789138793945\n",
      "the loss of the train batch  310  is :  3.251297950744629\n",
      "the loss of the train batch  311  is :  3.2409939765930176\n",
      "the loss of the train batch  312  is :  3.2433059215545654\n",
      "the loss of the train batch  313  is :  3.2431411743164062\n",
      "the loss of the train batch  314  is :  3.244464635848999\n",
      "the loss of the train batch  315  is :  3.2469334602355957\n",
      "the loss of the train batch  316  is :  3.2412052154541016\n",
      "the loss of the train batch  317  is :  3.2484488487243652\n",
      "the loss of the train batch  318  is :  3.2531328201293945\n",
      "the loss of the train batch  319  is :  3.2527177333831787\n",
      "the loss of the train batch  320  is :  3.240856170654297\n",
      "the loss of the train batch  321  is :  3.247645854949951\n",
      "the loss of the train batch  322  is :  3.2453224658966064\n",
      "the loss of the train batch  323  is :  3.246569871902466\n",
      "the loss of the train batch  324  is :  3.248168468475342\n",
      "the loss of the train batch  325  is :  3.2487435340881348\n",
      "the loss of the train batch  326  is :  3.245126724243164\n",
      "the loss of the train batch  327  is :  3.249237298965454\n",
      "the loss of the train batch  328  is :  3.2543880939483643\n",
      "the loss of the train batch  329  is :  3.244858980178833\n",
      "the loss of the train batch  330  is :  3.248234510421753\n",
      "the loss of the train batch  331  is :  3.249464988708496\n",
      "the loss of the train batch  332  is :  3.246018648147583\n",
      "the loss of the train batch  333  is :  3.2414159774780273\n",
      "the loss of the train batch  334  is :  3.2453527450561523\n",
      "the loss of the train batch  335  is :  3.24263596534729\n",
      "the loss of the train batch  336  is :  3.2440643310546875\n",
      "the loss of the train batch  337  is :  3.24960994720459\n",
      "the loss of the train batch  338  is :  3.243931770324707\n",
      "the loss of the train batch  339  is :  3.2468299865722656\n",
      "the loss of the train batch  340  is :  3.249091148376465\n",
      "the loss of the train batch  341  is :  3.2477359771728516\n",
      "the loss of the train batch  342  is :  3.2454710006713867\n",
      "the loss of the train batch  343  is :  3.2378063201904297\n",
      "the loss of the train batch  344  is :  3.247429132461548\n",
      "the loss of the train batch  345  is :  3.2444167137145996\n",
      "the loss of the train batch  346  is :  3.25171160697937\n",
      "the loss of the train batch  347  is :  3.244169235229492\n",
      "the loss of the train batch  348  is :  3.2482337951660156\n",
      "the loss of the train batch  349  is :  3.2438724040985107\n",
      "the loss of the train batch  350  is :  3.2553672790527344\n",
      "the loss of the train batch  351  is :  3.247243881225586\n",
      "the loss of the train batch  352  is :  3.2468905448913574\n",
      "the loss of the train batch  353  is :  3.239823579788208\n",
      "the loss of the train batch  354  is :  3.249875783920288\n",
      "the loss of the train batch  355  is :  3.2446346282958984\n",
      "the loss of the train batch  356  is :  3.2459800243377686\n",
      "the loss of the train batch  357  is :  3.250152826309204\n",
      "the loss of the train batch  358  is :  3.245166778564453\n",
      "the loss of the train batch  359  is :  3.2479746341705322\n",
      "the loss of the train batch  360  is :  3.24648380279541\n",
      "the loss of the train batch  361  is :  3.2516491413116455\n",
      "the loss of the train batch  362  is :  3.247546434402466\n",
      "the loss of the train batch  363  is :  3.247999906539917\n",
      "the loss of the train batch  364  is :  3.246002435684204\n",
      "the loss of the train batch  365  is :  3.249535083770752\n",
      "the loss of the train batch  366  is :  3.2424240112304688\n",
      "the loss of the train batch  367  is :  3.2481069564819336\n",
      "the loss of the train batch  368  is :  3.247523307800293\n",
      "the loss of the train batch  369  is :  3.246190071105957\n",
      "the loss of the train batch  370  is :  3.247025489807129\n",
      "the loss of the train batch  371  is :  3.2400901317596436\n",
      "the loss of the train batch  372  is :  3.249785900115967\n",
      "the loss of the train batch  373  is :  3.2506067752838135\n",
      "the loss of the train batch  374  is :  3.246992588043213\n",
      "the loss of the train batch  375  is :  3.2495462894439697\n",
      "the loss of the train batch  376  is :  3.2472054958343506\n",
      "the loss of the train batch  377  is :  3.244900941848755\n",
      "the loss of the train batch  378  is :  3.2465384006500244\n",
      "the loss of the train batch  379  is :  3.2456672191619873\n",
      "the loss of the train batch  380  is :  3.25030255317688\n",
      "the loss of the train batch  381  is :  3.2451059818267822\n",
      "the loss of the train batch  382  is :  3.2473044395446777\n",
      "the loss of the train batch  383  is :  3.250098466873169\n",
      "the loss of the train batch  384  is :  3.2475943565368652\n",
      "the loss of the train batch  385  is :  3.2442855834960938\n",
      "the loss of the train batch  386  is :  3.246380090713501\n",
      "the loss of the train batch  387  is :  3.2460944652557373\n",
      "the loss of the train batch  388  is :  3.2537333965301514\n",
      "the loss of the train batch  389  is :  3.2481138706207275\n",
      "the loss of the train batch  390  is :  3.2468037605285645\n",
      "the loss of the train batch  391  is :  3.2488012313842773\n",
      "the loss of the train batch  392  is :  3.255671262741089\n",
      "the loss of the train batch  393  is :  3.249918222427368\n",
      "the loss of the train batch  394  is :  3.2459805011749268\n",
      "the loss of the train batch  395  is :  3.242298126220703\n",
      "the loss of the train batch  396  is :  3.2462589740753174\n",
      "the loss of the train batch  397  is :  3.245936632156372\n",
      "the loss of the train batch  398  is :  3.2434327602386475\n",
      "the loss of the train batch  399  is :  3.2469139099121094\n",
      "the loss of the train batch  400  is :  3.248436450958252\n",
      "the loss of the train batch  401  is :  3.241103172302246\n",
      "the loss of the train batch  402  is :  3.2382452487945557\n",
      "the loss of the train batch  403  is :  3.2478995323181152\n",
      "the loss of the train batch  404  is :  3.2471930980682373\n",
      "the loss of the train batch  405  is :  3.247096300125122\n",
      "the loss of the train batch  406  is :  3.2481396198272705\n",
      "the loss of the train batch  407  is :  3.259566068649292\n",
      "the loss of the train batch  408  is :  3.245302677154541\n",
      "the loss of the train batch  409  is :  3.24745774269104\n",
      "the loss of the train batch  410  is :  3.248530864715576\n",
      "the loss of the train batch  411  is :  3.242870569229126\n",
      "the loss of the train batch  412  is :  3.2478315830230713\n",
      "the loss of the train batch  413  is :  3.2450811862945557\n",
      "the loss of the train batch  414  is :  3.251542806625366\n",
      "the loss of the train batch  415  is :  3.2474751472473145\n",
      "the loss of the train batch  416  is :  3.2466185092926025\n",
      "the loss of the train batch  417  is :  3.2485921382904053\n",
      "the loss of the train batch  418  is :  3.252023458480835\n",
      "the loss of the train batch  419  is :  3.244731903076172\n",
      "the loss of the train batch  420  is :  3.2504982948303223\n",
      "the loss of the train batch  421  is :  3.2478187084198\n",
      "the loss of the train batch  422  is :  3.250659704208374\n",
      "the loss of the train batch  423  is :  3.2505950927734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss of the train batch  424  is :  3.2417383193969727\n",
      "the loss of the train batch  425  is :  3.248202323913574\n",
      "the loss of the train batch  426  is :  3.237534999847412\n",
      "the loss of the train batch  427  is :  3.242185592651367\n",
      "the loss of the train batch  428  is :  3.2393295764923096\n",
      "the loss of the train batch  429  is :  3.253091335296631\n",
      "the loss of the train batch  430  is :  3.2470204830169678\n",
      "the loss of the train batch  431  is :  3.2482364177703857\n",
      "the loss of the train batch  432  is :  3.246723175048828\n",
      "the loss of the train batch  433  is :  3.2443487644195557\n",
      "the loss of the train batch  434  is :  3.247401237487793\n",
      "the loss of the train batch  435  is :  3.248199462890625\n",
      "the loss of the train batch  436  is :  3.247392177581787\n",
      "the loss of the train batch  437  is :  3.247722864151001\n",
      "the loss of the train batch  438  is :  3.2456612586975098\n",
      "the loss of the train batch  439  is :  3.2433664798736572\n",
      "the loss of the train batch  440  is :  3.2474324703216553\n",
      "the loss of the train batch  441  is :  3.2584633827209473\n",
      "the loss of the train batch  442  is :  3.249185085296631\n",
      "the loss of the train batch  443  is :  3.242865800857544\n",
      "the loss of the train batch  444  is :  3.243406295776367\n",
      "the loss of the train batch  445  is :  3.242314577102661\n",
      "the loss of the train batch  446  is :  3.2524142265319824\n",
      "the loss of the train batch  447  is :  3.2429378032684326\n",
      "the loss of the train batch  448  is :  3.247095823287964\n",
      "the loss of the train batch  449  is :  3.238496780395508\n",
      "the loss of the train batch  450  is :  3.244575023651123\n",
      "the loss of the train batch  451  is :  3.2441043853759766\n",
      "the loss of the train batch  452  is :  3.2442736625671387\n",
      "the loss of the train batch  453  is :  3.247084856033325\n",
      "the loss of the train batch  454  is :  3.2464652061462402\n",
      "the loss of the train batch  455  is :  3.250307083129883\n",
      "the loss of the train batch  456  is :  3.2418406009674072\n",
      "the loss of the train batch  457  is :  3.2456164360046387\n",
      "the loss of the train batch  458  is :  3.251905679702759\n",
      "the loss of the train batch  459  is :  3.2408430576324463\n",
      "the loss of the train batch  460  is :  3.2512707710266113\n",
      "the loss of the train batch  461  is :  3.246638298034668\n",
      "the loss of the train batch  462  is :  3.2489235401153564\n",
      "the loss of the train batch  463  is :  3.2460238933563232\n",
      "the loss of the train batch  464  is :  3.242065906524658\n",
      "the loss of the train batch  465  is :  3.240685224533081\n",
      "the loss of the train batch  466  is :  3.2410149574279785\n",
      "the loss of the train batch  467  is :  3.246030569076538\n",
      "the loss of the train batch  468  is :  3.2439520359039307\n",
      "the loss of the train batch  469  is :  3.2561264038085938\n",
      "the loss of the train batch  470  is :  3.241215467453003\n",
      "the loss of the train batch  471  is :  3.24444317817688\n",
      "the loss of the train batch  472  is :  3.2473506927490234\n",
      "the loss of the train batch  473  is :  3.2432026863098145\n",
      "the loss of the train batch  474  is :  3.2462737560272217\n",
      "the loss of the train batch  475  is :  3.240290403366089\n",
      "the loss of the train batch  476  is :  3.245086908340454\n",
      "the loss of the train batch  477  is :  3.246703624725342\n",
      "the loss of the train batch  478  is :  3.2452096939086914\n",
      "the loss of the train batch  479  is :  3.24527645111084\n",
      "the loss of the train batch  480  is :  3.2434768676757812\n",
      "the loss of the train batch  481  is :  3.2387139797210693\n",
      "the loss of the train batch  482  is :  3.2311017513275146\n",
      "the loss of the train batch  483  is :  3.2472646236419678\n",
      "the loss of the train batch  484  is :  3.244633913040161\n",
      "the loss of the train batch  485  is :  3.249457359313965\n",
      "the loss of the train batch  486  is :  3.2464864253997803\n",
      "the loss of the train batch  487  is :  3.247908353805542\n",
      "the loss of the train batch  488  is :  3.2407429218292236\n",
      "the loss of the train batch  489  is :  3.239283800125122\n",
      "the loss of the train batch  490  is :  3.241292953491211\n",
      "the loss of the train batch  491  is :  3.25034499168396\n",
      "the loss of the train batch  492  is :  3.244661569595337\n",
      "the loss of the train batch  493  is :  3.2489161491394043\n",
      "the loss of the train batch  494  is :  3.247542381286621\n",
      "the loss of the train batch  495  is :  3.2474524974823\n",
      "the loss of the train batch  496  is :  3.24715518951416\n",
      "the loss of the train batch  497  is :  3.249300003051758\n",
      "the loss of the train batch  498  is :  3.2464394569396973\n",
      "the loss of the train batch  499  is :  3.235698938369751\n",
      "the loss of the train batch  500  is :  3.2447283267974854\n",
      "the loss of the train batch  501  is :  3.2452480792999268\n",
      "the loss of the train batch  502  is :  3.2432284355163574\n",
      "the loss of the train batch  503  is :  3.241706371307373\n",
      "the loss of the train batch  504  is :  3.2524008750915527\n",
      "the loss of the train batch  505  is :  3.3230106830596924\n",
      "the loss of the train batch  506  is :  3.27384352684021\n",
      "the loss of the train batch  507  is :  3.269551992416382\n",
      "the loss of the train batch  508  is :  3.269436836242676\n",
      "the loss of the train batch  509  is :  3.2663886547088623\n",
      "the loss of the train batch  510  is :  3.2666420936584473\n",
      "the loss of the train batch  511  is :  3.271578788757324\n",
      "the loss of the train batch  512  is :  3.2683441638946533\n",
      "the loss of the train batch  513  is :  3.2706682682037354\n",
      "the loss of the train batch  514  is :  3.2719342708587646\n",
      "the loss of the train batch  515  is :  3.2723584175109863\n",
      "the loss of the train batch  516  is :  3.2738583087921143\n",
      "the loss of the train batch  517  is :  3.2723705768585205\n",
      "the loss of the train batch  518  is :  3.2716293334960938\n",
      "the loss of the train batch  519  is :  3.267353057861328\n",
      "the loss of the train batch  520  is :  3.2686593532562256\n",
      "the loss of the train batch  521  is :  3.2622103691101074\n",
      "the loss of the train batch  522  is :  3.273916244506836\n",
      "the loss of the train batch  523  is :  3.266228675842285\n",
      "the loss of the train batch  524  is :  3.263310194015503\n",
      "the loss of the train batch  525  is :  3.2680959701538086\n",
      "the loss of the train batch  526  is :  3.2662582397460938\n",
      "the loss of the train batch  527  is :  3.2690629959106445\n",
      "the loss of the train batch  528  is :  3.2659378051757812\n",
      "the loss of the train batch  529  is :  3.267580509185791\n",
      "the loss of the train batch  530  is :  3.2652363777160645\n",
      "the loss of the train batch  531  is :  3.2694249153137207\n",
      "the loss of the train batch  532  is :  3.27545428276062\n",
      "the loss of the train batch  533  is :  3.2689311504364014\n",
      "the loss of the train batch  534  is :  3.2651846408843994\n",
      "the loss of the train batch  535  is :  3.2699663639068604\n",
      "the loss of the train batch  536  is :  3.272501230239868\n",
      "the loss of the train batch  537  is :  3.2720375061035156\n",
      "the loss of the train batch  538  is :  3.26769757270813\n",
      "the loss of the train batch  539  is :  3.2686877250671387\n",
      "the loss of the train batch  540  is :  3.2660374641418457\n",
      "the loss of the train batch  541  is :  3.268998861312866\n",
      "the loss of the train batch  542  is :  3.2692577838897705\n",
      "the loss of the train batch  543  is :  3.2681081295013428\n",
      "the loss of the train batch  544  is :  3.2629263401031494\n",
      "the loss of the train batch  545  is :  3.2628586292266846\n",
      "the loss of the train batch  546  is :  3.265235185623169\n",
      "the loss of the train batch  547  is :  3.266322374343872\n",
      "the loss of the train batch  548  is :  3.259587287902832\n",
      "the loss of the train batch  549  is :  3.2558658123016357\n",
      "the loss of the train batch  550  is :  3.2479639053344727\n",
      "the loss of the train batch  551  is :  3.245438814163208\n",
      "the loss of the train batch  552  is :  3.242553234100342\n",
      "the loss of the train batch  553  is :  3.243061065673828\n",
      "the loss of the train batch  554  is :  3.2414166927337646\n",
      "the loss of the train batch  555  is :  3.239267349243164\n",
      "the loss of the train batch  556  is :  3.2459328174591064\n",
      "the loss of the train batch  557  is :  3.2529118061065674\n",
      "the loss of the train batch  558  is :  3.2384374141693115\n",
      "the loss of the train batch  559  is :  3.2449400424957275\n",
      "the loss of the train batch  560  is :  3.2439022064208984\n",
      "the loss of the train batch  561  is :  3.237882137298584\n",
      "the loss of the train batch  562  is :  3.2526400089263916\n",
      "the loss of the train batch  563  is :  3.2498974800109863\n",
      "the loss of the train batch  564  is :  3.241727590560913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss of the train batch  565  is :  3.2413547039031982\n",
      "the loss of the train batch  566  is :  3.244192361831665\n",
      "the loss of the train batch  567  is :  3.248594284057617\n",
      "the loss of the train batch  568  is :  3.249035358428955\n",
      "the loss of the train batch  569  is :  3.2521603107452393\n",
      "the loss of the train batch  570  is :  3.2416460514068604\n",
      "the loss of the train batch  571  is :  3.24820876121521\n",
      "the loss of the train batch  572  is :  3.2560057640075684\n",
      "the loss of the train batch  573  is :  3.2458336353302\n",
      "the loss of the train batch  574  is :  3.2476093769073486\n",
      "the loss of the train batch  575  is :  3.244934558868408\n",
      "the loss of the train batch  576  is :  3.2405786514282227\n",
      "the loss of the train batch  577  is :  3.2428605556488037\n",
      "the loss of the train batch  578  is :  3.2419116497039795\n",
      "the loss of the train batch  579  is :  3.237229585647583\n",
      "the loss of the train batch  580  is :  3.242180109024048\n",
      "the loss of the train batch  581  is :  3.243151903152466\n",
      "the loss of the train batch  582  is :  3.2536985874176025\n",
      "the loss of the train batch  583  is :  3.2474517822265625\n",
      "the loss of the train batch  584  is :  3.2482943534851074\n",
      "the loss of the train batch  585  is :  3.2404754161834717\n",
      "the loss of the train batch  586  is :  3.239426612854004\n",
      "the loss of the train batch  587  is :  3.2414660453796387\n",
      "the loss of the train batch  588  is :  3.2467117309570312\n",
      "the loss of the train batch  589  is :  3.241716146469116\n",
      "the loss of the train batch  590  is :  3.2479686737060547\n",
      "the loss of the train batch  591  is :  3.250168561935425\n",
      "the loss of the train batch  592  is :  3.241269111633301\n",
      "the loss of the train batch  593  is :  3.248469114303589\n",
      "the loss of the train batch  594  is :  3.2466201782226562\n",
      "the loss of the train batch  595  is :  3.2444519996643066\n",
      "the loss of the train batch  596  is :  3.235450506210327\n",
      "the loss of the train batch  597  is :  3.245103120803833\n",
      "the loss of the train batch  598  is :  3.2474944591522217\n",
      "the loss of the train batch  599  is :  3.24423885345459\n",
      "the loss of the train batch  600  is :  3.240894079208374\n",
      "the loss of the train batch  601  is :  3.2409112453460693\n",
      "the loss of the train batch  602  is :  3.242483377456665\n",
      "the loss of the train batch  603  is :  3.247236728668213\n",
      "the loss of the train batch  604  is :  3.2464611530303955\n",
      "the loss of the train batch  605  is :  3.2474660873413086\n",
      "the loss of the train batch  606  is :  3.249191999435425\n",
      "the loss of the train batch  607  is :  3.243643045425415\n",
      "the loss of the train batch  608  is :  3.238358497619629\n",
      "the loss of the train batch  609  is :  3.2477831840515137\n",
      "the loss of the train batch  610  is :  3.2443761825561523\n",
      "the loss of the train batch  611  is :  3.2400126457214355\n",
      "the loss of the train batch  612  is :  3.2440569400787354\n",
      "the loss of the train batch  613  is :  3.2504637241363525\n",
      "the loss of the train batch  614  is :  3.245425224304199\n",
      "the loss of the train batch  615  is :  3.241337537765503\n",
      "the loss of the train batch  616  is :  3.235720157623291\n",
      "the loss of the train batch  617  is :  3.243809461593628\n",
      "the loss of the train batch  618  is :  3.247657537460327\n",
      "the loss of the train batch  619  is :  3.239445924758911\n",
      "the loss of the train batch  620  is :  3.2446017265319824\n",
      "the loss of the train batch  621  is :  3.242522954940796\n",
      "the loss of the train batch  622  is :  3.242434024810791\n",
      "the loss of the train batch  623  is :  3.246129035949707\n",
      "the loss of the train batch  624  is :  3.2401018142700195\n",
      "the loss of the train batch  625  is :  3.2488784790039062\n",
      "the loss of the train batch  626  is :  3.2493860721588135\n",
      "the loss of the train batch  627  is :  3.236358642578125\n",
      "the loss of the train batch  628  is :  3.240692377090454\n",
      "the loss of the train batch  629  is :  3.2322700023651123\n",
      "the loss of the train batch  630  is :  3.233647584915161\n",
      "the loss of the train batch  631  is :  3.2374911308288574\n",
      "the loss of the train batch  632  is :  3.2455668449401855\n",
      "the loss of the train batch  633  is :  3.2368552684783936\n",
      "the loss of the train batch  634  is :  3.2371346950531006\n",
      "the loss of the train batch  635  is :  3.2363667488098145\n",
      "the loss of the train batch  636  is :  3.2388641834259033\n",
      "the loss of the train batch  637  is :  3.2350010871887207\n",
      "the loss of the train batch  638  is :  3.2467894554138184\n",
      "the loss of the train batch  639  is :  3.237734794616699\n",
      "the loss of the train batch  640  is :  3.2429277896881104\n",
      "the loss of the train batch  641  is :  3.2399141788482666\n",
      "the loss of the train batch  642  is :  3.235069513320923\n",
      "the loss of the train batch  643  is :  3.246598482131958\n",
      "the loss of the train batch  644  is :  3.234239101409912\n",
      "the loss of the train batch  645  is :  3.232747793197632\n",
      "the loss of the train batch  646  is :  3.2417452335357666\n",
      "the loss of the train batch  647  is :  3.239036798477173\n",
      "the loss of the train batch  648  is :  3.235722541809082\n",
      "the loss of the train batch  649  is :  3.2422075271606445\n",
      "the loss of the train batch  650  is :  3.2442855834960938\n",
      "the loss of the train batch  651  is :  3.241549491882324\n",
      "the loss of the train batch  652  is :  3.2361156940460205\n",
      "the loss of the train batch  653  is :  3.238312244415283\n",
      "the loss of the train batch  654  is :  3.2389400005340576\n",
      "the loss of the train batch  655  is :  3.2364280223846436\n",
      "the loss of the train batch  656  is :  3.2400734424591064\n",
      "the loss of the train batch  657  is :  3.233002185821533\n",
      "the loss of the train batch  658  is :  3.2447307109832764\n",
      "the loss of the train batch  659  is :  3.236720561981201\n",
      "the loss of the train batch  660  is :  3.2371623516082764\n",
      "the loss of the train batch  661  is :  3.255549669265747\n",
      "the loss of the train batch  662  is :  3.235738515853882\n",
      "the loss of the train batch  663  is :  3.2495810985565186\n",
      "the loss of the train batch  664  is :  3.2421324253082275\n",
      "the loss of the train batch  665  is :  3.2362685203552246\n",
      "the loss of the train batch  666  is :  3.2368786334991455\n",
      "the loss of the train batch  667  is :  3.2347702980041504\n",
      "the loss of the train batch  668  is :  3.2356019020080566\n",
      "the loss of the train batch  669  is :  3.2419025897979736\n",
      "the loss of the train batch  670  is :  3.2452476024627686\n",
      "the loss of the train batch  671  is :  3.2406599521636963\n",
      "the loss of the train batch  672  is :  3.244062900543213\n",
      "the loss of the train batch  673  is :  3.232985019683838\n",
      "the loss of the train batch  674  is :  3.2315919399261475\n",
      "the loss of the train batch  675  is :  3.246443271636963\n",
      "the loss of the train batch  676  is :  3.242515802383423\n",
      "the loss of the train batch  677  is :  3.2380211353302\n",
      "the loss of the train batch  678  is :  3.2389349937438965\n",
      "the loss of the train batch  679  is :  3.241304397583008\n",
      "the loss of the train batch  680  is :  3.2301740646362305\n",
      "the loss of the train batch  681  is :  3.2287654876708984\n",
      "the loss of the train batch  682  is :  3.242321252822876\n",
      "the loss of the train batch  683  is :  3.2558252811431885\n",
      "the loss of the train batch  684  is :  3.241135597229004\n",
      "the loss of the train batch  685  is :  3.249995470046997\n",
      "the loss of the train batch  686  is :  3.241358757019043\n",
      "the loss of the train batch  687  is :  3.233058452606201\n",
      "the loss of the train batch  688  is :  3.2278339862823486\n",
      "the loss of the train batch  689  is :  3.2419378757476807\n",
      "the loss of the train batch  690  is :  3.2441275119781494\n",
      "the loss of the train batch  691  is :  3.224536418914795\n",
      "the loss of the train batch  692  is :  3.224609375\n",
      "the loss of the train batch  693  is :  3.234433889389038\n",
      "the loss of the train batch  694  is :  3.2366943359375\n",
      "the loss of the train batch  695  is :  3.2372493743896484\n",
      "the loss of the train batch  696  is :  3.236952781677246\n",
      "the loss of the train batch  697  is :  3.2352709770202637\n",
      "the loss of the train batch  698  is :  3.2393686771392822\n",
      "the loss of the train batch  699  is :  3.2349345684051514\n",
      "the loss of the train batch  700  is :  3.243443012237549\n",
      "the loss of the train batch  701  is :  3.2356274127960205\n",
      "the loss of the train batch  702  is :  3.2327914237976074\n",
      "the loss of the train batch  703  is :  3.2470853328704834\n",
      "the loss of the train batch  704  is :  3.235903263092041\n",
      "the loss of the train batch  705  is :  3.241908550262451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss of the train batch  706  is :  3.242140769958496\n",
      "the loss of the train batch  707  is :  3.2326278686523438\n",
      "the loss of the train batch  708  is :  3.239206552505493\n",
      "the loss of the train batch  709  is :  3.2379119396209717\n",
      "the loss of the train batch  710  is :  3.2374565601348877\n",
      "the loss of the train batch  711  is :  3.2408199310302734\n",
      "the loss of the train batch  712  is :  3.230748176574707\n",
      "the loss of the train batch  713  is :  3.244701862335205\n",
      "the loss of the train batch  714  is :  3.2358930110931396\n",
      "the loss of the train batch  715  is :  3.242849588394165\n",
      "the loss of the train batch  716  is :  3.2404041290283203\n",
      "the loss of the train batch  717  is :  3.236936092376709\n",
      "the loss of the train batch  718  is :  3.236704111099243\n",
      "the loss of the train batch  719  is :  3.2351248264312744\n",
      "the loss of the train batch  720  is :  3.236184597015381\n",
      "the loss of the train batch  721  is :  3.2380311489105225\n",
      "the loss of the train batch  722  is :  3.24908185005188\n",
      "the loss of the train batch  723  is :  3.240365743637085\n",
      "the loss of the train batch  724  is :  3.2330684661865234\n",
      "the loss of the train batch  725  is :  3.2333755493164062\n",
      "the loss of the train batch  726  is :  3.247781276702881\n",
      "the loss of the train batch  727  is :  3.237276792526245\n",
      "the loss of the train batch  728  is :  3.235563278198242\n",
      "the loss of the train batch  729  is :  3.248894214630127\n",
      "the loss of the train batch  730  is :  3.235495090484619\n",
      "the loss of the train batch  731  is :  3.236940622329712\n",
      "the loss of the train batch  732  is :  3.2449679374694824\n",
      "the loss of the train batch  733  is :  3.234527111053467\n",
      "the loss of the train batch  734  is :  3.2401342391967773\n",
      "the loss of the train batch  735  is :  3.2351419925689697\n",
      "the loss of the train batch  736  is :  3.2402775287628174\n",
      "the loss of the train batch  737  is :  3.238131523132324\n",
      "the loss of the train batch  738  is :  3.243619918823242\n",
      "the loss of the train batch  739  is :  3.2267322540283203\n",
      "the loss of the train batch  740  is :  3.246379852294922\n",
      "the loss of the train batch  741  is :  3.239412784576416\n",
      "the loss of the train batch  742  is :  3.2228002548217773\n",
      "the loss of the train batch  743  is :  3.237469434738159\n",
      "the loss of the train batch  744  is :  3.2399046421051025\n",
      "the loss of the train batch  745  is :  3.2283003330230713\n",
      "the loss of the train batch  746  is :  3.2462680339813232\n",
      "the loss of the train batch  747  is :  3.2345991134643555\n",
      "the loss of the train batch  748  is :  3.237602472305298\n",
      "the loss of the train batch  749  is :  3.2366983890533447\n",
      "the loss of the train batch  750  is :  3.2238869667053223\n",
      "the loss of the train batch  751  is :  3.231788158416748\n",
      "the loss of the train batch  752  is :  3.2444310188293457\n",
      "the loss of the train batch  753  is :  3.235762119293213\n",
      "the loss of the train batch  754  is :  3.2347922325134277\n",
      "the loss of the train batch  755  is :  3.239651679992676\n",
      "the loss of the train batch  756  is :  3.2425739765167236\n",
      "the loss of the train batch  757  is :  3.238917589187622\n",
      "the loss of the train batch  758  is :  3.237499952316284\n",
      "the loss of the train batch  759  is :  3.2387094497680664\n",
      "the loss of the train batch  760  is :  3.238264799118042\n",
      "the loss of the train batch  761  is :  3.2310774326324463\n",
      "the loss of the train batch  762  is :  3.2412831783294678\n",
      "the loss of the train batch  763  is :  3.2364907264709473\n",
      "the loss of the train batch  764  is :  3.237971544265747\n",
      "the loss of the train batch  765  is :  3.2357053756713867\n",
      "the loss of the train batch  766  is :  3.2361185550689697\n",
      "the loss of the train batch  767  is :  3.2427918910980225\n",
      "the loss of the train batch  768  is :  3.234025239944458\n",
      "the loss of the train batch  769  is :  3.2291100025177\n",
      "the loss of the train batch  770  is :  3.2391605377197266\n",
      "the loss of the train batch  771  is :  3.2369415760040283\n",
      "the loss of the train batch  772  is :  3.234064817428589\n",
      "the loss of the train batch  773  is :  3.239835262298584\n",
      "the loss of the train batch  774  is :  3.246365785598755\n",
      "the loss of the train batch  775  is :  3.240065097808838\n",
      "the loss of the train batch  776  is :  3.238919258117676\n",
      "the loss of the train batch  777  is :  3.234187364578247\n",
      "the loss of the train batch  778  is :  3.2450501918792725\n",
      "the loss of the train batch  779  is :  3.2409324645996094\n",
      "the loss of the train batch  780  is :  3.236431837081909\n",
      "the loss of the train batch  781  is :  3.239145278930664\n",
      "the loss of the train batch  782  is :  3.234623432159424\n",
      "the loss of the train batch  783  is :  3.2469239234924316\n",
      "the loss of the train batch  784  is :  3.2471494674682617\n",
      "the loss of the train batch  785  is :  3.240325689315796\n",
      "the loss of the train batch  786  is :  3.2441632747650146\n",
      "the loss of the train batch  787  is :  3.245375633239746\n",
      "the loss of the train batch  788  is :  3.2327778339385986\n",
      "the loss of the train batch  789  is :  3.2349953651428223\n",
      "the loss of the train batch  790  is :  3.243800163269043\n",
      "the loss of the train batch  791  is :  3.2443270683288574\n",
      "the loss of the train batch  792  is :  3.2311434745788574\n",
      "the loss of the train batch  793  is :  3.24263334274292\n",
      "the loss of the train batch  794  is :  3.2380988597869873\n",
      "the loss of the train batch  795  is :  3.235485553741455\n",
      "the loss of the train batch  796  is :  3.2424674034118652\n",
      "the loss of the train batch  797  is :  3.2405779361724854\n",
      "the loss of the train batch  798  is :  3.239750385284424\n",
      "the loss of the train batch  799  is :  3.2372243404388428\n",
      "the loss of the train batch  800  is :  3.2267708778381348\n",
      "the loss of the train batch  801  is :  3.2302682399749756\n",
      "the loss of the train batch  802  is :  3.239701986312866\n",
      "the loss of the train batch  803  is :  3.2449915409088135\n",
      "the loss of the train batch  804  is :  3.2511210441589355\n",
      "the loss of the train batch  805  is :  3.2360315322875977\n",
      "the loss of the train batch  806  is :  3.245521306991577\n",
      "the loss of the train batch  807  is :  3.239354133605957\n",
      "the loss of the train batch  808  is :  3.2399208545684814\n",
      "the loss of the train batch  809  is :  3.236069679260254\n",
      "the loss of the train batch  810  is :  3.232609510421753\n",
      "the loss of the train batch  811  is :  3.239813804626465\n",
      "the loss of the train batch  812  is :  3.2363877296447754\n",
      "the loss of the train batch  813  is :  3.239370346069336\n",
      "the loss of the train batch  814  is :  3.2327542304992676\n",
      "the loss of the train batch  815  is :  3.2385549545288086\n",
      "the loss of the train batch  816  is :  3.230830192565918\n",
      "the loss of the train batch  817  is :  3.2354016304016113\n",
      "the loss of the train batch  818  is :  3.2319419384002686\n",
      "the loss of the train batch  819  is :  3.2415902614593506\n",
      "the loss of the train batch  820  is :  3.233952283859253\n",
      "the loss of the train batch  821  is :  3.239086627960205\n",
      "the loss of the train batch  822  is :  3.2362008094787598\n",
      "the loss of the train batch  823  is :  3.238039016723633\n",
      "the loss of the train batch  824  is :  3.240332841873169\n",
      "the loss of the train batch  825  is :  3.230553388595581\n",
      "the loss of the train batch  826  is :  3.236793041229248\n",
      "the loss of the train batch  827  is :  3.2382283210754395\n",
      "the loss of the train batch  828  is :  3.234239101409912\n",
      "the loss of the train batch  829  is :  3.2340199947357178\n",
      "the loss of the train batch  830  is :  3.242767095565796\n",
      "the loss of the train batch  831  is :  3.2404353618621826\n",
      "the loss of the train batch  832  is :  3.2393407821655273\n",
      "the loss of the train batch  833  is :  3.2324986457824707\n",
      "the loss of the train batch  834  is :  3.242126703262329\n",
      "the loss of the train batch  835  is :  3.226989507675171\n",
      "the loss of the train batch  836  is :  3.2372970581054688\n",
      "the loss of the train batch  837  is :  3.238691806793213\n",
      "the loss of the train batch  838  is :  3.2380495071411133\n",
      "the loss of the train batch  839  is :  3.2412962913513184\n",
      "the loss of the train batch  840  is :  3.252424955368042\n",
      "the loss of the train batch  841  is :  3.2466628551483154\n",
      "the loss of the train batch  842  is :  3.236072063446045\n",
      "the loss of the train batch  843  is :  3.2340216636657715\n",
      "the loss of the train batch  844  is :  3.242835760116577\n",
      "the loss of the train batch  845  is :  3.23742413520813\n",
      "the loss of the train batch  846  is :  3.223167657852173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss of the train batch  847  is :  3.2417359352111816\n",
      "the loss of the train batch  848  is :  3.238518238067627\n",
      "the loss of the train batch  849  is :  3.2455036640167236\n",
      "the loss of the train batch  850  is :  3.237635850906372\n",
      "the loss of the train batch  851  is :  3.2430624961853027\n",
      "the loss of the train batch  852  is :  3.2342371940612793\n",
      "the loss of the train batch  853  is :  3.2419254779815674\n",
      "the loss of the train batch  854  is :  3.229968309402466\n",
      "the loss of the train batch  855  is :  3.2341103553771973\n",
      "the loss of the train batch  856  is :  3.231009006500244\n",
      "the loss of the train batch  857  is :  3.234459400177002\n",
      "the loss of the train batch  858  is :  3.23561429977417\n",
      "the loss of the train batch  859  is :  3.2426810264587402\n",
      "the loss of the train batch  860  is :  3.237022876739502\n",
      "the loss of the train batch  861  is :  3.244290590286255\n",
      "the loss of the train batch  862  is :  3.2437615394592285\n",
      "the loss of the train batch  863  is :  3.2384042739868164\n",
      "the loss of the train batch  864  is :  3.2401816844940186\n",
      "the loss of the train batch  865  is :  3.235044240951538\n",
      "the loss of the train batch  866  is :  3.229520797729492\n",
      "the loss of the train batch  867  is :  3.236417770385742\n",
      "the loss of the train batch  868  is :  3.2348501682281494\n",
      "the loss of the train batch  869  is :  3.242638349533081\n",
      "the loss of the train batch  870  is :  3.244342803955078\n",
      "the loss of the train batch  871  is :  3.2312912940979004\n",
      "the loss of the train batch  872  is :  3.2302849292755127\n",
      "the loss of the train batch  873  is :  3.248966693878174\n",
      "the loss of the train batch  874  is :  3.243675708770752\n",
      "the loss of the train batch  875  is :  3.2376902103424072\n",
      "the loss of the train batch  876  is :  3.2357211112976074\n",
      "the loss of the train batch  877  is :  3.232771158218384\n",
      "the loss of the train batch  878  is :  3.2350049018859863\n",
      "the loss of the train batch  879  is :  3.244481325149536\n",
      "the loss of the train batch  880  is :  3.2436461448669434\n",
      "the loss of the train batch  881  is :  3.237644910812378\n",
      "the loss of the train batch  882  is :  3.2367944717407227\n",
      "the loss of the train batch  883  is :  3.2405340671539307\n",
      "the loss of the train batch  884  is :  3.229135513305664\n",
      "the loss of the train batch  885  is :  3.238010883331299\n",
      "the loss of the train batch  886  is :  3.240830183029175\n",
      "the loss of the train batch  887  is :  3.2300260066986084\n",
      "the loss of the train batch  888  is :  3.2367212772369385\n",
      "the loss of the train batch  889  is :  3.231384515762329\n",
      "the loss of the train batch  890  is :  3.2355449199676514\n",
      "the loss of the train batch  891  is :  3.2303216457366943\n",
      "the loss of the train batch  892  is :  3.24135422706604\n",
      "the loss of the train batch  893  is :  3.229600429534912\n",
      "the loss of the train batch  894  is :  3.241347074508667\n",
      "the loss of the train batch  895  is :  3.238511800765991\n",
      "the loss of the train batch  896  is :  3.2422609329223633\n",
      "the loss of the train batch  897  is :  3.2504990100860596\n",
      "the loss of the train batch  898  is :  3.2356321811676025\n",
      "the loss of the train batch  899  is :  3.243478775024414\n",
      "the loss of the train batch  900  is :  3.234435558319092\n",
      "the loss of the train batch  901  is :  3.238016128540039\n",
      "the loss of the train batch  902  is :  3.2348012924194336\n",
      "the loss of the train batch  903  is :  3.242037057876587\n",
      "the loss of the train batch  904  is :  3.240816593170166\n",
      "the loss of the train batch  905  is :  3.2295467853546143\n",
      "the loss of the train batch  906  is :  3.236797332763672\n",
      "the loss of the train batch  907  is :  3.2386186122894287\n",
      "the loss of the train batch  908  is :  3.2318766117095947\n",
      "the loss of the train batch  909  is :  3.2433462142944336\n",
      "the loss of the train batch  910  is :  3.233139753341675\n",
      "the loss of the train batch  911  is :  3.2403039932250977\n",
      "the loss of the train batch  912  is :  3.234286069869995\n",
      "the loss of the train batch  913  is :  3.2390191555023193\n",
      "the loss of the train batch  914  is :  3.234736680984497\n",
      "the loss of the train batch  915  is :  3.2370312213897705\n",
      "the loss of the train batch  916  is :  3.239835500717163\n",
      "the loss of the train batch  917  is :  3.2365100383758545\n",
      "the loss of the train batch  918  is :  3.235004186630249\n",
      "the loss of the train batch  919  is :  3.2294368743896484\n",
      "the loss of the train batch  920  is :  3.241727828979492\n",
      "the loss of the train batch  921  is :  3.232792377471924\n",
      "the loss of the train batch  922  is :  3.2324459552764893\n",
      "the loss of the train batch  923  is :  3.236034393310547\n",
      "the loss of the train batch  924  is :  3.241637706756592\n",
      "the loss of the train batch  925  is :  3.237924337387085\n",
      "the loss of the train batch  926  is :  3.243274211883545\n",
      "the loss of the train batch  927  is :  3.2268693447113037\n",
      "the loss of the train batch  928  is :  3.241718292236328\n",
      "the loss of the train batch  929  is :  3.237748622894287\n",
      "the loss of the train batch  930  is :  3.245347738265991\n",
      "the loss of the train batch  931  is :  3.2355611324310303\n",
      "the loss of the train batch  932  is :  3.2407712936401367\n",
      "the loss of the train batch  933  is :  3.2485897541046143\n",
      "the loss of the train batch  934  is :  3.2326979637145996\n",
      "the loss of the train batch  935  is :  3.235539197921753\n",
      "the loss of the train batch  936  is :  3.230882406234741\n",
      "the loss of the train batch  937  is :  3.238276720046997\n",
      "the loss of the train batch  938  is :  3.236814022064209\n",
      "the loss of the train batch  939  is :  3.235060691833496\n",
      "the loss of the train batch  940  is :  3.2382261753082275\n",
      "the loss of the train batch  941  is :  3.244809150695801\n",
      "the loss of the train batch  942  is :  3.2316505908966064\n",
      "the loss of the train batch  943  is :  3.231985330581665\n",
      "the loss of the train batch  944  is :  3.2414462566375732\n",
      "the loss of the train batch  945  is :  3.229968547821045\n",
      "the loss of the train batch  946  is :  3.23677396774292\n",
      "the loss of the train batch  947  is :  3.223452091217041\n",
      "the loss of the train batch  948  is :  3.234832525253296\n",
      "the loss of the train batch  949  is :  3.2347593307495117\n",
      "the loss of the train batch  950  is :  3.233553647994995\n",
      "the loss of the train batch  951  is :  3.2458736896514893\n",
      "the loss of the train batch  952  is :  3.2369253635406494\n",
      "the loss of the train batch  953  is :  3.2358195781707764\n",
      "the loss of the train batch  954  is :  3.2228143215179443\n",
      "the loss of the train batch  955  is :  3.242122173309326\n",
      "the loss of the train batch  956  is :  3.256542444229126\n",
      "the loss of the train batch  957  is :  3.2366440296173096\n",
      "the loss of the train batch  958  is :  3.2453789710998535\n",
      "the loss of the train batch  959  is :  3.2337286472320557\n",
      "the loss of the train batch  960  is :  3.2280406951904297\n",
      "the loss of the train batch  961  is :  3.231720447540283\n",
      "the loss of the train batch  962  is :  3.222620725631714\n",
      "the loss of the train batch  963  is :  3.2246322631835938\n",
      "the loss of the train batch  964  is :  3.2423593997955322\n",
      "the loss of the train batch  965  is :  3.2246596813201904\n",
      "the loss of the train batch  966  is :  3.2464966773986816\n",
      "the loss of the train batch  967  is :  3.2359516620635986\n",
      "the loss of the train batch  968  is :  3.2402164936065674\n",
      "the loss of the train batch  969  is :  3.235522985458374\n",
      "the loss of the train batch  970  is :  3.2291269302368164\n",
      "the loss of the train batch  971  is :  3.2245051860809326\n",
      "the loss of the train batch  972  is :  3.2306020259857178\n",
      "the loss of the train batch  973  is :  3.2392539978027344\n",
      "the loss of the train batch  974  is :  3.2289202213287354\n",
      "the loss of the train batch  975  is :  3.2393391132354736\n",
      "the loss of the train batch  976  is :  3.2353975772857666\n",
      "the loss of the train batch  977  is :  3.2495944499969482\n",
      "the loss of the train batch  978  is :  3.2415339946746826\n",
      "the loss of the train batch  979  is :  3.238062620162964\n",
      "the loss of the train batch  980  is :  3.2427828311920166\n",
      "the loss of the train batch  981  is :  3.236537218093872\n",
      "the loss of the train batch  982  is :  3.246392250061035\n",
      "the loss of the train batch  983  is :  3.237009048461914\n",
      "the loss of the train batch  984  is :  3.239713430404663\n",
      "the loss of the train batch  985  is :  3.2343523502349854\n",
      "the loss of the train batch  986  is :  3.233635187149048\n",
      "the loss of the train batch  987  is :  3.231832265853882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss of the train batch  988  is :  3.246032476425171\n",
      "the loss of the train batch  989  is :  3.238886594772339\n",
      "the loss of the train batch  990  is :  3.239346742630005\n",
      "the loss of the train batch  991  is :  3.2360587120056152\n",
      "the loss of the train batch  992  is :  3.235269069671631\n",
      "the loss of the train batch  993  is :  3.231198787689209\n",
      "the loss of the train batch  994  is :  3.2397282123565674\n",
      "the loss of the train batch  995  is :  3.236402750015259\n",
      "the loss of the train batch  996  is :  3.233729839324951\n",
      "the loss of the train batch  997  is :  3.2345457077026367\n",
      "the loss of the train batch  998  is :  3.232931613922119\n",
      "the loss of the train batch  999  is :  3.2349441051483154\n",
      "the loss of the train batch  1000  is :  3.244171142578125\n",
      "the loss of the train batch  1001  is :  3.229979991912842\n",
      "the loss of the train batch  1002  is :  3.230489492416382\n",
      "the loss of the train batch  1003  is :  3.233675479888916\n",
      "the loss of the train batch  1004  is :  3.2387053966522217\n",
      "the loss of the train batch  1005  is :  3.2401325702667236\n",
      "the loss of the train batch  1006  is :  3.2332825660705566\n",
      "the loss of the train batch  1007  is :  3.223435163497925\n",
      "the loss of the train batch  1008  is :  3.224431276321411\n",
      "the loss of the train batch  1009  is :  3.2445859909057617\n",
      "the loss of the train batch  1010  is :  3.2287468910217285\n",
      "the loss of the train batch  1011  is :  3.238341808319092\n",
      "the loss of the train batch  1012  is :  3.2281200885772705\n",
      "the loss of the train batch  1013  is :  3.2385499477386475\n",
      "the loss of the train batch  1014  is :  3.226740598678589\n",
      "the loss of the train batch  1015  is :  3.2305057048797607\n",
      "the loss of the train batch  1016  is :  3.237457513809204\n",
      "the loss of the train batch  1017  is :  3.2357177734375\n",
      "the loss of the train batch  1018  is :  3.2364728450775146\n",
      "the loss of the train batch  1019  is :  3.2345235347747803\n",
      "the loss of the train batch  1020  is :  3.2452666759490967\n",
      "the loss of the train batch  1021  is :  3.239189386367798\n",
      "the loss of the train batch  1022  is :  3.2429068088531494\n",
      "the loss of the train batch  1023  is :  3.230201244354248\n",
      "the loss of the train batch  1024  is :  3.2336161136627197\n",
      "the loss of the train batch  1025  is :  3.238805055618286\n",
      "the loss of the train batch  1026  is :  3.2374396324157715\n",
      "the loss of the train batch  1027  is :  3.2475883960723877\n",
      "the loss of the train batch  1028  is :  3.243360996246338\n",
      "the loss of the train batch  1029  is :  3.2301087379455566\n",
      "the loss of the train batch  1030  is :  3.243375778198242\n",
      "the loss of the train batch  1031  is :  3.23836612701416\n",
      "the loss of the train batch  1032  is :  3.2323546409606934\n",
      "the loss of the train batch  1033  is :  3.2359790802001953\n",
      "the loss of the train batch  1034  is :  3.239450216293335\n",
      "the loss of the train batch  1035  is :  3.251060724258423\n",
      "the loss of the train batch  1036  is :  3.2430806159973145\n",
      "the loss of the train batch  1037  is :  3.244597911834717\n",
      "the loss of the train batch  1038  is :  3.2341432571411133\n",
      "the loss of the train batch  1039  is :  3.2295048236846924\n",
      "the loss of the train batch  1040  is :  3.237920045852661\n",
      "the loss of the train batch  1041  is :  3.2376513481140137\n",
      "the loss of the train batch  1042  is :  3.2428066730499268\n",
      "the loss of the train batch  1043  is :  3.230360507965088\n",
      "the loss of the train batch  1044  is :  3.231750965118408\n",
      "the loss of the train batch  1045  is :  3.2343921661376953\n",
      "the loss of the train batch  1046  is :  3.2434771060943604\n",
      "the loss of the train batch  1047  is :  3.2446770668029785\n",
      "the loss of the train batch  1048  is :  3.24723744392395\n",
      "the loss of the train batch  1049  is :  3.243607997894287\n",
      "the loss of the train batch  1050  is :  3.2408199310302734\n",
      "the loss of the train batch  1051  is :  3.237039566040039\n",
      "the loss of the train batch  1052  is :  3.234753370285034\n",
      "the loss of the train batch  1053  is :  3.2253153324127197\n",
      "the loss of the train batch  1054  is :  3.232708692550659\n",
      "the loss of the train batch  1055  is :  3.233708620071411\n",
      "the loss of the train batch  1056  is :  3.238121509552002\n",
      "the loss of the train batch  1057  is :  3.2335638999938965\n",
      "the loss of the train batch  1058  is :  3.2282540798187256\n",
      "the loss of the train batch  1059  is :  3.2426133155822754\n",
      "the loss of the train batch  1060  is :  3.236219882965088\n",
      "the loss of the train batch  1061  is :  3.2345237731933594\n",
      "the loss of the train batch  1062  is :  3.2350668907165527\n",
      "the loss of the train batch  1063  is :  3.236111640930176\n",
      "the loss of the train batch  1064  is :  3.234575033187866\n",
      "the loss of the train batch  1065  is :  3.244917154312134\n",
      "the loss of the train batch  1066  is :  3.2313594818115234\n",
      "the loss of the train batch  1067  is :  3.24023699760437\n",
      "the loss of the train batch  1068  is :  3.2276251316070557\n",
      "the loss of the train batch  1069  is :  3.2334890365600586\n",
      "the loss of the train batch  1070  is :  3.2415599822998047\n",
      "the loss of the train batch  1071  is :  3.228149890899658\n",
      "the loss of the train batch  1072  is :  3.2386410236358643\n",
      "the loss of the train batch  1073  is :  3.235111713409424\n",
      "the loss of the train batch  1074  is :  3.237961769104004\n",
      "the loss of the train batch  1075  is :  3.241191864013672\n",
      "the loss of the train batch  1076  is :  3.2292072772979736\n",
      "the loss of the train batch  1077  is :  3.2329742908477783\n",
      "the loss of the train batch  1078  is :  3.2438764572143555\n",
      "the loss of the train batch  1079  is :  3.2315587997436523\n",
      "the loss of the train batch  1080  is :  3.2383859157562256\n",
      "the loss of the train batch  1081  is :  3.243685245513916\n",
      "the loss of the train batch  1082  is :  3.2415034770965576\n",
      "the loss of the train batch  1083  is :  3.2418222427368164\n",
      "the loss of the train batch  1084  is :  3.2323648929595947\n",
      "the loss of the train batch  1085  is :  3.233062982559204\n",
      "the loss of the train batch  1086  is :  3.2352185249328613\n",
      "the loss of the train batch  1087  is :  3.227418899536133\n",
      "the loss of the train batch  1088  is :  3.238759994506836\n",
      "the loss of the train batch  1089  is :  3.2355215549468994\n",
      "the loss of the train batch  1090  is :  3.231844186782837\n",
      "the loss of the train batch  1091  is :  3.227278232574463\n",
      "the loss of the train batch  1092  is :  3.2366364002227783\n",
      "the loss of the train batch  1093  is :  3.242966651916504\n",
      "the loss of the train batch  1094  is :  3.2376418113708496\n",
      "the loss of the train batch  1095  is :  3.231861114501953\n",
      "the loss of the train batch  1096  is :  3.238185405731201\n",
      "the loss of the train batch  1097  is :  3.236696481704712\n",
      "the loss of the train batch  1098  is :  3.232149600982666\n",
      "the loss of the train batch  1099  is :  3.2387545108795166\n",
      "the loss of the train batch  1100  is :  3.231234073638916\n",
      "the loss of the train batch  1101  is :  3.2357277870178223\n",
      "the loss of the train batch  1102  is :  3.234353542327881\n",
      "the loss of the train batch  1103  is :  3.237856864929199\n",
      "the loss of the train batch  1104  is :  3.223810911178589\n",
      "the loss of the train batch  1105  is :  3.238269329071045\n",
      "the loss of the train batch  1106  is :  3.231775999069214\n",
      "the loss of the train batch  1107  is :  3.245319366455078\n",
      "the loss of the train batch  1108  is :  3.230860710144043\n",
      "the loss of the train batch  1109  is :  3.230377674102783\n",
      "the loss of the train batch  1110  is :  3.24040150642395\n",
      "the loss of the train batch  1111  is :  3.2333998680114746\n",
      "the loss of the train batch  1112  is :  3.230699062347412\n",
      "the loss of the train batch  1113  is :  3.2377877235412598\n",
      "the loss of the train batch  1114  is :  3.240424871444702\n",
      "the loss of the train batch  1115  is :  3.2377092838287354\n",
      "the loss of the train batch  1116  is :  3.232754707336426\n",
      "the loss of the train batch  1117  is :  3.2401225566864014\n",
      "the loss of the train batch  1118  is :  3.241100788116455\n",
      "the loss of the train batch  1119  is :  3.240901231765747\n",
      "the loss of the train batch  1120  is :  3.243030071258545\n",
      "the loss of the train batch  1121  is :  3.241274833679199\n",
      "the loss of the train batch  1122  is :  3.24192476272583\n",
      "the loss of the train batch  1123  is :  3.2370994091033936\n",
      "the loss of the train batch  1124  is :  3.2354304790496826\n",
      "the loss of the train batch  1125  is :  3.2406740188598633\n",
      "the loss of the train batch  1126  is :  3.232771873474121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss of the train batch  1127  is :  3.235210418701172\n",
      "the loss of the train batch  1128  is :  3.2365193367004395\n",
      "the loss of the train batch  1129  is :  3.233816623687744\n",
      "the loss of the train batch  1130  is :  3.2407968044281006\n",
      "the loss of the train batch  1131  is :  3.2354209423065186\n",
      "the loss of the train batch  1132  is :  3.2280120849609375\n",
      "the loss of the train batch  1133  is :  3.2375595569610596\n",
      "the loss of the train batch  1134  is :  3.2341740131378174\n",
      "the loss of the train batch  1135  is :  3.2384166717529297\n",
      "the loss of the train batch  1136  is :  3.2426998615264893\n",
      "the loss of the train batch  1137  is :  3.2467851638793945\n",
      "the loss of the train batch  1138  is :  3.23193621635437\n",
      "the loss of the train batch  1139  is :  3.2418057918548584\n",
      "the loss of the train batch  1140  is :  3.237238883972168\n",
      "the loss of the train batch  1141  is :  3.2357468605041504\n",
      "the loss of the train batch  1142  is :  3.239203691482544\n",
      "the loss of the train batch  1143  is :  3.2349722385406494\n",
      "the loss of the train batch  1144  is :  3.234290599822998\n",
      "the loss of the train batch  1145  is :  3.24125599861145\n",
      "the loss of the train batch  1146  is :  3.254293441772461\n",
      "the loss of the train batch  1147  is :  3.2617084980010986\n",
      "the loss of the train batch  1148  is :  3.251437187194824\n",
      "the loss of the train batch  1149  is :  3.237708568572998\n",
      "the loss of the train batch  1150  is :  3.235706090927124\n",
      "the loss of the train batch  1151  is :  3.2278594970703125\n",
      "the loss of the train batch  1152  is :  3.2391979694366455\n",
      "the loss of the train batch  1153  is :  3.2431092262268066\n",
      "the loss of the train batch  1154  is :  3.242619037628174\n",
      "the loss of the train batch  1155  is :  3.24333119392395\n",
      "the loss of the train batch  1156  is :  3.231154203414917\n",
      "the loss of the train batch  1157  is :  3.2376601696014404\n",
      "the loss of the train batch  1158  is :  3.233961343765259\n",
      "the loss of the train batch  1159  is :  3.234203338623047\n",
      "the loss of the train batch  1160  is :  3.2347073554992676\n",
      "the loss of the train batch  1161  is :  3.235764503479004\n",
      "the loss of the train batch  1162  is :  3.2400050163269043\n",
      "the loss of the train batch  1163  is :  3.2433104515075684\n",
      "the loss of the train batch  1164  is :  3.2400777339935303\n",
      "the loss of the train batch  1165  is :  3.231924057006836\n",
      "the loss of the train batch  1166  is :  3.237323760986328\n",
      "the loss of the train batch  1167  is :  3.229975938796997\n",
      "the loss of the train batch  1168  is :  3.237605333328247\n",
      "the loss of the train batch  1169  is :  3.2424283027648926\n",
      "the loss of the train batch  1170  is :  3.2410943508148193\n",
      "the loss of the train batch  1171  is :  3.2381207942962646\n",
      "the loss of the train batch  1172  is :  3.2344672679901123\n",
      "the loss of the train batch  1173  is :  3.2346251010894775\n",
      "the loss of the train batch  1174  is :  3.225860834121704\n",
      "the loss of the train batch  1175  is :  3.2391786575317383\n",
      "the loss of the train batch  1176  is :  3.235006332397461\n",
      "the loss of the train batch  1177  is :  3.244950532913208\n",
      "the loss of the train batch  1178  is :  3.2396483421325684\n",
      "the loss of the train batch  1179  is :  3.2349326610565186\n",
      "the loss of the train batch  1180  is :  3.239607572555542\n",
      "the loss of the train batch  1181  is :  3.2422633171081543\n",
      "the loss of the train batch  1182  is :  3.235818862915039\n",
      "the loss of the train batch  1183  is :  3.2302045822143555\n",
      "the loss of the train batch  1184  is :  3.236734628677368\n",
      "the loss of the train batch  1185  is :  3.226264476776123\n",
      "the loss of the train batch  1186  is :  3.2283568382263184\n",
      "the loss of the train batch  1187  is :  3.2381153106689453\n",
      "the loss of the train batch  1188  is :  3.239935874938965\n",
      "the loss of the train batch  1189  is :  3.235976457595825\n",
      "the loss of the train batch  1190  is :  3.229910373687744\n",
      "the loss of the train batch  1191  is :  3.2317941188812256\n",
      "the loss of the train batch  1192  is :  3.23515248298645\n",
      "the loss of the train batch  1193  is :  3.2323503494262695\n",
      "the loss of the train batch  1194  is :  3.229994297027588\n",
      "the loss of the train batch  1195  is :  3.229614734649658\n",
      "the loss of the train batch  1196  is :  3.238100051879883\n",
      "the loss of the train batch  1197  is :  3.2384629249572754\n",
      "the loss of the train batch  1198  is :  3.244577169418335\n",
      "the loss of the train batch  1199  is :  3.240419387817383\n",
      "the loss of the train batch  1200  is :  3.244063377380371\n",
      "the loss of the train batch  1201  is :  3.2293195724487305\n",
      "the loss of the train batch  1202  is :  3.2347829341888428\n",
      "the loss of the train batch  1203  is :  3.240334987640381\n",
      "the loss of the train batch  1204  is :  3.2370831966400146\n",
      "the loss of the train batch  1205  is :  3.2385544776916504\n",
      "the loss of the train batch  1206  is :  3.241199493408203\n",
      "the loss of the train batch  1207  is :  3.2376513481140137\n",
      "the loss of the train batch  1208  is :  3.2408764362335205\n",
      "the loss of the train batch  1209  is :  3.2393157482147217\n",
      "the loss of the train batch  1210  is :  3.232541084289551\n",
      "the loss of the train batch  1211  is :  3.2288734912872314\n",
      "the loss of the train batch  1212  is :  3.2347586154937744\n",
      "the loss of the train batch  1213  is :  3.23911714553833\n",
      "the loss of the train batch  1214  is :  3.2271175384521484\n",
      "the loss of the train batch  1215  is :  3.235015869140625\n"
     ]
    }
   ],
   "source": [
    "mod = model(d, 256, 64 , 256 , 6 , 0.2 ,0.35, 0.0001)\n",
    "res = mod.fit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ecbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b975bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d4d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e80414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.array(res)\n",
    "plt.figure()\n",
    "x = list(range(5))\n",
    "plt.plot(x, res[:,0])\n",
    "plt.plot(x, res[:,1])\n",
    "plt.title(\"evolution of loss with epochs\")\n",
    "plt.xlabel('epochs ')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df568b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b231012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849be848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c16f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68cfd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5231259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of a paper : \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch \n",
    "import string\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "\n",
    "class model(nn.Module): \n",
    "    \n",
    "    def __init__(self, data, batch_size ,embedding_size, hidden_size, num_layers, teacher_forcing_ratio):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''\n",
    "        our input data are arabic words with their roots, with the hypothesis that each word has it's own root.\n",
    "        \n",
    "        (our dataset for this is named root_data)\n",
    "        '''\n",
    "        self.sow = '$' # the start of root character \n",
    "        self.eow = '£' # the end of root character\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab = self.extract_dict()\n",
    "        self.char_index_dic = self.char_to_index()\n",
    "        print(self.char_index_dic)\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(num_embeddings = len(self.vocab), embedding_dim = self.embedding_size) \n",
    "        self.bigru = nn.GRU(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=self.num_layers, bidirectional=True)\n",
    "        self.gru = nn.GRU(input_size= self.embedding_size ,hidden_size = self.hidden_size, num_layers = self.num_layers)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.Linear = nn.Linear(self.hidden_size,len(self.vocab))\n",
    "        self.optimizer = optim.Adam([*self.bigru.parameters(), *self.gru.parameters(), *self.Linear.parameters()], lr = 0.1)\n",
    "\n",
    "    \n",
    "    def extract_dict(self) :\n",
    "        '''\n",
    "        this function extracts all the unique characters from the given dataset\n",
    "        '''\n",
    "        dictt = []\n",
    "        for word in self.data :\n",
    "            for item in word : \n",
    "                tmp = set(item)\n",
    "                for k in tmp : \n",
    "                    if k not in dictt : \n",
    "                        dictt.append(k)\n",
    "        return dictt \n",
    "    \n",
    "    \n",
    "    # Let's now construct our model : \n",
    "    \n",
    "    # we should think about character embeddings in order to create an embeded matrix for each word\n",
    "    \n",
    "    def char_to_index(self):\n",
    "        '''\n",
    "        this function creates unique indexes of each character\n",
    "        '''\n",
    "        char_to_idx_map = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        return char_to_idx_map  \n",
    "    \n",
    "    def data_batches(self): \n",
    "        size = self.batch_size\n",
    "        \n",
    "        batches = [self.data[i:i + size] for i in range(0, len(self.data), size)]\n",
    "        \n",
    "        return batches\n",
    "        \n",
    "    \n",
    "    def word_to_idx_seq(self, word):\n",
    "        '''\n",
    "        this function returns a sequence of the unique indexes for the given word \n",
    "        (sequence is tensor that can be changed using a .tolist() )\n",
    "        '''\n",
    "        word_char_idx_tensor = torch.tensor([self.char_index_dic[char] for char in word])        \n",
    "        return word_char_idx_tensor # word sequence\n",
    "    \n",
    "    def word_to_seq(self, word):\n",
    "        '''\n",
    "        this function returns a sequence of the unique indexes for the given word \n",
    "        (sequence is tensor that can be changed using a .tolist() )\n",
    "        '''\n",
    "        word_char_idx_tensor =[self.char_index_dic[char] for char in word]    \n",
    "        return word_char_idx_tensor # word sequence\n",
    "    \n",
    "    \n",
    "    def encode_word(self, word):\n",
    "        \n",
    "            \n",
    "        # we create embedding of the sequence : \n",
    "        word_seq = self.word_to_idx_seq(word)\n",
    "        embedded_vec = self.embedding(word_seq)\n",
    "        \n",
    "        outputs, hidden = self.bigru(embedded_vec) # we pass the emebedded vector through the bi-GRU \n",
    "\n",
    "        \n",
    "        '''\n",
    "        kaynin two cases : \n",
    "        \n",
    "             case1 :  we work on the outputs  ==> chosen\n",
    "             case2 :  we work on the final hidden state ==> discarted \n",
    "            \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        '''test_hidden = torch.flatten(hidden)\n",
    "        final_hidden = torch.unsqueeze(test_hidden, 0)'''\n",
    "        \n",
    "        \n",
    "        # can also be outputs.\n",
    "        \n",
    "        encoder_output = torch.mean(hidden , dim=0)  # Average the hidden vectors across all time steps. Shape: (hidden_size*2,) if bidirectional, else (hidden_size,)\n",
    "        \n",
    "        final_output = torch.unsqueeze(encoder_output, 0)\n",
    "               \n",
    "        \n",
    "        return final_output\n",
    "    \n",
    "    \n",
    "    def decode_word(self, encoding, character):\n",
    "\n",
    "        \n",
    "        '''\n",
    "        encoding : output of the encoder network. \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        seq = self.word_to_idx_seq(self.sow)\n",
    "        embedded_sow= self.embedding(self.word_to_idx_seq(character))   # starts with self.sow\n",
    "\n",
    "        input_size = embedded_sow.size(1)\n",
    "\n",
    "        hidden_size = encoding.size(1)\n",
    "                 \n",
    "        dec_out , dec_hidden = self.gru(embedded_sow,encoding)\n",
    "                \n",
    "        a = self.Linear(dec_out)\n",
    "        m = nn.Softmax(dim = 1)\n",
    "        \n",
    "        output = m(a)\n",
    "        \n",
    "        top1 = output.argmax(1)[0].item()\n",
    "        \n",
    "        key_list = list(self.char_index_dic.keys())\n",
    "        val_list = list(self.char_index_dic.values())\n",
    "\n",
    "        position = val_list.index(top1)\n",
    "        \n",
    "        result_char = key_list[position]\n",
    "\n",
    "        return result_char, dec_hidden , top1\n",
    "        \n",
    "        \n",
    "    def train_batch(self, batch):\n",
    "                \n",
    "        \n",
    "        for instance in batch: \n",
    "            '''\n",
    "            word = instance[0] : the word to extract root from\n",
    "            target_word = instance[1] : the root of the word\n",
    "            '''\n",
    "            word = instance[0]\n",
    "            target_root = instance[1]\n",
    "            \n",
    "            hidd = self.encode_word(word)\n",
    "            predicted_root = []\n",
    "            original_root_seq = self.word_to_idx_seq(target_root)\n",
    "            predicted_root_seq = []\n",
    "            res_char = target_root[0]\n",
    "            for char in target_root : \n",
    "                if len(predicted_root) == len(target_root):\n",
    "                    break\n",
    "                \n",
    "                \n",
    "                if random.random() < self.teacher_forcing_ratio : \n",
    "                    res_char = char\n",
    "                \n",
    "                res_char , hidd , idx = self.decode_word(hidd, res_char)\n",
    "                predicted_root_seq.append(idx)\n",
    "                test = hidd\n",
    "                predicted_root.append(res_char)\n",
    "\n",
    "            predicted_root = ''.join(predicted_root)\n",
    "\n",
    "            predicted_root_seq = torch.tensor(predicted_root_seq)\n",
    "\n",
    "            org = self.embedding(original_root_seq)\n",
    "            pred = self.embedding(predicted_root_seq)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(org, pred)\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            print('target root :', target_root)\n",
    "            print('predicted root : ', predicted_root)\n",
    "\n",
    "            print('the loss : ', loss.item())        \n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        batches = self.data_batches()\n",
    "        \n",
    "        for batch in batches : \n",
    "            \n",
    "            self.train_batch(batch)\n",
    "\n",
    "        #print(em1)\n",
    "        \n",
    "        \n",
    "\n",
    "mod = model(data_root, 100, 50, 100, 1, 0.9)\n",
    "mod.train()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b751a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3769b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c8969b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710504b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc375018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ad99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b8531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a2290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e94deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee58704c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaad5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417ec90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d5ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f1ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb81318a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c788cfd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160fffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0bd253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b448e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77a906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd7ee52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3429b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed2aac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2619f608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bea57e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e852de90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27004d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80bedd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ef987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b25293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f63a82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
