{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ea0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c89e9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 05:41:23.308434: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-17 05:41:23.308471: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense , Activation , Dropout, Embedding, SpatialDropout1D, LSTM , TextVectorization, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f7603ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb1349c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50137a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'corpus_morphological_analysis'\n",
    "file_paths = []\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    file_paths.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c5f5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file_paths = file_paths[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2602e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "#i = 0\n",
    "for filepath in temp_file_paths :\n",
    "    #print(i)\n",
    "    with open(filepath, encoding='utf-8') as f :\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()  \n",
    "    text = soup.get_text()\n",
    "    content.append(text)\n",
    "    #i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d7aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "104b5ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35a75b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_list = []\n",
    "for item in content : \n",
    "    tmp = item.splitlines()\n",
    "    split_list.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3544ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_list = []\n",
    "for k in split_list :\n",
    "    l = [item for item in k if 'لا توجد نتائج لتحليل هذه الكلمة' not in item]\n",
    "    tmp_l = [item.replace(\"#\",'') for item in l]\n",
    "    work_list.append(tmp_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "429730b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_list = []\n",
    "for k in work_list :\n",
    "    tst = [item.split(':') for item in k]\n",
    "    final_list.append(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9135fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifie le prefixe, racine et le suffixe et les placent dans un dictionnaire\n",
    "def identify(word_l):\n",
    "    if len(word_l) < 4 :\n",
    "        return None\n",
    "    dictt = {}\n",
    "    dictt['word'] = word_l[0]\n",
    "    # le cas s'il existe un préfixe\n",
    "    if word_l[2] != '' and word_l[2] != ' ' :   \n",
    "        if word_l[4] not in word_l[0]: \n",
    "            if word_l[5] in word_l[0] and word_l[5] != '': \n",
    "                dictt['prefixe'] = word_l[2]\n",
    "                dictt['root'] = word_l[8]\n",
    "                dictt['suffixe'] = word_l[9]\n",
    "            elif word_l[3] in word_l[0] and word_l[3] != '':\n",
    "                dictt['prefixe'] = word_l[2]\n",
    "                dictt['root'] = word_l[3]\n",
    "                dictt['suffixe'] = ''\n",
    "        else :\n",
    "            dictt['prefixe'] = word_l[2]\n",
    "            dictt['root'] = word_l[7]\n",
    "            dictt['suffixe'] = word_l[8]\n",
    "    # s'il n'existe pas un préfixe\n",
    "    else : \n",
    "        if word_l[2] == '' : \n",
    "            dictt['prefixe'] = word_l[2]\n",
    "            dictt['root'] = word_l[6]\n",
    "            dictt['suffixe'] = word_l[7]\n",
    "        elif  word_l[2] == ' ' :\n",
    "            dictt['prefixe'] = ''\n",
    "            dictt['root'] = word_l[3]\n",
    "            dictt['suffixe'] = ''    \n",
    "    return dictt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807dc08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69a52da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtre la liste de mots en liste de dictionnaires\n",
    "def word_to_dict_list(wordlist):\n",
    "    dictlist = []\n",
    "    for k in wordlist : \n",
    "        dictlist.append(identify(k))\n",
    "    return dictlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "897b24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for k in final_list: \n",
    "    for j in k :\n",
    "        s = identify(j)\n",
    "        if s == None :\n",
    "            continue\n",
    "        final.append(identify(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cce4d9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'ستقام', 'prefixe': 'سَ', 'root': 'قوم', 'suffixe': ''}\n"
     ]
    }
   ],
   "source": [
    "print(final[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0fe335f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dic_to_list(listt):\n",
    "    L = []\n",
    "    for k in listt : \n",
    "        tmp = []\n",
    "        #print(k)\n",
    "        if len(k) == 4 : \n",
    "            tmp.append(k['word'])\n",
    "            tmp.append(k['prefixe'])\n",
    "            tmp.append(k['root'])\n",
    "            tmp.append(k['suffixe'])\n",
    "            L.append(tmp)\n",
    "    return L\n",
    "data = dic_to_list(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad205283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f81f4b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['مسقط', '', 'سقط', '']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_l = dic_to_list(final)\n",
    "final_l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22118e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimenting with our first neural network to predict the root of a word.\n",
    "\n",
    "data_1 = []\n",
    "\n",
    "for word in final_l:\n",
    "    tmp = []\n",
    "    word[0] = word[0].replace('+', ' ')\n",
    "    word[2] = word[2].replace('+', ' ')\n",
    "    tmp.append(word[0])\n",
    "    tmp.append(word[2])\n",
    "    data_1.append(tmp)\n",
    "    #print(tmp)\n",
    "#data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f8ebb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we extract our dictionnary from the our dataset \n",
    "def extract_dict(listt) :\n",
    "    dictt = []\n",
    "    for word in listt :\n",
    "        for item in word : \n",
    "            tmp = set(item)\n",
    "            for k in tmp : \n",
    "                if k not in dictt : \n",
    "                    dictt.append(k)\n",
    "    return dictt       \n",
    "dic = extract_dict(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247cf020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc451ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data = []\n",
    "for word in data : \n",
    "    tmp =[]\n",
    "    tmp.append(word[0])\n",
    "    tmp.append(word[2])\n",
    "    root_data.append(tmp)\n",
    "#root_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51d4a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encode :\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def code_sequence(self):\n",
    "        L = []\n",
    "        for word in self.data : \n",
    "            tmp_string = code_word(word)\n",
    "            L.append(tmp_string)\n",
    "        final_string = '#'.join(map(str,L))\n",
    "        return final_string \n",
    "    \n",
    "    def code_normal_text(self):\n",
    "        L = []\n",
    "        for word in self.data : \n",
    "            L.append(word[1])\n",
    "        final_string = ' '.join(map(str,L))\n",
    "        return final_string\n",
    "        \n",
    "final_str = encode(root_data).code_normal_text()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d08082",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3dbc01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "90b091db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = []\n",
    "for item in root_data : \n",
    "    tmp = []\n",
    "    if len(item[1]) <= 3 and len(item[1]) != 0:\n",
    "        tmp.append('$'+item[0]+'£')\n",
    "        tmp.append('$'+item[1]+'£')\n",
    "        data_root.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d85cf4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32e29629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_padding(dat, padding_char):\n",
    "    #Le'ts create a padding character : \n",
    "    pad_char = padding_char\n",
    "    padded_data = []\n",
    "    \n",
    "    ls_words = []\n",
    "    ls_roots = []\n",
    "\n",
    "    for instance in dat : \n",
    "        ls_words.append(instance[0])\n",
    "        ls_roots.append(instance[1])\n",
    "    max_len_words = max([len(item) for item in ls_words])\n",
    "    max_len_roots = max([len(item) for item in ls_roots])\n",
    "    \n",
    "    for instance in dat: \n",
    "        tmp = []\n",
    "        word,root = instance[0], instance[1]\n",
    "        while(len(word) != max_len_words):\n",
    "            word += pad_char\n",
    "        tmp.append(word)\n",
    "        while(len(root) != max_len_roots):\n",
    "            root += pad_char\n",
    "        tmp.append(root)\n",
    "        padded_data.append(tmp)\n",
    "        \n",
    "    return padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11f4f468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['$مسقط£%%%%%%%%', '$سقط£'],\n",
       " ['$يقام£%%%%%%%%', '$قمي£'],\n",
       " ['$الرابع£%%%%%%', '$ربع£'],\n",
       " ['$والعشرين£%%%%', '$عشر£'],\n",
       " ['$من£%%%%%%%%%%', '$من£%'],\n",
       " ['$شهر£%%%%%%%%%', '$شهر£'],\n",
       " ['$فبراير£%%%%%%', '$رير£'],\n",
       " ['$المقبل£%%%%%%', '$قبل£'],\n",
       " ['$قصر£%%%%%%%%%', '$قصر£'],\n",
       " ['$البستان£%%%%%', '$بسس£']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_padding(data_root,'%')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7246f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    def train_batch(self, batch):\n",
    "                \n",
    "        self.optimizer.zero_grad()\n",
    "        for instance in batch: \n",
    "            '''\n",
    "            word = instance[0] : the word to extract root from\n",
    "            target_word = instance[1] : the root of the word\n",
    "            '''\n",
    "            word = instance[0]\n",
    "            target_root = instance[1]\n",
    "            \n",
    "            res1 = self.encode_word(word)\n",
    "            predicted_root = []\n",
    "            original_root_seq = self.word_to_idx_seq(target_root)\n",
    "            predicted_root_seq = []\n",
    "            res_char = target_root[0]\n",
    "            \n",
    "            i = 0\n",
    "            for char in target_root : \n",
    "                if len(predicted_root) == len(target_root):\n",
    "                    break\n",
    "                \n",
    "                \n",
    "                if random.random() < self.teacher_forcing_ratio and i!=0 : \n",
    "                    res_char = char\n",
    "                    i+= 1\n",
    "                \n",
    "                res_char , hidd , idx = self.decode_word(res1, res_char)\n",
    "                predicted_root_seq.append(idx)\n",
    "                test = hidd\n",
    "                predicted_root.append(res_char)\n",
    "\n",
    "            predicted_root = ''.join(predicted_root)\n",
    "\n",
    "            predicted_root_seq = torch.tensor(predicted_root_seq)\n",
    "\n",
    "            org = self.embedding(original_root_seq)\n",
    "            pred = self.embedding(predicted_root_seq)\n",
    "            \n",
    "            \n",
    "            loss = self.criterion(org, pred)\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            print('target root :', target_root)\n",
    "            print('predicted root : ', predicted_root)\n",
    "\n",
    "            print('the loss : ', loss.item())  \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return\n",
    "    \n",
    "            \n",
    "            \n",
    "        #print(em1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95150815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(self):\n",
    "    \n",
    "    #Le'ts create a padding for ouriinstances : \n",
    "    \n",
    "    pad_char = ''\n",
    "    padded_data = []\n",
    "    ls_words = []\n",
    "    ls_roots = []\n",
    "    for instance in self.data : \n",
    "        ls_words.append(instance[0])\n",
    "        ls_roots.append(instance[1])\n",
    "    max_len_words = max([len(item) for item in ls_words])\n",
    "    max_len_roots = max([len(item) for item in ls_roots])\n",
    "    \n",
    "    for instance in self.data: \n",
    "        tmp = []\n",
    "        word,root = instance[0], instance[1]\n",
    "        while(len(word) != max_len_words):\n",
    "            word += pad_char\n",
    "        tmp.append(word)\n",
    "        while(len(root) != max_len_roots):\n",
    "            root += pad_char\n",
    "        tmp.append(root)\n",
    "        padded_data.append(tmp)\n",
    "    \n",
    "\n",
    "    # let's create our vocab : \n",
    "    \n",
    "    vocab = []\n",
    "    for word in padded_data :\n",
    "        for item in word : \n",
    "            tmp = set(item)\n",
    "            for k in tmp : \n",
    "                if k not in vocab : \n",
    "                    vocab.append(k)\n",
    "    \n",
    "    \n",
    "    # Let's create our dictionnary with unique indexes\n",
    "    \n",
    "    char_to_idx_map = {char: idx for idx, char in enumerate(dictt)}\n",
    "    \n",
    "    # Let's now split our data to batches\n",
    "   \n",
    "    final_data = []\n",
    "    for instance in padded_data : \n",
    "        tmp = []\n",
    "        word = self.word_to_seq(instance[0])\n",
    "        root = self.word_to_seq(instance[1])\n",
    "        tmp.append(word)\n",
    "        tmp.append(root)\n",
    "        final_data.append(tmp)\n",
    "        \n",
    "    size= self.batch_size \n",
    "    batches = [final_data[i:i + size] for i in range(0, len(final_data), size)]\n",
    "    \n",
    "    return batches , vocab , char_to_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d695d5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "c250db0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# implementation of a paper : \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch \n",
    "import string\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "\n",
    "class model(nn.Module): \n",
    "    \n",
    "    def __init__(self, data, batch_size ,embedding_size, hidden_size, num_layers, teacher_forcing_ratio, learning_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''\n",
    "        our input data are arabic words with their roots, with the hypothesis that each word has it's own root.\n",
    "        \n",
    "        (our dataset for this is named root_data)\n",
    "        '''\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.batches, self.vocab, self.char_index_dic = self.prepare_data()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = len(self.vocab)\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(num_embeddings = len(self.vocab), embedding_dim = self.embedding_size, padding_idx = self.char_index_dic['%']) \n",
    "        \n",
    "        \n",
    "        #self.bigru = nn.GRU(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=self.num_layers, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.BILSTM = nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=self.num_layers, bidirectional=True, batch_first=True)\n",
    "\n",
    "        \n",
    "        #self.gru = nn.GRU(input_size= self.embedding_size ,hidden_size = self.hidden_size * 2, num_layers = self.num_layers, batch_first = True)\n",
    "        self.LSTM = nn.LSTM(input_size= self.embedding_size ,hidden_size = self.hidden_size*2, num_layers = int(self.num_layers / self.num_layers), batch_first = True)\n",
    "\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index =self.char_index_dic['%'])\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.Linear = nn.Linear(self.hidden_size * 2,len(self.vocab))\n",
    "        \n",
    "        #self.optimizer = optim.Adam([*self.bigru.parameters(), *self.gru.parameters(), *self.Linear.parameters()], lr = 0.001)\n",
    "        #self.optimizer = optim.Adam([*self.BILSTM.parameters(), *self.LSTM.parameters()], lr = 0.1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = self.lr)\n",
    "\n",
    "    \n",
    "    def prepare_data(self):\n",
    "    \n",
    "        #Le'ts create a padding for ouriinstances : \n",
    "\n",
    "        pad_char = '%'\n",
    "        padded_data = []\n",
    "        ls_words = []\n",
    "        ls_roots = []\n",
    "        for instance in self.data : \n",
    "            ls_words.append(instance[0])\n",
    "            ls_roots.append(instance[1])\n",
    "        \n",
    "        # Let's calculate the biggest length\n",
    "        max_len_words = max([len(item) for item in ls_words])\n",
    "        max_len_roots = max([len(item) for item in ls_roots])\n",
    "\n",
    "        # Now we pad the word until we reach the max length\n",
    "        for instance in self.data: \n",
    "            tmp = []\n",
    "            word,root = instance[0], instance[1]\n",
    "            while(len(word) != max_len_words):\n",
    "                word += pad_char\n",
    "            tmp.append(word)\n",
    "            while(len(root) != max_len_roots):\n",
    "                root += pad_char\n",
    "            tmp.append(root)\n",
    "            padded_data.append(tmp)\n",
    "\n",
    "        # let's create our vocab : \n",
    "\n",
    "        vocab = []\n",
    "        for word in padded_data :\n",
    "            for item in word : \n",
    "                tmp = set(item)\n",
    "                for k in tmp : \n",
    "                    if k not in vocab : \n",
    "                        vocab.append(k)\n",
    "\n",
    "        # Let's create our dictionnary with unique indexes\n",
    "\n",
    "        char_to_idx_map = {char: idx for idx, char in enumerate(vocab)}\n",
    "\n",
    "        # Let's now split our data to batches\n",
    "\n",
    "        final_data = []\n",
    "        for instance in padded_data : \n",
    "            tmp = []\n",
    "            word = [char_to_idx_map[char] for char in instance[0]]\n",
    "            root = [char_to_idx_map[char] for char in instance[1]]\n",
    "            tmp.append(word)\n",
    "            tmp.append(root)\n",
    "            final_data.append(tmp)\n",
    "\n",
    "        size= self.batch_size \n",
    "        batches = [final_data[i:i + size] for i in range(0, len(final_data), size)]\n",
    "        \n",
    "        return batches , vocab , char_to_idx_map\n",
    "    \n",
    "    \n",
    "    def word_to_seq(self, word):\n",
    "        '''\n",
    "        this function returns a sequence of the unique indexes for the given word \n",
    "        (sequence is tensor that can be changed using a .tolist() )\n",
    "        '''\n",
    "        word_char_idx_seq =[self.char_index_dic[char] for char in word]    \n",
    "        return word_char_idx_seq # word sequence\n",
    "    \n",
    "    \n",
    "    # Let's now construct our model : \n",
    "    \n",
    "    # we should think about character embeddings in order to create an embeded matrix for each word\n",
    "        \n",
    "    \n",
    "    \n",
    "    def encode(self, batch):    \n",
    "        '''\n",
    "        input : a batch of sequences of instances : [word_seq , root_seq] * batch_size\n",
    "                input_size : (input_size,2)\n",
    "        '''\n",
    "        \n",
    "        word_batch = [] # list of words in the batch\n",
    "        root_batch = [] # list of roots in the batch\n",
    "        \n",
    "        for instance in batch : \n",
    "            word_batch.append(instance[0])\n",
    "            root_batch.append(instance[1])\n",
    "            \n",
    "        word_batch = torch.tensor(word_batch)\n",
    "        root_batch = torch.tensor(root_batch)\n",
    "        \n",
    "        # we create embedding of the word batch : \n",
    "        \n",
    "        embedded_word_batch = self.embedding(word_batch)\n",
    "                \n",
    "        outputs, (hidden, cell) = self.BILSTM(embedded_word_batch) # we pass the emebedded vector through the bi-GRU \n",
    "        # we take the last layer input : \n",
    "        \n",
    "        hid , ce = hidden[-2:, :, :] , cell[-2:, :, :]\n",
    "                        \n",
    "        final_hidden = torch.unsqueeze(torch.cat((hid[0], hid[1]), dim = 1), 0)\n",
    "        final_cell = torch.unsqueeze(torch.cat((ce[0], ce[1]), dim = 1), 0)\n",
    "            \n",
    "        return root_batch , (final_hidden, final_cell)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def decode(self, encoder_hidden_cell , batch):\n",
    "        \n",
    "        '''\n",
    "        input : encoding_hidden_layer => corresponds to the concatenation of the final hidden layers \n",
    "                                        of the bidirectionnal gru in our encoder\n",
    "                \n",
    "                batch : subset of data that contains the roots of the words we encoded.\n",
    "                \n",
    "        output : we'll see :) \n",
    "        \n",
    "        '''\n",
    "\n",
    "        (hidden_layer , cell) , root_batch = encoder_hidden_cell , batch \n",
    "                \n",
    "        #print('root batch :', root_batch.size())\n",
    "        \n",
    "        embedded_char = self.embedding(torch.unsqueeze(root_batch[:, 0], 1))\n",
    "            \n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(root_batch.size(1)): \n",
    "            \n",
    "            decoder_output , (hidden_layer, cell) = self.LSTM(embedded_char, (hidden_layer, cell))\n",
    "            \n",
    "            #print(decoder_output.size())\n",
    "            \n",
    "            embedded_char = self.embedding(torch.unsqueeze(torch.clone(root_batch[:, i]), 1))\n",
    "            \n",
    "            mask = np.where([random.random() > self.teacher_forcing_ratio for i in range(root_batch.size(0))])[0]\n",
    "\n",
    "            Dense_decoded_output = self.Linear(decoder_output)\n",
    "            outputs.append(Dense_decoded_output)\n",
    "\n",
    "            \n",
    "            input_dense = nn.Linear(self.hidden_size * 2,self.embedding_size)\n",
    "            input_decoder_output = input_dense(decoder_output)\n",
    "                    \n",
    "            embedded_char[mask] = input_decoder_output[mask]\n",
    "        \n",
    "        return outputs \n",
    "                            \n",
    "        \n",
    "    \n",
    "    def train(self, num_epochs):\n",
    "        \n",
    "        epochs = list(range(num_epochs))\n",
    "        \n",
    "        final_batches = self.batches\n",
    "        \n",
    "        res = []\n",
    "        \n",
    "        for epoch in epochs :\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            \n",
    "            print('epoch num ', epoch)\n",
    "            \n",
    "            n = 0\n",
    "            \n",
    "            for batch in final_batches :\n",
    "                                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                root_batch, encoder_states = self.encode(batch)\n",
    "                                \n",
    "                outputs = self.decode(encoder_states, root_batch)\n",
    "                \n",
    "                a = [torch.squeeze(item, 1) for item in outputs]\n",
    "                a = [torch.unsqueeze(item, 0) for item in a]\n",
    "                \n",
    "                output = torch.cat(a, dim = 0)\n",
    "                \n",
    "                output_dim = output.shape[-1]\n",
    "        \n",
    "                output = output[1:].view(-1, output_dim)\n",
    "                \n",
    "                trg = root_batch.transpose(0, 1)\n",
    "                                           \n",
    "                trg = trg[1:].reshape(-1)\n",
    "                \n",
    "                loss = self.criterion(output, trg)\n",
    "        \n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
    "\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                epoch_loss+=loss\n",
    "                \n",
    "                n+=1\n",
    "                \n",
    "                print(loss)\n",
    "            \n",
    "            res.append(epoch_loss/n)\n",
    "                \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "46c9388d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1647147575.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_15975/1647147575.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    res = mod.train(10)*\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mod = model(data_root, 1000, 64, 100, 16, 0.5, 0.01)\n",
    "res = mod.train(10)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "849be848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"temp_soft = tuple(temp_soft)\\n        \\n        res = torch.tensor(res)\\n        \\n        tst = torch.cat(temp_soft, dim = 1)      \\n        \\n        predicted_sequences = []\\n\\n        for instance in tst :\\n\\n            best_char_indexes = [torch.argmax(item).item() for item in instance]\\n\\n            predicted_sequences.append(best_char_indexes)\\n\\n        #print(torch.tensor(predicted_sequences).size(), '*'*100)\\n        \\n        char_batch = torch.tensor(predicted_sequences)\\n        \\n        #print(char_batch)\\n                \\n        #print('predicted : ', predicted_roots.size(), 'original :', root_batch.size())           \\n        \\n        key_list = list(self.char_index_dic.keys())\\n        val_list = list(self.char_index_dic.values())\\n\\n        pos_all = []\\n\\n        for seq in char_batch : \\n\\n            positions = [val_list.index(item) for item in seq]\\n            result_chars = [key_list[pos] for pos in positions]\\n            char = ''.join(result_chars)\\n\\n            pos_all.append(char)\\n\\n        #print(pos_all)\\n                \\n        return root_batch , res , pos_all\\n    \""
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''temp_soft = tuple(temp_soft)\n",
    "        \n",
    "        res = torch.tensor(res)\n",
    "        \n",
    "        tst = torch.cat(temp_soft, dim = 1)      \n",
    "        \n",
    "        predicted_sequences = []\n",
    "\n",
    "        for instance in tst :\n",
    "\n",
    "            best_char_indexes = [torch.argmax(item).item() for item in instance]\n",
    "\n",
    "            predicted_sequences.append(best_char_indexes)\n",
    "\n",
    "        #print(torch.tensor(predicted_sequences).size(), '*'*100)\n",
    "        \n",
    "        char_batch = torch.tensor(predicted_sequences)\n",
    "        \n",
    "        #print(char_batch)\n",
    "                \n",
    "        #print('predicted : ', predicted_roots.size(), 'original :', root_batch.size())           \n",
    "        \n",
    "        key_list = list(self.char_index_dic.keys())\n",
    "        val_list = list(self.char_index_dic.values())\n",
    "\n",
    "        pos_all = []\n",
    "\n",
    "        for seq in char_batch : \n",
    "\n",
    "            positions = [val_list.index(item) for item in seq]\n",
    "            result_chars = [key_list[pos] for pos in positions]\n",
    "            char = ''.join(result_chars)\n",
    "\n",
    "            pos_all.append(char)\n",
    "\n",
    "        #print(pos_all)\n",
    "                \n",
    "        return root_batch , res , pos_all\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c16f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68cfd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "d5231259",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1341526408.py, line 221)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_15975/1341526408.py\"\u001b[0;36m, line \u001b[0;32m221\u001b[0m\n\u001b[0;31m    mod.train()*\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# implementation of a paper : \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch \n",
    "import string\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "\n",
    "class model(nn.Module): \n",
    "    \n",
    "    def __init__(self, data, batch_size ,embedding_size, hidden_size, num_layers, teacher_forcing_ratio):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''\n",
    "        our input data are arabic words with their roots, with the hypothesis that each word has it's own root.\n",
    "        \n",
    "        (our dataset for this is named root_data)\n",
    "        '''\n",
    "        self.sow = '$' # the start of root character \n",
    "        self.eow = '£' # the end of root character\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab = self.extract_dict()\n",
    "        self.char_index_dic = self.char_to_index()\n",
    "        print(self.char_index_dic)\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(num_embeddings = len(self.vocab), embedding_dim = self.embedding_size) \n",
    "        self.bigru = nn.GRU(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=self.num_layers, bidirectional=True)\n",
    "        self.gru = nn.GRU(input_size= self.embedding_size ,hidden_size = self.hidden_size, num_layers = self.num_layers)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.Linear = nn.Linear(self.hidden_size,len(self.vocab))\n",
    "        self.optimizer = optim.Adam([*self.bigru.parameters(), *self.gru.parameters(), *self.Linear.parameters()], lr = 0.1)\n",
    "\n",
    "    \n",
    "    def extract_dict(self) :\n",
    "        '''\n",
    "        this function extracts all the unique characters from the given dataset\n",
    "        '''\n",
    "        dictt = []\n",
    "        for word in self.data :\n",
    "            for item in word : \n",
    "                tmp = set(item)\n",
    "                for k in tmp : \n",
    "                    if k not in dictt : \n",
    "                        dictt.append(k)\n",
    "        return dictt \n",
    "    \n",
    "    \n",
    "    # Let's now construct our model : \n",
    "    \n",
    "    # we should think about character embeddings in order to create an embeded matrix for each word\n",
    "    \n",
    "    def char_to_index(self):\n",
    "        '''\n",
    "        this function creates unique indexes of each character\n",
    "        '''\n",
    "        char_to_idx_map = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        return char_to_idx_map  \n",
    "    \n",
    "    def data_batches(self): \n",
    "        size = self.batch_size\n",
    "        \n",
    "        batches = [self.data[i:i + size] for i in range(0, len(self.data), size)]\n",
    "        \n",
    "        return batches\n",
    "        \n",
    "    \n",
    "    def word_to_idx_seq(self, word):\n",
    "        '''\n",
    "        this function returns a sequence of the unique indexes for the given word \n",
    "        (sequence is tensor that can be changed using a .tolist() )\n",
    "        '''\n",
    "        word_char_idx_tensor = torch.tensor([self.char_index_dic[char] for char in word])        \n",
    "        return word_char_idx_tensor # word sequence\n",
    "    \n",
    "    def word_to_seq(self, word):\n",
    "        '''\n",
    "        this function returns a sequence of the unique indexes for the given word \n",
    "        (sequence is tensor that can be changed using a .tolist() )\n",
    "        '''\n",
    "        word_char_idx_tensor =[self.char_index_dic[char] for char in word]    \n",
    "        return word_char_idx_tensor # word sequence\n",
    "    \n",
    "    \n",
    "    def encode_word(self, word):\n",
    "        \n",
    "            \n",
    "        # we create embedding of the sequence : \n",
    "        word_seq = self.word_to_idx_seq(word)\n",
    "        embedded_vec = self.embedding(word_seq)\n",
    "        \n",
    "        outputs, hidden = self.bigru(embedded_vec) # we pass the emebedded vector through the bi-GRU \n",
    "\n",
    "        \n",
    "        '''\n",
    "        kaynin two cases : \n",
    "        \n",
    "             case1 :  we work on the outputs  ==> chosen\n",
    "             case2 :  we work on the final hidden state ==> discarted \n",
    "            \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        '''test_hidden = torch.flatten(hidden)\n",
    "        final_hidden = torch.unsqueeze(test_hidden, 0)'''\n",
    "        \n",
    "        \n",
    "        # can also be outputs.\n",
    "        \n",
    "        encoder_output = torch.mean(hidden , dim=0)  # Average the hidden vectors across all time steps. Shape: (hidden_size*2,) if bidirectional, else (hidden_size,)\n",
    "        \n",
    "        final_output = torch.unsqueeze(encoder_output, 0)\n",
    "               \n",
    "        \n",
    "        return final_output\n",
    "    \n",
    "    \n",
    "    def decode_word(self, encoding, character):\n",
    "\n",
    "        \n",
    "        '''\n",
    "        encoding : output of the encoder network. \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        seq = self.word_to_idx_seq(self.sow)\n",
    "        embedded_sow= self.embedding(self.word_to_idx_seq(character))   # starts with self.sow\n",
    "\n",
    "        input_size = embedded_sow.size(1)\n",
    "\n",
    "        hidden_size = encoding.size(1)\n",
    "                 \n",
    "        dec_out , dec_hidden = self.gru(embedded_sow,encoding)\n",
    "                \n",
    "        a = self.Linear(dec_out)\n",
    "        m = nn.Softmax(dim = 1)\n",
    "        \n",
    "        output = m(a)\n",
    "        \n",
    "        top1 = output.argmax(1)[0].item()\n",
    "        \n",
    "        key_list = list(self.char_index_dic.keys())\n",
    "        val_list = list(self.char_index_dic.values())\n",
    "\n",
    "        position = val_list.index(top1)\n",
    "        \n",
    "        result_char = key_list[position]\n",
    "\n",
    "        return result_char, dec_hidden , top1\n",
    "        \n",
    "        \n",
    "    def train_batch(self, batch):\n",
    "                \n",
    "        \n",
    "        for instance in batch: \n",
    "            '''\n",
    "            word = instance[0] : the word to extract root from\n",
    "            target_word = instance[1] : the root of the word\n",
    "            '''\n",
    "            word = instance[0]\n",
    "            target_root = instance[1]\n",
    "            \n",
    "            hidd = self.encode_word(word)\n",
    "            predicted_root = []\n",
    "            original_root_seq = self.word_to_idx_seq(target_root)\n",
    "            predicted_root_seq = []\n",
    "            res_char = target_root[0]\n",
    "            for char in target_root : \n",
    "                if len(predicted_root) == len(target_root):\n",
    "                    break\n",
    "                \n",
    "                \n",
    "                if random.random() < self.teacher_forcing_ratio : \n",
    "                    res_char = char\n",
    "                \n",
    "                res_char , hidd , idx = self.decode_word(hidd, res_char)\n",
    "                predicted_root_seq.append(idx)\n",
    "                test = hidd\n",
    "                predicted_root.append(res_char)\n",
    "\n",
    "            predicted_root = ''.join(predicted_root)\n",
    "\n",
    "            predicted_root_seq = torch.tensor(predicted_root_seq)\n",
    "\n",
    "            org = self.embedding(original_root_seq)\n",
    "            pred = self.embedding(predicted_root_seq)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(org, pred)\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            print('target root :', target_root)\n",
    "            print('predicted root : ', predicted_root)\n",
    "\n",
    "            print('the loss : ', loss.item())        \n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        batches = self.data_batches()\n",
    "        \n",
    "        for batch in batches : \n",
    "            \n",
    "            self.train_batch(batch)\n",
    "\n",
    "        #print(em1)\n",
    "        \n",
    "        \n",
    "\n",
    "mod = model(data_root, 100, 50, 100, 1, 0.9)\n",
    "mod.train()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b751a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3769b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c8969b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''l = 0\n",
    "for item in root_data:\n",
    "    if len(item[1]) > 3:\n",
    "        print(item[1])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc375018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ad99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def decode_word(self, word):\n",
    "        \n",
    "        hidden_size = 32  # Number of hidden units in the GRU\n",
    "        num_layers = 1  # Number of layers in the GRU\n",
    "        dropout = 0.2  # Dropout rate\n",
    "        \n",
    "        gru = nn.GRU(input_size=self.embedding_size, hidden_size=hidden_size, batch_first=True)\n",
    "        \n",
    "        e = self.encode_word(word)\n",
    "        # Initialize the hidden state h0\n",
    "        \n",
    "        h0 = torch.zeros(1, hidden_size)\n",
    "\n",
    "        \n",
    "        # Concatenate the encoder representation and start-of-word character\n",
    "        print(e.size())\n",
    "        print(self.hsowi.unsqueeze(1).size())\n",
    "        \n",
    "        x = torch.cat([self.hsowi.unsqueeze(0).float(), e.unsqueeze(0)], dim=1)\n",
    "\n",
    "        print(x.size())\n",
    "        print(h0.size())\n",
    "        \n",
    "        # Apply the GRU layer\n",
    "        out, h = gru(x)\n",
    "        \n",
    "        # Apply the output layer\n",
    "        out_l = nn.Linear(hidden_size, len(self.vocab))\n",
    "        \n",
    "        out = out_l(out)\n",
    "        \n",
    "        softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "        # Apply the softmax activation function\n",
    "        out = softmax(out)\n",
    "        \n",
    "        return out\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b8531",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class model:\n",
    "    def __init__(self, data, embedding_size):\n",
    "        self.sow = '$'  # the start of word character\n",
    "        self.eow = '£'  # the end of word character\n",
    "        self.data = data\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab = self.extract_dict()\n",
    "        self.char_index_dic = self.char_to_index()\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(self.vocab), embedding_dim=self.embedding_size)\n",
    "\n",
    "    def extract_dict(self):\n",
    "        dictt = []\n",
    "        for word in self.data:\n",
    "            for item in word:\n",
    "                tmp = set(item)\n",
    "                for k in tmp:\n",
    "                    if k not in dictt:\n",
    "                        dictt.append(k)\n",
    "        dictt.append(self.sow)\n",
    "        dictt.append(self.eow)\n",
    "        return dictt\n",
    "\n",
    "    def char_to_index(self):\n",
    "        char_to_idx_map = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        #print(char_to_idx_map)\n",
    "        return char_to_idx_map\n",
    "\n",
    "    def word_to_idx_seq(self, word):\n",
    "        word_char_idx_tensor = torch.tensor([self.char_index_dic[char] for char in word])\n",
    "        return word_char_idx_tensor\n",
    "\n",
    "    def encode_word(self, word):\n",
    "        \n",
    "        hidden_size = 32\n",
    "        bidirectional = True\n",
    "        num_layers = 1\n",
    "        \n",
    "        \n",
    "        gru = nn.GRU(input_size=self.embedding_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                     bidirectional=bidirectional, batch_first=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        word_seq = self.word_to_idx_seq(word)\n",
    "        embedded_vec = self.embedding(word_seq)\n",
    "        outputs, hidden = gru(embedded_vec)\n",
    "\n",
    "        h1 = torch.reshape(hidden[-1], (1, hidden_size))\n",
    "        h2 = torch.reshape(hidden[-2], (1, hidden_size))\n",
    "        hidden = torch.cat((h2, h1), 1)\n",
    "\n",
    "        encoder_output = torch.mean(hidden, dim=0)\n",
    "\n",
    "        return encoder_output\n",
    "\n",
    "    def decode_word(self, encoding):\n",
    "        input_size = encoding.shape[-1] + len(self.vocab)\n",
    "        hidden_size = 64\n",
    "        num_layers = 1\n",
    "\n",
    "        embedded_sow = self.embedding(torch.tensor(self.char_index_dic[self.sow]))\n",
    "        decoder_input = torch.cat((encoding, embedded_sow), dim=1)\n",
    "\n",
    "        gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "        outputs, hidden = gru(decoder_input.unsqueeze(0))\n",
    "        decoder_output = torch.mean(hidden, dim=0)\n",
    "\n",
    "        decoded = []\n",
    "        for i in range(self.max_word_length):\n",
    "            output = self.output_layer(decoder_output)\n",
    "            _, argmax = torch.max(output, dim=1)\n",
    "            decoded.append(argmax.item())\n",
    "            decoder_input = self.embedding(argmax)\n",
    "            decoder_input = torch.cat((decoder_input, encoding), dim=1)\n",
    "            decoder_output, hidden = gru(decoder_input.unsqueeze(0), hidden)\n",
    "            decoder_output = torch.mean(hidden, dim=0)\n",
    "\n",
    "            if argmax == self.eow_index:\n",
    "                break\n",
    "\n",
    "        decoded_word = ' '.join([self.index_char_dic[i] for i in decoded])\n",
    "        return decoded_word\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        #for instance in self.data:\n",
    "        word = self.data[0]\n",
    "        encoding = self.encode_word(word)\n",
    "        print(f'Encoding for {word}: {encoding}')\n",
    "\n",
    "        decoding = self.decode_word(encoding)\n",
    "        print(f'Decoding for {word}: {decoding}')\n",
    "        break\n",
    "\n",
    "mod = model(root_data, 10)\n",
    "mod.train()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# create an embedding layer with 50 input dimensions and 100 output dimensions\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=43, output_dim=100)\n",
    "sequence = [5 , 10 , 10, 20 , 7, 1]\n",
    "# get the embedding vectors for the sequence of character indices\n",
    "embedding_vectors = embedding_layer(np.array(sequence))\n",
    "embedding_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e94deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 32  # Number of hidden units in the GRU\n",
    "bidirectional = True  # Whether to use a bidirectional GRU or not\n",
    "num_layers = 1  # Number of layers in the GRU\n",
    "dropout = 0.2  # Dropout rate\n",
    "gru = nn.GRU(input_size=16, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee58704c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaad5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d5ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx_map = {char: idx for idx, char in enumerate(dic)}\n",
    "print(f\"Character to Index Mapping:\\n{char_to_idx_map}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_characters = torch.eye(n=len(dic))  # using the eye method for identity matrix\n",
    "print(f\"One hot encoded characters:\\n{ohe_characters}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb81318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_characters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c788cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_repr_a = ohe_characters[char_to_idx_map['ح']]\n",
    "print(f\"One hot encoded representation of 1:\\n{ohe_repr_a}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word, char_to_idx_map):\n",
    "    \n",
    "    word_char_idx_tensor = torch.tensor([char_to_idx_map[char] for char in word])\n",
    "    word_ohe_repr = ohe_characters[word_char_idx_tensor].T\n",
    "    \n",
    "    return word_ohe_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0bd253",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_char_idx_tensor = torch.tensor([char_to_idx_map[char] for char in \"مَرْحَبًا\"])\n",
    "print(word_char_idx_tensor.tolist())\n",
    "word_ohe_repr = ohe_characters[word_char_idx_tensor].T\n",
    "word_ohe_repr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b448e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "convolution_layer = nn.Conv1d(in_channels=43, out_channels=128, kernel_size=3, bias=False)\n",
    "conv_out = convolution_layer(word_ohe_repr.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of convolution output: {conv_out.shape}\")\n",
    "print(f\"Convolution Output:\\n{conv_out}\\n\")\n",
    "\n",
    "# adding an activation layer so as to add non linearity to the output\n",
    "activation_layer = nn.Tanh()\n",
    "activation_out = activation_layer(conv_out)\n",
    "activation_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd7ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of activation output: {activation_out.shape}\")\n",
    "print(f\"Activation Output:\\n{activation_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling Layer\n",
    "max_pooling_layer = nn.MaxPool1d(kernel_size=conv_out.shape[2])\n",
    "\n",
    "# finally here is our word embedding of size -> 256\n",
    "word_embedding = max_pooling_layer(activation_out).squeeze()\n",
    "\n",
    "print(f\"Final word embedding shape: {word_embedding.shape}\")\n",
    "print(f\"Final word embedding:\\n{word_embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed2aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_embedding():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2619f608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bea57e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e852de90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = ''.join(dic)\n",
    "vowel_pattern = r'[\\u064e\\u064b\\u0650\\u064d\\u064f\\u064c\\u064f\\u0652\\u0651]'\n",
    "vowels = re.findall(vowel_pattern, st)\n",
    "l = []\n",
    "for char in st :\n",
    "    if char not in vowels : \n",
    "        l.append(char)\n",
    "        \n",
    "l = ''.join(l)\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c27d12",
   "metadata": {},
   "source": [
    "### Let's create a data structure for each word of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ef987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = work_list[2]\n",
    "tmp.split(\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b25293",
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel_pattern = r'[\\u064b\\u064e\\u064f\\u0650\\u0651\\u0652]'\n",
    "word = \"مَرْحَبًا\"\n",
    "vowels = re.findall(vowel_pattern, word)\n",
    "print(f\"Vowels in '{word}': {vowels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f63a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''filtered_split_list = []\n",
    "for item in split_list : \n",
    "    temp_item_list = []\n",
    "    for subitem in item : \n",
    "        if 'لا توجد نتائج لتحليل هذه الكلمة' not in item : \n",
    "            temp_item_list.append(item)\n",
    "    filtered_split_list.append(temp_item_list)\n",
    "filtered_split_list[0]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a7be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Check the data type of each element in texts\n",
    "texts = pd.DataFrame(data_1[:, -1])\n",
    "texts = texts.iloc[:, 1:]\n",
    "for i, text in enumerate(texts):\n",
    "    if not isinstance(text, str):\n",
    "        print(f\"Element {i} is not a string: {text}\")'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
