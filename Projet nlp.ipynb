{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ea0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89e9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f7603ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb1349c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50137a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'corpus_morphological_analysis'\n",
    "file_paths = []\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    file_paths.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c5f5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file_paths = file_paths[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2602e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "#i = 0\n",
    "for filepath in temp_file_paths :\n",
    "    #print(i)\n",
    "    with open(filepath, encoding='utf-8') as f :\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()  \n",
    "    text = soup.get_text()\n",
    "    content.append(text)\n",
    "    #i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d7aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b5ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a75b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_list = []\n",
    "for item in content : \n",
    "    tmp = item.splitlines()\n",
    "    split_list.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3544ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_list = []\n",
    "for k in split_list :\n",
    "    l = [item for item in k if 'لا توجد نتائج لتحليل هذه الكلمة' not in item]\n",
    "    tmp_l = [item.replace(\"#\",'') for item in l]\n",
    "    work_list.append(tmp_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "429730b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_list = []\n",
    "for k in work_list :\n",
    "    tst = [item.split(':') for item in k]\n",
    "    final_list.append(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9135fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifie le prefixe, racine et le suffixe et les placent dans un dictionnaire\n",
    "def identify(word_l):\n",
    "    if len(word_l) < 4 :\n",
    "        return None\n",
    "    dictt = {}\n",
    "    dictt['word'] = word_l[0]\n",
    "    # le cas s'il existe un préfixe\n",
    "    if word_l[2] != '' and word_l[2] != ' ' :   \n",
    "        if word_l[4] not in word_l[0]: \n",
    "            if word_l[5] in word_l[0] and word_l[5] != '': \n",
    "                dictt['prefixe'] = word_l[2]\n",
    "                dictt['root'] = word_l[8]\n",
    "                dictt['suffixe'] = word_l[9]\n",
    "            elif word_l[3] in word_l[0] and word_l[3] != '':\n",
    "                dictt['prefixe'] = word_l[2]\n",
    "                dictt['root'] = word_l[3]\n",
    "                dictt['suffixe'] = ''\n",
    "        else :\n",
    "            dictt['prefixe'] = word_l[2]\n",
    "            dictt['root'] = word_l[7]\n",
    "            dictt['suffixe'] = word_l[8]\n",
    "    # s'il n'existe pas un préfixe\n",
    "    else : \n",
    "        if word_l[2] == '' : \n",
    "            dictt['prefixe'] = word_l[2]\n",
    "            dictt['root'] = word_l[6]\n",
    "            dictt['suffixe'] = word_l[7]\n",
    "        elif  word_l[2] == ' ' :\n",
    "            dictt['prefixe'] = ''\n",
    "            dictt['root'] = word_l[3]\n",
    "            dictt['suffixe'] = ''    \n",
    "    return dictt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807dc08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69a52da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtre la liste de mots en liste de dictionnaires\n",
    "def word_to_dict_list(wordlist):\n",
    "    dictlist = []\n",
    "    for k in wordlist : \n",
    "        dictlist.append(identify(k))\n",
    "    return dictlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "897b24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for k in final_list: \n",
    "    for j in k :\n",
    "        s = identify(j)\n",
    "        if s == None :\n",
    "            continue\n",
    "        final.append(identify(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce4d9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0fe335f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dic_to_list(listt):\n",
    "    L = []\n",
    "    for k in listt : \n",
    "        tmp = []\n",
    "        #print(k)\n",
    "        if len(k) == 4 : \n",
    "            tmp.append(k['word'])\n",
    "            tmp.append(k['prefixe'])\n",
    "            tmp.append(k['root'])\n",
    "            tmp.append(k['suffixe'])\n",
    "            L.append(tmp)\n",
    "    return L\n",
    "data = dic_to_list(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad205283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f81f4b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['يقدمها', 'ي', 'قدم', 'هَا']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_l = dic_to_list(final)\n",
    "final_l[5990]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22118e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimenting with our first neural network to predict the root of a word.\n",
    "\n",
    "data_1 = []\n",
    "\n",
    "for word in final_l:\n",
    "    tmp = []\n",
    "    word[0] = word[0].replace('+', ' ')\n",
    "    word[2] = word[2].replace('+', ' ')\n",
    "    tmp.append(word[0])\n",
    "    tmp.append(word[2])\n",
    "    data_1.append(tmp)\n",
    "    #print(tmp)\n",
    "#data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f8ebb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we extract our dictionnary from the our dataset \n",
    "def extract_dict(listt) :\n",
    "    dictt = []\n",
    "    for word in listt :\n",
    "        for item in word : \n",
    "            tmp = set(item)\n",
    "            for k in tmp : \n",
    "                if k not in dictt : \n",
    "                    dictt.append(k)\n",
    "    return dictt       \n",
    "dic = extract_dict(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247cf020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc451ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data = []\n",
    "for word in data : \n",
    "    tmp =[]\n",
    "    tmp.append(word[0])\n",
    "    tmp.append(word[2])\n",
    "    root_data.append(tmp)\n",
    "#root_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51d4a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encode :\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def code_sequence(self):\n",
    "        L = []\n",
    "        for word in self.data : \n",
    "            tmp_string = code_word(word)\n",
    "            L.append(tmp_string)\n",
    "        final_string = '#'.join(map(str,L))\n",
    "        return final_string \n",
    "    \n",
    "    def code_normal_text(self):\n",
    "        L = []\n",
    "        for word in self.data : \n",
    "            L.append(word[1])\n",
    "        final_string = ' '.join(map(str,L))\n",
    "        return final_string\n",
    "        \n",
    "final_str = encode(root_data).code_normal_text()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d08082",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3dbc01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90b091db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = []\n",
    "for item in root_data : \n",
    "    tmp = []\n",
    "    if len(item[1]) <= 3 and len(item[1]) != 0:\n",
    "        tmp.append('$'+item[0]+'£')\n",
    "        tmp.append('$'+item[1]+'£')\n",
    "        data_root.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d85cf4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32e29629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_padding(dat, padding_char):\n",
    "    #Le'ts create a padding character : \n",
    "    pad_char = padding_char\n",
    "    padded_data = []\n",
    "    \n",
    "    ls_words = []\n",
    "    ls_roots = []\n",
    "\n",
    "    for instance in dat : \n",
    "        ls_words.append(instance[0])\n",
    "        ls_roots.append(instance[1])\n",
    "    max_len_words = max([len(item) for item in ls_words])\n",
    "    max_len_roots = max([len(item) for item in ls_roots])\n",
    "    \n",
    "    for instance in dat: \n",
    "        tmp = []\n",
    "        word,root = instance[0], instance[1]\n",
    "        while(len(word) != max_len_words):\n",
    "            word += pad_char\n",
    "        tmp.append(word)\n",
    "        while(len(root) != max_len_roots):\n",
    "            root += pad_char\n",
    "        tmp.append(root)\n",
    "        padded_data.append(tmp)\n",
    "        \n",
    "    return padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11f4f468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['$مسقط£%%%%%%%%%', '$سقط£'],\n",
       " ['$يقام£%%%%%%%%%', '$قمي£'],\n",
       " ['$الرابع£%%%%%%%', '$ربع£'],\n",
       " ['$والعشرين£%%%%%', '$عشر£'],\n",
       " ['$من£%%%%%%%%%%%', '$من£%'],\n",
       " ['$شهر£%%%%%%%%%%', '$شهر£'],\n",
       " ['$فبراير£%%%%%%%', '$رير£'],\n",
       " ['$المقبل£%%%%%%%', '$قبل£'],\n",
       " ['$قصر£%%%%%%%%%%', '$قصر£'],\n",
       " ['$البستان£%%%%%%', '$بسس£']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_padding(data_root,'%')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7246f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    def train_batch(self, batch):\n",
    "                \n",
    "        self.optimizer.zero_grad()\n",
    "        for instance in batch: \n",
    "            '''\n",
    "            word = instance[0] : the word to extract root from\n",
    "            target_word = instance[1] : the root of the word\n",
    "            '''\n",
    "            word = instance[0]\n",
    "            target_root = instance[1]\n",
    "            \n",
    "            res1 = self.encode_word(word)\n",
    "            predicted_root = []\n",
    "            original_root_seq = self.word_to_idx_seq(target_root)\n",
    "            predicted_root_seq = []\n",
    "            res_char = target_root[0]\n",
    "            \n",
    "            i = 0\n",
    "            for char in target_root : \n",
    "                if len(predicted_root) == len(target_root):\n",
    "                    break\n",
    "                \n",
    "                \n",
    "                if random.random() < self.teacher_forcing_ratio and i!=0 : \n",
    "                    res_char = char\n",
    "                    i+= 1\n",
    "                \n",
    "                res_char , hidd , idx = self.decode_word(res1, res_char)\n",
    "                predicted_root_seq.append(idx)\n",
    "                test = hidd\n",
    "                predicted_root.append(res_char)\n",
    "\n",
    "            predicted_root = ''.join(predicted_root)\n",
    "\n",
    "            predicted_root_seq = torch.tensor(predicted_root_seq)\n",
    "\n",
    "            org = self.embedding(original_root_seq)\n",
    "            pred = self.embedding(predicted_root_seq)\n",
    "            \n",
    "            \n",
    "            loss = self.criterion(org, pred)\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            print('target root :', target_root)\n",
    "            print('predicted root : ', predicted_root)\n",
    "\n",
    "            print('the loss : ', loss.item())  \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return\n",
    "    \n",
    "            \n",
    "            \n",
    "        #print(em1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95150815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(self):\n",
    "    \n",
    "    #Le'ts create a padding for ouriinstances : \n",
    "    \n",
    "    pad_char = ''\n",
    "    padded_data = []\n",
    "    ls_words = []\n",
    "    ls_roots = []\n",
    "    for instance in self.data : \n",
    "        ls_words.append(instance[0])\n",
    "        ls_roots.append(instance[1])\n",
    "    max_len_words = max([len(item) for item in ls_words])\n",
    "    max_len_roots = max([len(item) for item in ls_roots])\n",
    "    \n",
    "    for instance in self.data: \n",
    "        tmp = []\n",
    "        word,root = instance[0], instance[1]\n",
    "        while(len(word) != max_len_words):\n",
    "            word += pad_char\n",
    "        tmp.append(word)\n",
    "        while(len(root) != max_len_roots):\n",
    "            root += pad_char\n",
    "        tmp.append(root)\n",
    "        padded_data.append(tmp)\n",
    "    \n",
    "\n",
    "    # let's create our vocab : \n",
    "    \n",
    "    vocab = []\n",
    "    for word in padded_data :\n",
    "        for item in word : \n",
    "            tmp = set(item)\n",
    "            for k in tmp : \n",
    "                if k not in vocab : \n",
    "                    vocab.append(k)\n",
    "    \n",
    "    \n",
    "    # Let's create our dictionnary with unique indexes\n",
    "    \n",
    "    char_to_idx_map = {char: idx for idx, char in enumerate(dictt)}\n",
    "    \n",
    "    # Let's now split our data to batches\n",
    "   \n",
    "    final_data = []\n",
    "    for instance in padded_data : \n",
    "        tmp = []\n",
    "        word = self.word_to_seq(instance[0])\n",
    "        root = self.word_to_seq(instance[1])\n",
    "        tmp.append(word)\n",
    "        tmp.append(root)\n",
    "        final_data.append(tmp)\n",
    "        \n",
    "    size= self.batch_size \n",
    "    batches = [final_data[i:i + size] for i in range(0, len(final_data), size)]\n",
    "    \n",
    "    return batches , vocab , char_to_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d695d5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(item[0]) for item in data_root])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa595ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794907\n",
      "794906\n"
     ]
    }
   ],
   "source": [
    "print(len(data_root))\n",
    "for item in data_root :\n",
    "    if len(item[0])==15:\n",
    "        data_root.pop(data_root.index(item))\n",
    "print(len(data_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549d514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c250db0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# implementation of a paper : \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch \n",
    "import string\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "\n",
    "class model(nn.Module): \n",
    "\n",
    "    def __init__(self, data, batch_size ,embedding_size, num_layers ,dropout, teacher_forcing_ratio, learning_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''\n",
    "        our input data are arabic words with their roots, with the hypothesis that each word has it's own root.\n",
    "        \n",
    "        (our dataset for this is named root_data)\n",
    "        '''\n",
    "        self.sow = '$'\n",
    "        self.eow = '£'\n",
    "        self.lr = learning_rate\n",
    "        self.ratio = 0.9\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.batches, self.vocab, self.char_index_dic = self.prepare_data(self.data)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = len(self.vocab)\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embedding(num_embeddings = len(self.vocab), embedding_dim = self.embedding_size, padding_idx = self.char_index_dic['%']) \n",
    "        \n",
    "        \n",
    "        #self.bigru = nn.GRU(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=self.num_layers, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.BILSTM = nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=self.num_layers, bidirectional=True, batch_first=True, dropout = self.dropout)\n",
    "\n",
    "        \n",
    "        #self.gru = nn.GRU(input_size= self.embedding_size ,hidden_size = self.hidden_size * 2, num_layers = self.num_layers, batch_first = True)\n",
    "        self.LSTM = nn.LSTM(input_size= self.embedding_size ,hidden_size = self.hidden_size*2, num_layers = self.num_layers, batch_first = True, dropout = self.dropout)\n",
    "\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index =self.char_index_dic['%'])\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.Linear = nn.Linear(self.hidden_size * 2,len(self.vocab))\n",
    "        \n",
    "        #self.optimizer = optim.Adam([*self.bigru.parameters(), *self.gru.parameters(), *self.Linear.parameters()], lr = 0.001)\n",
    "        #self.optimizer = optim.Adam([*self.BILSTM.parameters(), *self.LSTM.parameters()], lr = 0.1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = self.lr)\n",
    "\n",
    "    \n",
    "    def prepare_data(self, data):\n",
    "    \n",
    "        #Le'ts create a padding for ouriinstances : \n",
    "\n",
    "        pad_char = '%'\n",
    "        padded_data = []\n",
    "        ls_words = []\n",
    "        ls_roots = []\n",
    "        for instance in data : \n",
    "            ls_words.append(instance[0])\n",
    "            ls_roots.append(instance[1])\n",
    "        \n",
    "        # Let's calculate the biggest length\n",
    "        max_len_words = max([len(item) for item in ls_words])\n",
    "        max_len_roots = max([len(item) for item in ls_roots])\n",
    "\n",
    "        # Now we pad the word until we reach the max length\n",
    "        for instance in data: \n",
    "            tmp = []\n",
    "            word,root = instance[0], instance[1]\n",
    "            while(len(word) != max_len_words):\n",
    "                word += pad_char\n",
    "            tmp.append(word)\n",
    "            while(len(root) != max_len_roots):\n",
    "                root += pad_char\n",
    "            tmp.append(root)\n",
    "            padded_data.append(tmp)\n",
    "\n",
    "        # let's create our vocab : \n",
    "\n",
    "        vocab = []\n",
    "        for word in padded_data :\n",
    "            for item in word : \n",
    "                tmp = set(item)\n",
    "                for k in tmp : \n",
    "                    if k not in vocab : \n",
    "                        vocab.append(k)\n",
    "\n",
    "        # Let's create our dictionnary with unique indexes\n",
    "\n",
    "        char_to_idx_map = {char: idx for idx, char in enumerate(vocab)}\n",
    "\n",
    "        # Let's now split our data to batches\n",
    "\n",
    "        final_data = []\n",
    "        for instance in padded_data : \n",
    "            tmp = []\n",
    "            word = [char_to_idx_map[char] for char in instance[0]]\n",
    "            root = [char_to_idx_map[char] for char in instance[1]]\n",
    "            tmp.append(word)\n",
    "            tmp.append(root)\n",
    "            final_data.append(tmp)\n",
    "\n",
    "        size= self.batch_size \n",
    "        batches = [final_data[i:i + size] for i in range(0, len(final_data), size)]\n",
    "        \n",
    "        return batches , vocab , char_to_idx_map\n",
    "    \n",
    "    \n",
    "    def word_to_seq(self, word):\n",
    "        '''\n",
    "        this function returns a sequence of the unique indexes for the given word \n",
    "        (sequence is tensor that can be changed using a .tolist() )\n",
    "        '''\n",
    "        word_char_idx_seq =[self.char_index_dic[char] for char in word]    \n",
    "        return word_char_idx_seq # word sequence\n",
    "    \n",
    "    \n",
    "    # Let's now construct our model : \n",
    "    \n",
    "    # we should think about character embeddings in order to create an embeded matrix for each word\n",
    "        \n",
    "    \n",
    "    \n",
    "    def encode(self, batch):    \n",
    "        '''\n",
    "        input : a batch of sequences of instances : [word_seq , root_seq] * batch_size\n",
    "                input_size : (input_size,2)\n",
    "        '''\n",
    "        \n",
    "        word_batch = [] # list of words in the batch\n",
    "        root_batch = [] # list of roots in the batch\n",
    "        \n",
    "        for instance in batch : \n",
    "            word_batch.append(instance[0])\n",
    "            root_batch.append(instance[1])\n",
    "            \n",
    "        word_batch = torch.tensor(word_batch)\n",
    "        root_batch = torch.tensor(root_batch)\n",
    "        \n",
    "        # we create embedding of the word batch : \n",
    "        \n",
    "        embedded_word_batch = self.embedding(word_batch)\n",
    "                \n",
    "        outputs, (hidden, cell) = self.BILSTM(embedded_word_batch) # we pass the emebedded vector through the bi-GRU \n",
    "        \n",
    "        \n",
    "        # hidden size : [2 * num_layers, batch_size , hidden_size]\n",
    "        \n",
    "        # we want hidden size : [num_layers , batch_size  , 2 * hidden_size]\n",
    "        \n",
    "        # we return an adequate layer for the decoder : \n",
    "        \n",
    "        final_hid, final_ce = [], []\n",
    "        for k in range(0,hidden.size(0), 2):\n",
    "            \n",
    "            tmp_hid = hidden[k:k+2 , :, :]\n",
    "            tmp_ce = cell[k:k+2, :, :]\n",
    "            \n",
    "            \n",
    "            cct_hid = torch.cat((tmp_hid[0], tmp_hid[1]), dim  = 1).tolist()\n",
    "            cct_ce = torch.cat((tmp_ce[0], tmp_ce[1]), dim  = 1).tolist()\n",
    "            \n",
    "            final_hid.append(cct_hid)\n",
    "            final_ce.append(cct_ce)\n",
    "        \n",
    "        final_hid, final_ce = torch.tensor(final_hid), torch.tensor(final_ce)\n",
    "        \n",
    "        #print(final_hid.size(), final_ce.size())\n",
    "            \n",
    "        return root_batch , (final_hid, final_ce)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def decode(self, encoder_hidden_cell , batch, teacher_forcing_bool):\n",
    "        \n",
    "        '''\n",
    "        input : encoding_hidden_layer => corresponds to the concatenation of the final hidden layers \n",
    "                                        of the bidirectionnal gru in our encoder\n",
    "                \n",
    "                batch : subset of data that contains the roots of the words we encoded.\n",
    "                \n",
    "        output : we'll see :) \n",
    "        \n",
    "        '''\n",
    "\n",
    "        (hidden_layer , cell) , root_batch = encoder_hidden_cell , batch \n",
    "                        \n",
    "        embedded_char = self.embedding(torch.unsqueeze(root_batch[:, 0], 1))\n",
    "            \n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(root_batch.size(1)): \n",
    "            \n",
    "            decoder_output , (hidden_layer, cell) = self.LSTM(embedded_char, (hidden_layer, cell))\n",
    "            \n",
    "            #hidden_layer , cell = hidden_layer[-1, :, :], cell[-1, :, :] \n",
    "            \n",
    "            input_dense = nn.Linear(self.hidden_size * 2,self.embedding_size)\n",
    "            input_decoder_output = input_dense(decoder_output)\n",
    "            \n",
    "            embedded_char = input_decoder_output\n",
    "    \n",
    "            mask = np.where([random.random() < self.teacher_forcing_ratio for i in range(root_batch.size(0))])[0]\n",
    "            \n",
    "            teacher_forcing_input = self.embedding(torch.unsqueeze(torch.clone(root_batch[:, i]), 1))\n",
    "            \n",
    "            if teacher_forcing_bool : \n",
    "\n",
    "                embedded_char[mask] = teacher_forcing_input[mask] \n",
    "                \n",
    "            Dense_decoded_output = self.Linear(decoder_output)\n",
    "            outputs.append(Dense_decoded_output)\n",
    "\n",
    "        return outputs \n",
    "                            \n",
    "        \n",
    "    \n",
    "    def train_model(self, batches, teacher_forcing_bool):\n",
    "                \n",
    "        train_batches = batches        \n",
    "         \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        n = 0            \n",
    "        \n",
    "        for batch in train_batches :\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            root_batch, encoder_states = self.encode(batch)\n",
    "\n",
    "            outputs = self.decode(encoder_states, root_batch, teacher_forcing_bool)\n",
    "\n",
    "            a = [torch.squeeze(item, 1) for item in outputs]\n",
    "            a = [torch.unsqueeze(item, 0) for item in a]\n",
    "\n",
    "            output = torch.cat(a, dim = 0)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "\n",
    "            trg = root_batch.transpose(0, 1)\n",
    "\n",
    "            trg = trg[1:].reshape(-1)\n",
    "\n",
    "            loss = self.criterion(output, trg)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_loss+=loss.item()\n",
    "\n",
    "            n+=1\n",
    "\n",
    "            print('the loss of the train batch ', n ,' is : ', loss.item())\n",
    "    \n",
    "        return epoch_loss/n\n",
    "\n",
    "    def evaluate_model(self, batches, teacher_forcing_bool):\n",
    "        '''\n",
    "        this method evaluates our model :=)\n",
    "        will be similar to train but without the teacher forcing/ using an optimizer \n",
    "        '''          \n",
    "        self.eval()\n",
    "\n",
    "        val_batches = batches\n",
    "\n",
    "        n = 0\n",
    "\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        with torch.no_grad() :\n",
    "\n",
    "            for batch in val_batches :\n",
    "\n",
    "                root_batch, encoder_states = self.encode(batch)\n",
    "\n",
    "                outputs = self.decode(encoder_states, root_batch, teacher_forcing_bool)\n",
    "\n",
    "                a = [torch.squeeze(item, 1) for item in outputs]\n",
    "                a = [torch.unsqueeze(item, 0) for item in a]\n",
    "\n",
    "                output = torch.cat(a, dim = 0)\n",
    "\n",
    "                output_dim = output.shape[-1]\n",
    "\n",
    "                output = output[1:].view(-1, output_dim)\n",
    "\n",
    "                trg = root_batch.transpose(0, 1)\n",
    "\n",
    "                trg = trg[1:].reshape(-1)\n",
    "\n",
    "                loss = self.criterion(output, trg)\n",
    "\n",
    "                epoch_loss+=loss.item()\n",
    "\n",
    "                n+=1\n",
    "\n",
    "                print('the loss of the val batch ', n ,' is : ', loss.item())\n",
    "\n",
    "        return epoch_loss / n\n",
    "    \n",
    "    def predict(self, word):\n",
    "        '''\n",
    "        this is the adaptation of encoder-decoder network on a single word w/o optimization\n",
    "        '''\n",
    "\n",
    "        # Let's turn the word into a sequence of word indexes \n",
    "        word_seq = self.word_to_seq(word)\n",
    "\n",
    "        # Let's create an embedding of the word seq\n",
    "        embedded_word = self.embedding(torch.tensor(word_seq))\n",
    "\n",
    "        # Let's feed our word embedding to the encoder network\n",
    "        outputs, (hidden, cell) = self.BILSTM(embedded_word)\n",
    "        \n",
    "        #print(hidden.size())\n",
    "        \n",
    "        final_hid, final_ce = [], []\n",
    "        for k in range(0,hidden.size(0), 2):\n",
    "            \n",
    "            tmp_hid = hidden[k:k+2 ,:]\n",
    "            tmp_ce = cell[k:k+2, :]\n",
    "        \n",
    "            cct_hid = torch.cat((tmp_hid[0], tmp_hid[1]), dim  = -1).tolist()\n",
    "            cct_ce = torch.cat((tmp_ce[0], tmp_ce[1]), dim  = -1).tolist()\n",
    "\n",
    "            final_hid.append(cct_hid)\n",
    "            final_ce.append(cct_ce)\n",
    "        \n",
    "        final_hidden, final_cell = torch.tensor(final_hid), torch.tensor(final_ce)\n",
    "                \n",
    "        #print(final_hidden.size(), final_cell.size())\n",
    "        \n",
    "        '''final_hidden = torch.unsqueeze(torch.cat((hid[0], hid[1]), dim = 0), 0)\n",
    "        final_cell = torch.unsqueeze(torch.cat((ce[0], ce[1]), dim = 0), 0)\n",
    "'''\n",
    "        #initialize the input of the decoder\n",
    "\n",
    "        embedded_char = torch.unsqueeze(self.embedding(torch.tensor(self.char_index_dic[self.sow])), 0)\n",
    "\n",
    "        prediction_output = [] # a list of the outputs of the decoder \n",
    "     \n",
    "        # we create a softmax layer : \n",
    "\n",
    "        soft = nn.Softmax(dim = 1 )\n",
    "        \n",
    "        key_list = list(self.char_index_dic.keys())\n",
    "        val_list = list(self.char_index_dic.values())\n",
    "        \n",
    "        for i in range(5):\n",
    "                        \n",
    "            decoder_output , (final_hidden, final_cell) = self.LSTM(embedded_char, (final_hidden, final_cell))\n",
    "\n",
    "            input_dense = nn.Linear(self.hidden_size * 2,self.embedding_size)\n",
    "            \n",
    "            input_decoder_output = input_dense(decoder_output)\n",
    "\n",
    "            embedded_char = input_decoder_output\n",
    "\n",
    "            Dense_decoded_output = self.Linear(decoder_output)\n",
    "            prediction_output.append(soft(Dense_decoded_output))\n",
    "\n",
    "                \n",
    "        best_char_indexes = [torch.argmax(item).item() for item in prediction_output]\n",
    "\n",
    "\n",
    " \n",
    "        position = [val_list.index(item) for item in best_char_indexes]\n",
    "        result_char = [key_list[pos] for pos in position]\n",
    "        predicted_root = ''.join(result_char)\n",
    "           \n",
    "        print(predicted_root)\n",
    "    \n",
    "        return predicted_root\n",
    "\n",
    "    \n",
    "\n",
    "    def fit(self, num_epochs):\n",
    "        \n",
    "        \"\"\"\n",
    "        let's first prepare our data\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        data = self.data\n",
    "        \n",
    "        data = random.sample(data, len(data))\n",
    "        data_size = len(data)\n",
    "        middle_index = int(data_size * self.ratio)        \n",
    "        train_data , val_data = data[:middle_index], data[middle_index:]\n",
    "        \n",
    "        train_batches, voc, dic = self.prepare_data(train_data)\n",
    "        val_batches ,voc , dic = self.prepare_data(val_data)\n",
    "        \n",
    "        epochs = list(range(num_epochs))\n",
    "        \n",
    "        best_val_loss = 1000\n",
    "        best_model_par = 0\n",
    "        \n",
    "        losses =[]\n",
    "        \n",
    "        for epoch in epochs : \n",
    "            \n",
    "            print('epoch num : ', epoch) \n",
    "            \n",
    "            t1 = time.time()\n",
    "            \n",
    "            train_batches = random.sample(train_batches , len(train_batches))\n",
    "            val_batches = random.sample(val_batches, len(val_batches))\n",
    "                        \n",
    "            train_loss = self.train_model(train_batches, 1)\n",
    "            val_loss = self.evaluate_model(val_batches, 0) # we set the teacher forcing to false            \n",
    "            t2 = time.time()\n",
    "            \n",
    "            test_word = '$' + 'تحليل' + '£'\n",
    "            print(self.predict(test_word))\n",
    "            \n",
    "            tmp = [train_loss, val_loss]\n",
    "            losses.append(tmp)\n",
    "            \n",
    "            print('the training loss : ', train_loss , 'the val loss :', val_loss)\n",
    "            print('epoch num : ' ,epoch , ' lasted : ', t2 - t1 , 'seconds')\n",
    "            \n",
    "            if val_loss < best_val_loss :\n",
    "                \n",
    "                best_val_loss = val_loss \n",
    "                best_model_par = self.state_dict()\n",
    "            \n",
    "            torch.save(best_model_par, 'best_model.pt')\n",
    "            \n",
    "        return losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "046db112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstuff to test in order to reduce overfitting :/\\n\\n==> shuffling the dataset before each epoch  \\n==> reclean my dataset and check for outliers\\n==> recheck the structure of my code for the model\\n\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "stuff to test in order to reduce overfitting :/\n",
    "\n",
    "==> shuffling the dataset before each epoch  \n",
    "==> reclean my dataset and check for outliers\n",
    "==> recheck the structure of my code for the model\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46c9388d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (997319673.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_5786/997319673.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    pred = mod.predict(test_word)*\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mod = model(data_root, 512, 39, 4 , 0.2 ,0.3, 0.001)\n",
    "res = mod.fit(10)\n",
    "test_word = '$' + 'تحليل' + '£'\n",
    "pred = mod.predict(test_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ecbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b975bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d4d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e80414e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = np.array(res)\n",
    "plt.figure()\n",
    "x = list(range(10))\n",
    "plt.plot(x, res[:,0])\n",
    "plt.plot(x, res[:,1])\n",
    "plt.title(\"evolution of loss with epochs\")\n",
    "plt.xlabel('epochs ')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df568b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b231012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849be848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c16f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68cfd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5231259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of a paper : \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch \n",
    "import string\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "\n",
    "class model(nn.Module): \n",
    "    \n",
    "    def __init__(self, data, batch_size ,embedding_size, hidden_size, num_layers, teacher_forcing_ratio):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''\n",
    "        our input data are arabic words with their roots, with the hypothesis that each word has it's own root.\n",
    "        \n",
    "        (our dataset for this is named root_data)\n",
    "        '''\n",
    "        self.sow = '$' # the start of root character \n",
    "        self.eow = '£' # the end of root character\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab = self.extract_dict()\n",
    "        self.char_index_dic = self.char_to_index()\n",
    "        print(self.char_index_dic)\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(num_embeddings = len(self.vocab), embedding_dim = self.embedding_size) \n",
    "        self.bigru = nn.GRU(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=self.num_layers, bidirectional=True)\n",
    "        self.gru = nn.GRU(input_size= self.embedding_size ,hidden_size = self.hidden_size, num_layers = self.num_layers)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.Linear = nn.Linear(self.hidden_size,len(self.vocab))\n",
    "        self.optimizer = optim.Adam([*self.bigru.parameters(), *self.gru.parameters(), *self.Linear.parameters()], lr = 0.1)\n",
    "\n",
    "    \n",
    "    def extract_dict(self) :\n",
    "        '''\n",
    "        this function extracts all the unique characters from the given dataset\n",
    "        '''\n",
    "        dictt = []\n",
    "        for word in self.data :\n",
    "            for item in word : \n",
    "                tmp = set(item)\n",
    "                for k in tmp : \n",
    "                    if k not in dictt : \n",
    "                        dictt.append(k)\n",
    "        return dictt \n",
    "    \n",
    "    \n",
    "    # Let's now construct our model : \n",
    "    \n",
    "    # we should think about character embeddings in order to create an embeded matrix for each word\n",
    "    \n",
    "    def char_to_index(self):\n",
    "        '''\n",
    "        this function creates unique indexes of each character\n",
    "        '''\n",
    "        char_to_idx_map = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        return char_to_idx_map  \n",
    "    \n",
    "    def data_batches(self): \n",
    "        size = self.batch_size\n",
    "        \n",
    "        batches = [self.data[i:i + size] for i in range(0, len(self.data), size)]\n",
    "        \n",
    "        return batches\n",
    "        \n",
    "    \n",
    "    def word_to_idx_seq(self, word):\n",
    "        '''\n",
    "        this function returns a sequence of the unique indexes for the given word \n",
    "        (sequence is tensor that can be changed using a .tolist() )\n",
    "        '''\n",
    "        word_char_idx_tensor = torch.tensor([self.char_index_dic[char] for char in word])        \n",
    "        return word_char_idx_tensor # word sequence\n",
    "    \n",
    "    def word_to_seq(self, word):\n",
    "        '''\n",
    "        this function returns a sequence of the unique indexes for the given word \n",
    "        (sequence is tensor that can be changed using a .tolist() )\n",
    "        '''\n",
    "        word_char_idx_tensor =[self.char_index_dic[char] for char in word]    \n",
    "        return word_char_idx_tensor # word sequence\n",
    "    \n",
    "    \n",
    "    def encode_word(self, word):\n",
    "        \n",
    "            \n",
    "        # we create embedding of the sequence : \n",
    "        word_seq = self.word_to_idx_seq(word)\n",
    "        embedded_vec = self.embedding(word_seq)\n",
    "        \n",
    "        outputs, hidden = self.bigru(embedded_vec) # we pass the emebedded vector through the bi-GRU \n",
    "\n",
    "        \n",
    "        '''\n",
    "        kaynin two cases : \n",
    "        \n",
    "             case1 :  we work on the outputs  ==> chosen\n",
    "             case2 :  we work on the final hidden state ==> discarted \n",
    "            \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        '''test_hidden = torch.flatten(hidden)\n",
    "        final_hidden = torch.unsqueeze(test_hidden, 0)'''\n",
    "        \n",
    "        \n",
    "        # can also be outputs.\n",
    "        \n",
    "        encoder_output = torch.mean(hidden , dim=0)  # Average the hidden vectors across all time steps. Shape: (hidden_size*2,) if bidirectional, else (hidden_size,)\n",
    "        \n",
    "        final_output = torch.unsqueeze(encoder_output, 0)\n",
    "               \n",
    "        \n",
    "        return final_output\n",
    "    \n",
    "    \n",
    "    def decode_word(self, encoding, character):\n",
    "\n",
    "        \n",
    "        '''\n",
    "        encoding : output of the encoder network. \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        seq = self.word_to_idx_seq(self.sow)\n",
    "        embedded_sow= self.embedding(self.word_to_idx_seq(character))   # starts with self.sow\n",
    "\n",
    "        input_size = embedded_sow.size(1)\n",
    "\n",
    "        hidden_size = encoding.size(1)\n",
    "                 \n",
    "        dec_out , dec_hidden = self.gru(embedded_sow,encoding)\n",
    "                \n",
    "        a = self.Linear(dec_out)\n",
    "        m = nn.Softmax(dim = 1)\n",
    "        \n",
    "        output = m(a)\n",
    "        \n",
    "        top1 = output.argmax(1)[0].item()\n",
    "        \n",
    "        key_list = list(self.char_index_dic.keys())\n",
    "        val_list = list(self.char_index_dic.values())\n",
    "\n",
    "        position = val_list.index(top1)\n",
    "        \n",
    "        result_char = key_list[position]\n",
    "\n",
    "        return result_char, dec_hidden , top1\n",
    "        \n",
    "        \n",
    "    def train_batch(self, batch):\n",
    "                \n",
    "        \n",
    "        for instance in batch: \n",
    "            '''\n",
    "            word = instance[0] : the word to extract root from\n",
    "            target_word = instance[1] : the root of the word\n",
    "            '''\n",
    "            word = instance[0]\n",
    "            target_root = instance[1]\n",
    "            \n",
    "            hidd = self.encode_word(word)\n",
    "            predicted_root = []\n",
    "            original_root_seq = self.word_to_idx_seq(target_root)\n",
    "            predicted_root_seq = []\n",
    "            res_char = target_root[0]\n",
    "            for char in target_root : \n",
    "                if len(predicted_root) == len(target_root):\n",
    "                    break\n",
    "                \n",
    "                \n",
    "                if random.random() < self.teacher_forcing_ratio : \n",
    "                    res_char = char\n",
    "                \n",
    "                res_char , hidd , idx = self.decode_word(hidd, res_char)\n",
    "                predicted_root_seq.append(idx)\n",
    "                test = hidd\n",
    "                predicted_root.append(res_char)\n",
    "\n",
    "            predicted_root = ''.join(predicted_root)\n",
    "\n",
    "            predicted_root_seq = torch.tensor(predicted_root_seq)\n",
    "\n",
    "            org = self.embedding(original_root_seq)\n",
    "            pred = self.embedding(predicted_root_seq)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(org, pred)\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            print('target root :', target_root)\n",
    "            print('predicted root : ', predicted_root)\n",
    "\n",
    "            print('the loss : ', loss.item())        \n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        batches = self.data_batches()\n",
    "        \n",
    "        for batch in batches : \n",
    "            \n",
    "            self.train_batch(batch)\n",
    "\n",
    "        #print(em1)\n",
    "        \n",
    "        \n",
    "\n",
    "mod = model(data_root, 100, 50, 100, 1, 0.9)\n",
    "mod.train()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b751a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3769b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c8969b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710504b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc375018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ad99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def decode_word(self, word):\n",
    "        \n",
    "        hidden_size = 32  # Number of hidden units in the GRU\n",
    "        num_layers = 1  # Number of layers in the GRU\n",
    "        dropout = 0.2  # Dropout rate\n",
    "        \n",
    "        gru = nn.GRU(input_size=self.embedding_size, hidden_size=hidden_size, batch_first=True)\n",
    "        \n",
    "        e = self.encode_word(word)\n",
    "        # Initialize the hidden state h0\n",
    "        \n",
    "        h0 = torch.zeros(1, hidden_size)\n",
    "\n",
    "        \n",
    "        # Concatenate the encoder representation and start-of-word character\n",
    "        print(e.size())\n",
    "        print(self.hsowi.unsqueeze(1).size())\n",
    "        \n",
    "        x = torch.cat([self.hsowi.unsqueeze(0).float(), e.unsqueeze(0)], dim=1)\n",
    "\n",
    "        print(x.size())\n",
    "        print(h0.size())\n",
    "        \n",
    "        # Apply the GRU layer\n",
    "        out, h = gru(x)\n",
    "        \n",
    "        # Apply the output layer\n",
    "        out_l = nn.Linear(hidden_size, len(self.vocab))\n",
    "        \n",
    "        out = out_l(out)\n",
    "        \n",
    "        softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "        # Apply the softmax activation function\n",
    "        out = softmax(out)\n",
    "        \n",
    "        return out\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b8531",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class model:\n",
    "    def __init__(self, data, embedding_size):\n",
    "        self.sow = '$'  # the start of word character\n",
    "        self.eow = '£'  # the end of word character\n",
    "        self.data = data\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab = self.extract_dict()\n",
    "        self.char_index_dic = self.char_to_index()\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(self.vocab), embedding_dim=self.embedding_size)\n",
    "\n",
    "    def extract_dict(self):\n",
    "        dictt = []\n",
    "        for word in self.data:\n",
    "            for item in word:\n",
    "                tmp = set(item)\n",
    "                for k in tmp:\n",
    "                    if k not in dictt:\n",
    "                        dictt.append(k)\n",
    "        dictt.append(self.sow)\n",
    "        dictt.append(self.eow)\n",
    "        return dictt\n",
    "\n",
    "    def char_to_index(self):\n",
    "        char_to_idx_map = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        #print(char_to_idx_map)\n",
    "        return char_to_idx_map\n",
    "\n",
    "    def word_to_idx_seq(self, word):\n",
    "        word_char_idx_tensor = torch.tensor([self.char_index_dic[char] for char in word])\n",
    "        return word_char_idx_tensor\n",
    "\n",
    "    def encode_word(self, word):\n",
    "        \n",
    "        hidden_size = 32\n",
    "        bidirectional = True\n",
    "        num_layers = 1\n",
    "        \n",
    "        \n",
    "        gru = nn.GRU(input_size=self.embedding_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                     bidirectional=bidirectional, batch_first=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        word_seq = self.word_to_idx_seq(word)\n",
    "        embedded_vec = self.embedding(word_seq)\n",
    "        outputs, hidden = gru(embedded_vec)\n",
    "\n",
    "        h1 = torch.reshape(hidden[-1], (1, hidden_size))\n",
    "        h2 = torch.reshape(hidden[-2], (1, hidden_size))\n",
    "        hidden = torch.cat((h2, h1), 1)\n",
    "\n",
    "        encoder_output = torch.mean(hidden, dim=0)\n",
    "\n",
    "        return encoder_output\n",
    "\n",
    "    def decode_word(self, encoding):\n",
    "        input_size = encoding.shape[-1] + len(self.vocab)\n",
    "        hidden_size = 64\n",
    "        num_layers = 1\n",
    "\n",
    "        embedded_sow = self.embedding(torch.tensor(self.char_index_dic[self.sow]))\n",
    "        decoder_input = torch.cat((encoding, embedded_sow), dim=1)\n",
    "\n",
    "        gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "        outputs, hidden = gru(decoder_input.unsqueeze(0))\n",
    "        decoder_output = torch.mean(hidden, dim=0)\n",
    "\n",
    "        decoded = []\n",
    "        for i in range(self.max_word_length):\n",
    "            output = self.output_layer(decoder_output)\n",
    "            _, argmax = torch.max(output, dim=1)\n",
    "            decoded.append(argmax.item())\n",
    "            decoder_input = self.embedding(argmax)\n",
    "            decoder_input = torch.cat((decoder_input, encoding), dim=1)\n",
    "            decoder_output, hidden = gru(decoder_input.unsqueeze(0), hidden)\n",
    "            decoder_output = torch.mean(hidden, dim=0)\n",
    "\n",
    "            if argmax == self.eow_index:\n",
    "                break\n",
    "\n",
    "        decoded_word = ' '.join([self.index_char_dic[i] for i in decoded])\n",
    "        return decoded_word\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        #for instance in self.data:\n",
    "        word = self.data[0]\n",
    "        encoding = self.encode_word(word)\n",
    "        print(f'Encoding for {word}: {encoding}')\n",
    "\n",
    "        decoding = self.decode_word(encoding)\n",
    "        print(f'Decoding for {word}: {decoding}')\n",
    "        break\n",
    "\n",
    "mod = model(root_data, 10)\n",
    "mod.train()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a2290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e94deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee58704c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaad5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417ec90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d5ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f1ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb81318a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c788cfd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160fffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0bd253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b448e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77a906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd7ee52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3429b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed2aac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2619f608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bea57e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e852de90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = ''.join(dic)\n",
    "vowel_pattern = r'[\\u064e\\u064b\\u0650\\u064d\\u064f\\u064c\\u064f\\u0652\\u0651]'\n",
    "vowels = re.findall(vowel_pattern, st)\n",
    "l = []\n",
    "for char in st :\n",
    "    if char not in vowels : \n",
    "        l.append(char)\n",
    "        \n",
    "l = ''.join(l)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80bedd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ef987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b25293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f63a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a7be5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
