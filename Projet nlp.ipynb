{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ea0e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 15:18:27.823547: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-12 15:18:27.823623: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c89e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense , Activation , Dropout, Embedding, SpatialDropout1D, LSTM , TextVectorization, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb1349c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50137a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'corpus_morphological_analysis'\n",
    "file_paths = []\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    file_paths.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c5f5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file_paths = file_paths[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2602e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "#i = 0\n",
    "for filepath in temp_file_paths :\n",
    "    #print(i)\n",
    "    with open(filepath, encoding='utf-8') as f :\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()  \n",
    "    text = soup.get_text()\n",
    "    content.append(text)\n",
    "    #i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d7aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "104b5ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35a75b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_list = []\n",
    "for item in content : \n",
    "    tmp = item.splitlines()\n",
    "    split_list.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3544ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_list = []\n",
    "for k in split_list :\n",
    "    l = [item for item in k if 'لا توجد نتائج لتحليل هذه الكلمة' not in item]\n",
    "    tmp_l = [item.replace(\"#\",'') for item in l]\n",
    "    work_list.append(tmp_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "429730b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_list = []\n",
    "for k in work_list :\n",
    "    tst = [item.split(':') for item in k]\n",
    "    final_list.append(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9135fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifie le prefixe, racine et le suffixe et les placent dans un dictionnaire\n",
    "def identify(word_l):\n",
    "    if len(word_l) < 4 :\n",
    "        return None\n",
    "    dictt = {}\n",
    "    dictt['word'] = word_l[1]\n",
    "    # le cas s'il existe un préfixe\n",
    "    if word_l[2] != '' and word_l[2] != ' ' :   \n",
    "        if word_l[4] not in word_l[0]: \n",
    "            if word_l[5] in word_l[0] and word_l[5] != '': \n",
    "                dictt['prefixe'] = word_l[2]\n",
    "                dictt['root'] = word_l[8]\n",
    "                dictt['suffixe'] = word_l[9]\n",
    "            elif word_l[3] in word_l[0] and word_l[3] != '':\n",
    "                dictt['prefixe'] = word_l[2]\n",
    "                dictt['root'] = word_l[3]\n",
    "                dictt['suffixe'] = ''\n",
    "        else :\n",
    "            dictt['prefixe'] = word_l[2]\n",
    "            dictt['root'] = word_l[7]\n",
    "            dictt['suffixe'] = word_l[8]\n",
    "    # s'il n'existe pas un préfixe\n",
    "    else : \n",
    "        if word_l[2] == '' : \n",
    "            dictt['prefixe'] = word_l[2]\n",
    "            dictt['root'] = word_l[6]\n",
    "            dictt['suffixe'] = word_l[7]\n",
    "        elif  word_l[2] == ' ' :\n",
    "            dictt['prefixe'] = ''\n",
    "            dictt['root'] = word_l[3]\n",
    "            dictt['suffixe'] = ''    \n",
    "    return dictt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69a52da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtre la liste de mots en liste de dictionnaires\n",
    "def word_to_dict_list(wordlist):\n",
    "    dictlist = []\n",
    "    for k in wordlist : \n",
    "        dictlist.append(identify(k))\n",
    "    return dictlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "897b24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for k in final_list: \n",
    "    for j in k :\n",
    "        s = identify(j)\n",
    "        if s == None :\n",
    "            continue\n",
    "        final.append(identify(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cce4d9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'الْجَاهِزَةِ', 'prefixe': 'ال', 'root': 'جهز', 'suffixe': 'ة'}\n"
     ]
    }
   ],
   "source": [
    "print(final[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0fe335f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dic_to_list(listt):\n",
    "    L = []\n",
    "    for k in listt : \n",
    "        tmp = []\n",
    "        #print(k)\n",
    "        if len(k) == 4 : \n",
    "            tmp.append(k['word'])\n",
    "            tmp.append(k['prefixe'])\n",
    "            tmp.append(k['root'])\n",
    "            tmp.append(k['suffixe'])\n",
    "            L.append(tmp)\n",
    "    return L\n",
    "data = dic_to_list(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f81f4b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['مُسْقَطٍ', '', 'سقط', '']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_l = dic_to_list(final)\n",
    "final_l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "22118e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimenting with our first neural network to predict the root of a word.\n",
    "\n",
    "data_1 = []\n",
    "\n",
    "for word in final_l:\n",
    "    tmp = []\n",
    "    word[0] = word[0].replace('+', ' ')\n",
    "    word[2] = word[2].replace('+', ' ')\n",
    "    tmp.append(word[0])\n",
    "    tmp.append(word[2])\n",
    "    data_1.append(tmp)\n",
    "    #print(tmp)\n",
    "#data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f8ebb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we extract our dictionnary from the our dataset \n",
    "def extract_dict(listt) :\n",
    "    dictt = []\n",
    "    for word in listt :\n",
    "        for item in word : \n",
    "            tmp = set(item)\n",
    "            for k in tmp : \n",
    "                if k not in dictt : \n",
    "                    dictt.append(k)\n",
    "    return dictt       \n",
    "dic = extract_dict(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247cf020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cc451ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data = []\n",
    "for word in data : \n",
    "    tmp =[]\n",
    "    tmp.append(word[0])\n",
    "    tmp.append(word[2])\n",
    "    root_data.append(tmp)\n",
    "#root_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51d4a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encode :\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def code_sequence(self):\n",
    "        L = []\n",
    "        for word in self.data : \n",
    "            tmp_string = code_word(word)\n",
    "            L.append(tmp_string)\n",
    "        final_string = '#'.join(map(str,L))\n",
    "        return final_string \n",
    "    \n",
    "    def code_normal_text(self):\n",
    "        L = []\n",
    "        for word in self.data : \n",
    "            L.append(word[1])\n",
    "        final_string = ' '.join(map(str,L))\n",
    "        return final_string\n",
    "        \n",
    "final_str = encode(root_data).code_normal_text()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d08082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b091db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c250db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Define the GRU layer\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        \n",
    "        # Define the output layer\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Define the softmax activation function\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, e, hsowi):\n",
    "        \"\"\"\n",
    "        e: Encoder representation of the input word\n",
    "        hsowi: Start-of-word character\n",
    "        \n",
    "        Returns the predicted root word and the attention weights\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize the hidden state h0\n",
    "        h0 = torch.zeros(1, e.size(0), self.hidden_size)\n",
    "        \n",
    "        # Concatenate the encoder representation and start-of-word character\n",
    "        x = torch.cat([hsowi.unsqueeze(1), e], dim=1)\n",
    "        \n",
    "        # Apply the GRU layer\n",
    "        out, h = self.gru(x, h0)\n",
    "        \n",
    "        # Apply the output layer\n",
    "        out = self.out(out)\n",
    "        \n",
    "        # Apply the softmax activation function\n",
    "        out = self.softmax(out)\n",
    "        \n",
    "        # Return the predicted root word and the attention weights\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d5231259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of a paper : \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch \n",
    "import string\n",
    "\n",
    "\n",
    "class model : \n",
    "    \n",
    "    def __init__(self, data, embedding_size):\n",
    "        '''\n",
    "        our input data are arabic words with their roots, with the hypothesis that each word has it's own root.\n",
    "        \n",
    "        (our dataset for this is named root_data)\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab = self.extract_dict()\n",
    "        self.char_index_dic = self.char_to_index() \n",
    "        #self.hsowi = torch.tensor(self.char_index_dic[hsowi]).unsqueeze(0)\n",
    "\n",
    "    \n",
    "    def extract_dict(self) :\n",
    "        '''\n",
    "        this function extracts all the unique characters from the given dataset\n",
    "        '''\n",
    "        dictt = []\n",
    "        for word in self.data :\n",
    "            for item in word : \n",
    "                tmp = set(item)\n",
    "                for k in tmp : \n",
    "                    if k not in dictt : \n",
    "                        dictt.append(k)\n",
    "        return dictt \n",
    "    \n",
    "    \n",
    "    # Let's now construct our model : \n",
    "    \n",
    "    # we should think about character embeddings in order to create an embeded matrix for each word\n",
    "    \n",
    "    def char_to_index(self):\n",
    "        '''\n",
    "        this function creates unique indexes of each character\n",
    "        '''\n",
    "        char_to_idx_map = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        #print(f\"Character to Index Mapping:\\n{char_to_idx_map}\\n\")\n",
    "        return char_to_idx_map  \n",
    "    \n",
    "    \n",
    "    def word_to_idx_seq(self, word):\n",
    "        '''\n",
    "        this function returns a sequence of the unique indexes for the given word \n",
    "        (sequence is tensor that can be changed using a .tolist() )\n",
    "        '''\n",
    "        word_char_idx_tensor = torch.tensor([self.char_index_dic[char] for char in word])        \n",
    "        return word_char_idx_tensor # word sequence\n",
    "    \n",
    "    \n",
    "    def embedding(self, word_sequence):    \n",
    "        '''\n",
    "        this function creates a character based embedding of each word as a sequence of characters \n",
    "        '''\n",
    "        embedding_layer = nn.Embedding(num_embeddings = len(self.vocab), embedding_dim = self.embedding_size) \n",
    "        embedding_vector = embedding_layer(word_sequence)\n",
    "        return embedding_vector\n",
    "    \n",
    "    \n",
    "    def encode_word(self, word):\n",
    "        \n",
    "        hidden_size = 32  # Number of hidden units in the GRU\n",
    "        bidirectional = True  # Whether to use a bidirectional GRU or not\n",
    "        num_layers = 1  # Number of layers in the GRU\n",
    "        gru = nn.GRU(input_size=self.embedding_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        \n",
    "        #@jit update_function\n",
    "        \n",
    "        word_seq = self.word_to_idx_seq(word)\n",
    "        embedded_vec = self.embedding(word_seq)\n",
    "        #print(embedded_vec.size())\n",
    "        outputs, hidden = gru(embedded_vec)\n",
    "        \n",
    "        \"\"\"print(word)\n",
    "        print(hidden.size())\n",
    "        print(hidden[-2].size())\n",
    "        print(hidden[-1].size())\"\"\"\n",
    "        \n",
    "        # Concatenate the forward and backward hidden vectors        \n",
    "        h1 = torch.reshape(hidden[-1], (1,hidden_size))\n",
    "        h2 = torch.reshape(hidden[-2], (1,hidden_size))\n",
    "        hidden = torch.cat((h2, h1),1)  # Shape: (batch_size, hidden_size*2)\n",
    "        \n",
    "        encoder_output = torch.mean(hidden, dim=0)  # Average the hidden vectors across all time steps. Shape: (hidden_size*2,) if bidirectional, else (hidden_size,)\n",
    "     \n",
    "        return encoder_output\n",
    "    \n",
    "    \n",
    "    def decode_word(self, encoding):\n",
    "\n",
    "        num_layers = 1  # Number of layers in the GRU\n",
    "        hidden_size= 32 # Number of hidden elements in the GRU\n",
    "        input_size = encoding.size(0) + 1\n",
    "        gru = nn.GRU(input_size= input_size ,hidden_size = hidden_size, num_layers = num_layers) \n",
    "        \n",
    "        loss_fn = nn.CrossEn#tropyLoss()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        return \n",
    "\n",
    "    \n",
    "    \n",
    "    def main(self):\n",
    "        \n",
    "        test = 0\n",
    "        for instance in self.data : \n",
    "            word = instance[0]\n",
    "            test = self.encode_word(word)\n",
    "            size = test.size(0)\n",
    "            #out = self.decode_word(test)\n",
    "            \n",
    "            #print(word)\n",
    "            #print(out)\n",
    "            break\n",
    "            \n",
    "            \n",
    "            \n",
    "        return test    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "51c8969b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1097, -0.2494,  0.4551, -0.2999,  0.0094, -0.1662, -0.0855, -0.0695,\n",
       "         0.1338, -0.2104, -0.2010, -0.0823,  0.4442,  0.1741,  0.1659,  0.0608,\n",
       "        -0.3120,  0.1714,  0.0645,  0.1078,  0.3804, -0.4028,  0.0044,  0.2451,\n",
       "        -0.3783, -0.2358, -0.2026,  0.0716,  0.1852, -0.4934,  0.3521,  0.0986,\n",
       "         0.0717,  0.0531,  0.1596,  0.1790,  0.0125,  0.1555,  0.0510,  0.2361,\n",
       "         0.2106,  0.0768,  0.0861,  0.2359, -0.2281, -0.0086,  0.1196,  0.1971,\n",
       "         0.0827, -0.1407,  0.1999,  0.3002, -0.3455,  0.0017,  0.0266,  0.3917,\n",
       "         0.0902,  0.0425,  0.1267, -0.0176, -0.3630,  0.0590,  0.3767,  0.2752],\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = model(root_data, 10)\n",
    "mod.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7519da04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l = 0\\nfor item in root_data:\\n    if len(item[1]) > 3:\\n        print(item[1])'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''l = 0\n",
    "for item in root_data:\n",
    "    if len(item[1]) > 3:\n",
    "        print(item[1])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2427e875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f23ad99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def decode_word(self, word):\\n        \\n        hidden_size = 32  # Number of hidden units in the GRU\\n        num_layers = 1  # Number of layers in the GRU\\n        dropout = 0.2  # Dropout rate\\n        \\n        gru = nn.GRU(input_size=self.embedding_size, hidden_size=hidden_size, batch_first=True)\\n        \\n        e = self.encode_word(word)\\n        # Initialize the hidden state h0\\n        \\n        h0 = torch.zeros(1, hidden_size)\\n\\n        \\n        # Concatenate the encoder representation and start-of-word character\\n        print(e.size())\\n        print(self.hsowi.unsqueeze(1).size())\\n        \\n        x = torch.cat([self.hsowi.unsqueeze(0).float(), e.unsqueeze(0)], dim=1)\\n\\n        print(x.size())\\n        print(h0.size())\\n        \\n        # Apply the GRU layer\\n        out, h = gru(x)\\n        \\n        # Apply the output layer\\n        out_l = nn.Linear(hidden_size, len(self.vocab))\\n        \\n        out = out_l(out)\\n        \\n        softmax = nn.Softmax(dim=2)\\n        \\n        # Apply the softmax activation function\\n        out = softmax(out)\\n        \\n        return out\\n    '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def decode_word(self, word):\n",
    "        \n",
    "        hidden_size = 32  # Number of hidden units in the GRU\n",
    "        num_layers = 1  # Number of layers in the GRU\n",
    "        dropout = 0.2  # Dropout rate\n",
    "        \n",
    "        gru = nn.GRU(input_size=self.embedding_size, hidden_size=hidden_size, batch_first=True)\n",
    "        \n",
    "        e = self.encode_word(word)\n",
    "        # Initialize the hidden state h0\n",
    "        \n",
    "        h0 = torch.zeros(1, hidden_size)\n",
    "\n",
    "        \n",
    "        # Concatenate the encoder representation and start-of-word character\n",
    "        print(e.size())\n",
    "        print(self.hsowi.unsqueeze(1).size())\n",
    "        \n",
    "        x = torch.cat([self.hsowi.unsqueeze(0).float(), e.unsqueeze(0)], dim=1)\n",
    "\n",
    "        print(x.size())\n",
    "        print(h0.size())\n",
    "        \n",
    "        # Apply the GRU layer\n",
    "        out, h = gru(x)\n",
    "        \n",
    "        # Apply the output layer\n",
    "        out_l = nn.Linear(hidden_size, len(self.vocab))\n",
    "        \n",
    "        out = out_l(out)\n",
    "        \n",
    "        softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "        # Apply the softmax activation function\n",
    "        out = softmax(out)\n",
    "        \n",
    "        return out\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "448c16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = []\n",
    "for item in root_data : \n",
    "    tmp = []\n",
    "    tmp.append(item[0])\n",
    "    tmp.append('$'+item[1])\n",
    "    data_root.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# create an embedding layer with 50 input dimensions and 100 output dimensions\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=43, output_dim=100)\n",
    "sequence = [5 , 10 , 10, 20 , 7, 1]\n",
    "# get the embedding vectors for the sequence of character indices\n",
    "embedding_vectors = embedding_layer(np.array(sequence))\n",
    "embedding_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e94deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 32  # Number of hidden units in the GRU\n",
    "bidirectional = True  # Whether to use a bidirectional GRU or not\n",
    "num_layers = 1  # Number of layers in the GRU\n",
    "dropout = 0.2  # Dropout rate\n",
    "gru = nn.GRU(input_size=16, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee58704c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaad5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d5ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx_map = {char: idx for idx, char in enumerate(dic)}\n",
    "print(f\"Character to Index Mapping:\\n{char_to_idx_map}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_characters = torch.eye(n=len(dic))  # using the eye method for identity matrix\n",
    "print(f\"One hot encoded characters:\\n{ohe_characters}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb81318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_characters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c788cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_repr_a = ohe_characters[char_to_idx_map['ح']]\n",
    "print(f\"One hot encoded representation of 1:\\n{ohe_repr_a}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word, char_to_idx_map):\n",
    "    \n",
    "    word_char_idx_tensor = torch.tensor([char_to_idx_map[char] for char in word])\n",
    "    word_ohe_repr = ohe_characters[word_char_idx_tensor].T\n",
    "    \n",
    "    return word_ohe_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0bd253",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_char_idx_tensor = torch.tensor([char_to_idx_map[char] for char in \"مَرْحَبًا\"])\n",
    "print(word_char_idx_tensor.tolist())\n",
    "word_ohe_repr = ohe_characters[word_char_idx_tensor].T\n",
    "word_ohe_repr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b448e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "convolution_layer = nn.Conv1d(in_channels=43, out_channels=128, kernel_size=3, bias=False)\n",
    "conv_out = convolution_layer(word_ohe_repr.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of convolution output: {conv_out.shape}\")\n",
    "print(f\"Convolution Output:\\n{conv_out}\\n\")\n",
    "\n",
    "# adding an activation layer so as to add non linearity to the output\n",
    "activation_layer = nn.Tanh()\n",
    "activation_out = activation_layer(conv_out)\n",
    "activation_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd7ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of activation output: {activation_out.shape}\")\n",
    "print(f\"Activation Output:\\n{activation_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling Layer\n",
    "max_pooling_layer = nn.MaxPool1d(kernel_size=conv_out.shape[2])\n",
    "\n",
    "# finally here is our word embedding of size -> 256\n",
    "word_embedding = max_pooling_layer(activation_out).squeeze()\n",
    "\n",
    "print(f\"Final word embedding shape: {word_embedding.shape}\")\n",
    "print(f\"Final word embedding:\\n{word_embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed2aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_embedding():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2619f608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bea57e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e852de90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27004d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2c27d12",
   "metadata": {},
   "source": [
    "### Let's create a data structure for each word of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ef987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = work_list[2]\n",
    "tmp.split(\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b25293",
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel_pattern = r'[\\u064b\\u064e\\u064f\\u0650\\u0651\\u0652]'\n",
    "word = \"مَرْحَبًا\"\n",
    "vowels = re.findall(vowel_pattern, word)\n",
    "print(f\"Vowels in '{word}': {vowels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f63a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''filtered_split_list = []\n",
    "for item in split_list : \n",
    "    temp_item_list = []\n",
    "    for subitem in item : \n",
    "        if 'لا توجد نتائج لتحليل هذه الكلمة' not in item : \n",
    "            temp_item_list.append(item)\n",
    "    filtered_split_list.append(temp_item_list)\n",
    "filtered_split_list[0]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a7be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Check the data type of each element in texts\n",
    "texts = pd.DataFrame(data_1[:, -1])\n",
    "texts = texts.iloc[:, 1:]\n",
    "for i, text in enumerate(texts):\n",
    "    if not isinstance(text, str):\n",
    "        print(f\"Element {i} is not a string: {text}\")'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
