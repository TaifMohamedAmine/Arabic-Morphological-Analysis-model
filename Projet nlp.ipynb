{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14ea0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c89e9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 01:02:06.984161: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-19 01:02:06.984225: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense , Activation , Dropout, Embedding, SpatialDropout1D, LSTM , TextVectorization, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f7603ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb1349c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50137a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'corpus_morphological_analysis'\n",
    "file_paths = []\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    file_paths.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8c5f5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file_paths = file_paths[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2602e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "#i = 0\n",
    "for filepath in temp_file_paths :\n",
    "    #print(i)\n",
    "    with open(filepath, encoding='utf-8') as f :\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()  \n",
    "    text = soup.get_text()\n",
    "    content.append(text)\n",
    "    #i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d7aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b5ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "35a75b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_list = []\n",
    "for item in content : \n",
    "    tmp = item.splitlines()\n",
    "    split_list.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d3544ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_list = []\n",
    "for k in split_list :\n",
    "    l = [item for item in k if 'لا توجد نتائج لتحليل هذه الكلمة' not in item]\n",
    "    tmp_l = [item.replace(\"#\",'') for item in l]\n",
    "    work_list.append(tmp_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "429730b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_list = []\n",
    "for k in work_list :\n",
    "    tst = [item.split(':') for item in k]\n",
    "    final_list.append(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9135fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifie le prefixe, racine et le suffixe et les placent dans un dictionnaire\n",
    "def identify(word_l):\n",
    "    if len(word_l) < 4 :\n",
    "        return None\n",
    "    dictt = {}\n",
    "    dictt['word'] = word_l[0]\n",
    "    # le cas s'il existe un préfixe\n",
    "    if word_l[2] != '' and word_l[2] != ' ' :   \n",
    "        if word_l[4] not in word_l[0]: \n",
    "            if word_l[5] in word_l[0] and word_l[5] != '': \n",
    "                dictt['prefixe'] = word_l[2]\n",
    "                dictt['root'] = word_l[8]\n",
    "                dictt['suffixe'] = word_l[9]\n",
    "            elif word_l[3] in word_l[0] and word_l[3] != '':\n",
    "                dictt['prefixe'] = word_l[2]\n",
    "                dictt['root'] = word_l[3]\n",
    "                dictt['suffixe'] = ''\n",
    "        else :\n",
    "            dictt['prefixe'] = word_l[2]\n",
    "            dictt['root'] = word_l[7]\n",
    "            dictt['suffixe'] = word_l[8]\n",
    "    # s'il n'existe pas un préfixe\n",
    "    else : \n",
    "        if word_l[2] == '' : \n",
    "            dictt['prefixe'] = word_l[2]\n",
    "            dictt['root'] = word_l[6]\n",
    "            dictt['suffixe'] = word_l[7]\n",
    "        elif  word_l[2] == ' ' :\n",
    "            dictt['prefixe'] = ''\n",
    "            dictt['root'] = word_l[3]\n",
    "            dictt['suffixe'] = ''    \n",
    "    return dictt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807dc08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "69a52da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtre la liste de mots en liste de dictionnaires\n",
    "def word_to_dict_list(wordlist):\n",
    "    dictlist = []\n",
    "    for k in wordlist : \n",
    "        dictlist.append(identify(k))\n",
    "    return dictlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "897b24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for k in final_list: \n",
    "    for j in k :\n",
    "        s = identify(j)\n",
    "        if s == None :\n",
    "            continue\n",
    "        final.append(identify(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce4d9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f0fe335f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dic_to_list(listt):\n",
    "    L = []\n",
    "    for k in listt : \n",
    "        tmp = []\n",
    "        #print(k)\n",
    "        if len(k) == 4 : \n",
    "            tmp.append(k['word'])\n",
    "            tmp.append(k['prefixe'])\n",
    "            tmp.append(k['root'])\n",
    "            tmp.append(k['suffixe'])\n",
    "            L.append(tmp)\n",
    "    return L\n",
    "data = dic_to_list(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad205283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f81f4b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['مسقط', '', 'سقط', '']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_l = dic_to_list(final)\n",
    "final_l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "22118e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimenting with our first neural network to predict the root of a word.\n",
    "\n",
    "data_1 = []\n",
    "\n",
    "for word in final_l:\n",
    "    tmp = []\n",
    "    word[0] = word[0].replace('+', ' ')\n",
    "    word[2] = word[2].replace('+', ' ')\n",
    "    tmp.append(word[0])\n",
    "    tmp.append(word[2])\n",
    "    data_1.append(tmp)\n",
    "    #print(tmp)\n",
    "#data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9f8ebb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we extract our dictionnary from the our dataset \n",
    "def extract_dict(listt) :\n",
    "    dictt = []\n",
    "    for word in listt :\n",
    "        for item in word : \n",
    "            tmp = set(item)\n",
    "            for k in tmp : \n",
    "                if k not in dictt : \n",
    "                    dictt.append(k)\n",
    "    return dictt       \n",
    "dic = extract_dict(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247cf020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cc451ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data = []\n",
    "for word in data : \n",
    "    tmp =[]\n",
    "    tmp.append(word[0])\n",
    "    tmp.append(word[2])\n",
    "    root_data.append(tmp)\n",
    "#root_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51d4a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encode :\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def code_sequence(self):\n",
    "        L = []\n",
    "        for word in self.data : \n",
    "            tmp_string = code_word(word)\n",
    "            L.append(tmp_string)\n",
    "        final_string = '#'.join(map(str,L))\n",
    "        return final_string \n",
    "    \n",
    "    def code_normal_text(self):\n",
    "        L = []\n",
    "        for word in self.data : \n",
    "            L.append(word[1])\n",
    "        final_string = ' '.join(map(str,L))\n",
    "        return final_string\n",
    "        \n",
    "final_str = encode(root_data).code_normal_text()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d08082",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1c3dbc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397396"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "90b091db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = []\n",
    "for item in root_data : \n",
    "    tmp = []\n",
    "    if len(item[1]) <= 3 and len(item[1]) != 0:\n",
    "        tmp.append('$'+item[0]+'£')\n",
    "        tmp.append('$'+item[1]+'£')\n",
    "        data_root.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d85cf4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32e29629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_padding(dat, padding_char):\n",
    "    #Le'ts create a padding character : \n",
    "    pad_char = padding_char\n",
    "    padded_data = []\n",
    "    \n",
    "    ls_words = []\n",
    "    ls_roots = []\n",
    "\n",
    "    for instance in dat : \n",
    "        ls_words.append(instance[0])\n",
    "        ls_roots.append(instance[1])\n",
    "    max_len_words = max([len(item) for item in ls_words])\n",
    "    max_len_roots = max([len(item) for item in ls_roots])\n",
    "    \n",
    "    for instance in dat: \n",
    "        tmp = []\n",
    "        word,root = instance[0], instance[1]\n",
    "        while(len(word) != max_len_words):\n",
    "            word += pad_char\n",
    "        tmp.append(word)\n",
    "        while(len(root) != max_len_roots):\n",
    "            root += pad_char\n",
    "        tmp.append(root)\n",
    "        padded_data.append(tmp)\n",
    "        \n",
    "    return padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11f4f468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['$مسقط£%%%%%%%%', '$سقط£'],\n",
       " ['$يقام£%%%%%%%%', '$قمي£'],\n",
       " ['$الرابع£%%%%%%', '$ربع£'],\n",
       " ['$والعشرين£%%%%', '$عشر£'],\n",
       " ['$من£%%%%%%%%%%', '$من£%'],\n",
       " ['$شهر£%%%%%%%%%', '$شهر£'],\n",
       " ['$فبراير£%%%%%%', '$رير£'],\n",
       " ['$المقبل£%%%%%%', '$قبل£'],\n",
       " ['$قصر£%%%%%%%%%', '$قصر£'],\n",
       " ['$البستان£%%%%%', '$بسس£']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_padding(data_root,'%')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7246f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    def train_batch(self, batch):\n",
    "                \n",
    "        self.optimizer.zero_grad()\n",
    "        for instance in batch: \n",
    "            '''\n",
    "            word = instance[0] : the word to extract root from\n",
    "            target_word = instance[1] : the root of the word\n",
    "            '''\n",
    "            word = instance[0]\n",
    "            target_root = instance[1]\n",
    "            \n",
    "            res1 = self.encode_word(word)\n",
    "            predicted_root = []\n",
    "            original_root_seq = self.word_to_idx_seq(target_root)\n",
    "            predicted_root_seq = []\n",
    "            res_char = target_root[0]\n",
    "            \n",
    "            i = 0\n",
    "            for char in target_root : \n",
    "                if len(predicted_root) == len(target_root):\n",
    "                    break\n",
    "                \n",
    "                \n",
    "                if random.random() < self.teacher_forcing_ratio and i!=0 : \n",
    "                    res_char = char\n",
    "                    i+= 1\n",
    "                \n",
    "                res_char , hidd , idx = self.decode_word(res1, res_char)\n",
    "                predicted_root_seq.append(idx)\n",
    "                test = hidd\n",
    "                predicted_root.append(res_char)\n",
    "\n",
    "            predicted_root = ''.join(predicted_root)\n",
    "\n",
    "            predicted_root_seq = torch.tensor(predicted_root_seq)\n",
    "\n",
    "            org = self.embedding(original_root_seq)\n",
    "            pred = self.embedding(predicted_root_seq)\n",
    "            \n",
    "            \n",
    "            loss = self.criterion(org, pred)\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            print('target root :', target_root)\n",
    "            print('predicted root : ', predicted_root)\n",
    "\n",
    "            print('the loss : ', loss.item())  \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return\n",
    "    \n",
    "            \n",
    "            \n",
    "        #print(em1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95150815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(self):\n",
    "    \n",
    "    #Le'ts create a padding for ouriinstances : \n",
    "    \n",
    "    pad_char = ''\n",
    "    padded_data = []\n",
    "    ls_words = []\n",
    "    ls_roots = []\n",
    "    for instance in self.data : \n",
    "        ls_words.append(instance[0])\n",
    "        ls_roots.append(instance[1])\n",
    "    max_len_words = max([len(item) for item in ls_words])\n",
    "    max_len_roots = max([len(item) for item in ls_roots])\n",
    "    \n",
    "    for instance in self.data: \n",
    "        tmp = []\n",
    "        word,root = instance[0], instance[1]\n",
    "        while(len(word) != max_len_words):\n",
    "            word += pad_char\n",
    "        tmp.append(word)\n",
    "        while(len(root) != max_len_roots):\n",
    "            root += pad_char\n",
    "        tmp.append(root)\n",
    "        padded_data.append(tmp)\n",
    "    \n",
    "\n",
    "    # let's create our vocab : \n",
    "    \n",
    "    vocab = []\n",
    "    for word in padded_data :\n",
    "        for item in word : \n",
    "            tmp = set(item)\n",
    "            for k in tmp : \n",
    "                if k not in vocab : \n",
    "                    vocab.append(k)\n",
    "    \n",
    "    \n",
    "    # Let's create our dictionnary with unique indexes\n",
    "    \n",
    "    char_to_idx_map = {char: idx for idx, char in enumerate(dictt)}\n",
    "    \n",
    "    # Let's now split our data to batches\n",
    "   \n",
    "    final_data = []\n",
    "    for instance in padded_data : \n",
    "        tmp = []\n",
    "        word = self.word_to_seq(instance[0])\n",
    "        root = self.word_to_seq(instance[1])\n",
    "        tmp.append(word)\n",
    "        tmp.append(root)\n",
    "        final_data.append(tmp)\n",
    "        \n",
    "    size= self.batch_size \n",
    "    batches = [final_data[i:i + size] for i in range(0, len(final_data), size)]\n",
    "    \n",
    "    return batches , vocab , char_to_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d695d5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c250db0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# implementation of a paper : \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch \n",
    "import string\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "\n",
    "class model(nn.Module): \n",
    "    \n",
    "    # (data,\n",
    "    \n",
    "    def __init__(self, data, batch_size ,embedding_size, num_layers,dropout, teacher_forcing_ratio, learning_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''\n",
    "        our input data are arabic words with their roots, with the hypothesis that each word has it's own root.\n",
    "        \n",
    "        (our dataset for this is named root_data)\n",
    "        '''\n",
    "        self.lr = learning_rate\n",
    "        self.ratio = 0.75\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.batches, self.vocab, self.char_index_dic = self.prepare_data(self.data)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = len(self.vocab)\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embedding(num_embeddings = len(self.vocab), embedding_dim = self.embedding_size, padding_idx = self.char_index_dic['%']) \n",
    "        \n",
    "        \n",
    "        #self.bigru = nn.GRU(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=self.num_layers, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.BILSTM = nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=self.num_layers, bidirectional=True, batch_first=True, dropout = self.dropout)\n",
    "\n",
    "        \n",
    "        #self.gru = nn.GRU(input_size= self.embedding_size ,hidden_size = self.hidden_size * 2, num_layers = self.num_layers, batch_first = True)\n",
    "        self.LSTM = nn.LSTM(input_size= self.embedding_size ,hidden_size = self.hidden_size*2, num_layers = int(self.num_layers / self.num_layers), batch_first = True)\n",
    "\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index =self.char_index_dic['%'])\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.Linear = nn.Linear(self.hidden_size * 2,len(self.vocab))\n",
    "        \n",
    "        #self.optimizer = optim.Adam([*self.bigru.parameters(), *self.gru.parameters(), *self.Linear.parameters()], lr = 0.001)\n",
    "        #self.optimizer = optim.Adam([*self.BILSTM.parameters(), *self.LSTM.parameters()], lr = 0.1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = self.lr)\n",
    "\n",
    "    \n",
    "    def prepare_data(self, data):\n",
    "    \n",
    "        #Le'ts create a padding for ouriinstances : \n",
    "\n",
    "        pad_char = '%'\n",
    "        padded_data = []\n",
    "        ls_words = []\n",
    "        ls_roots = []\n",
    "        for instance in data : \n",
    "            ls_words.append(instance[0])\n",
    "            ls_roots.append(instance[1])\n",
    "        \n",
    "        # Let's calculate the biggest length\n",
    "        max_len_words = max([len(item) for item in ls_words])\n",
    "        max_len_roots = max([len(item) for item in ls_roots])\n",
    "\n",
    "        # Now we pad the word until we reach the max length\n",
    "        for instance in data: \n",
    "            tmp = []\n",
    "            word,root = instance[0], instance[1]\n",
    "            while(len(word) != max_len_words):\n",
    "                word += pad_char\n",
    "            tmp.append(word)\n",
    "            while(len(root) != max_len_roots):\n",
    "                root += pad_char\n",
    "            tmp.append(root)\n",
    "            padded_data.append(tmp)\n",
    "\n",
    "        # let's create our vocab : \n",
    "\n",
    "        vocab = []\n",
    "        for word in padded_data :\n",
    "            for item in word : \n",
    "                tmp = set(item)\n",
    "                for k in tmp : \n",
    "                    if k not in vocab : \n",
    "                        vocab.append(k)\n",
    "\n",
    "        # Let's create our dictionnary with unique indexes\n",
    "\n",
    "        char_to_idx_map = {char: idx for idx, char in enumerate(vocab)}\n",
    "\n",
    "        # Let's now split our data to batches\n",
    "\n",
    "        final_data = []\n",
    "        for instance in padded_data : \n",
    "            tmp = []\n",
    "            word = [char_to_idx_map[char] for char in instance[0]]\n",
    "            root = [char_to_idx_map[char] for char in instance[1]]\n",
    "            tmp.append(word)\n",
    "            tmp.append(root)\n",
    "            final_data.append(tmp)\n",
    "\n",
    "        size= self.batch_size \n",
    "        batches = [final_data[i:i + size] for i in range(0, len(final_data), size)]\n",
    "        \n",
    "        return batches , vocab , char_to_idx_map\n",
    "    \n",
    "    \n",
    "    def word_to_seq(self, word):\n",
    "        '''\n",
    "        this function returns a sequence of the unique indexes for the given word \n",
    "        (sequence is tensor that can be changed using a .tolist() )\n",
    "        '''\n",
    "        word_char_idx_seq =[self.char_index_dic[char] for char in word]    \n",
    "        return word_char_idx_seq # word sequence\n",
    "    \n",
    "    \n",
    "    # Let's now construct our model : \n",
    "    \n",
    "    # we should think about character embeddings in order to create an embeded matrix for each word\n",
    "        \n",
    "    \n",
    "    \n",
    "    def encode(self, batch):    \n",
    "        '''\n",
    "        input : a batch of sequences of instances : [word_seq , root_seq] * batch_size\n",
    "                input_size : (input_size,2)\n",
    "        '''\n",
    "        \n",
    "        word_batch = [] # list of words in the batch\n",
    "        root_batch = [] # list of roots in the batch\n",
    "        \n",
    "        for instance in batch : \n",
    "            word_batch.append(instance[0])\n",
    "            root_batch.append(instance[1])\n",
    "            \n",
    "        word_batch = torch.tensor(word_batch)\n",
    "        root_batch = torch.tensor(root_batch)\n",
    "        \n",
    "        # we create embedding of the word batch : \n",
    "        \n",
    "        embedded_word_batch = self.embedding(word_batch)\n",
    "                \n",
    "        outputs, (hidden, cell) = self.BILSTM(embedded_word_batch) # we pass the emebedded vector through the bi-GRU \n",
    "        # we take the last layer input : \n",
    "        \n",
    "        hid , ce = hidden[-2:, :, :] , cell[-2:, :, :]\n",
    "                        \n",
    "        final_hidden = torch.unsqueeze(torch.cat((hid[0], hid[1]), dim = 1), 0)\n",
    "        final_cell = torch.unsqueeze(torch.cat((ce[0], ce[1]), dim = 1), 0)\n",
    "            \n",
    "        return root_batch , (final_hidden, final_cell)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def decode(self, encoder_hidden_cell , batch, teacher_forcing_bool):\n",
    "        \n",
    "        '''\n",
    "        input : encoding_hidden_layer => corresponds to the concatenation of the final hidden layers \n",
    "                                        of the bidirectionnal gru in our encoder\n",
    "                \n",
    "                batch : subset of data that contains the roots of the words we encoded.\n",
    "                \n",
    "        output : we'll see :) \n",
    "        \n",
    "        '''\n",
    "\n",
    "        (hidden_layer , cell) , root_batch = encoder_hidden_cell , batch \n",
    "                        \n",
    "        embedded_char = self.embedding(torch.unsqueeze(root_batch[:, 0], 1))\n",
    "            \n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(root_batch.size(1)): \n",
    "            \n",
    "            decoder_output , (hidden_layer, cell) = self.LSTM(embedded_char, (hidden_layer, cell))\n",
    "                        \n",
    "            input_dense = nn.Linear(self.hidden_size * 2,self.embedding_size)\n",
    "            input_decoder_output = input_dense(decoder_output)\n",
    "            \n",
    "            embedded_char = input_decoder_output\n",
    "        \n",
    "\n",
    "            mask = np.where([random.random() < self.teacher_forcing_ratio for i in range(root_batch.size(0))])[0]\n",
    "            \n",
    "            teacher_forcing_input = self.embedding(torch.unsqueeze(torch.clone(root_batch[:, i]), 1))\n",
    "            \n",
    "            if teacher_forcing_bool : \n",
    "\n",
    "                embedded_char[mask] = teacher_forcing_input[mask] \n",
    "                \n",
    "            Dense_decoded_output = self.Linear(decoder_output)\n",
    "            outputs.append(Dense_decoded_output)\n",
    "\n",
    "        return outputs \n",
    "                            \n",
    "        \n",
    "    \n",
    "    def train_model(self, batches, teacher_forcing_bool):\n",
    "                \n",
    "        train_batches = batches        \n",
    "         \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        n = 0            \n",
    "        \n",
    "        for batch in train_batches :\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            root_batch, encoder_states = self.encode(batch)\n",
    "\n",
    "            outputs = self.decode(encoder_states, root_batch, teacher_forcing_bool)\n",
    "\n",
    "            a = [torch.squeeze(item, 1) for item in outputs]\n",
    "            a = [torch.unsqueeze(item, 0) for item in a]\n",
    "\n",
    "            output = torch.cat(a, dim = 0)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "\n",
    "            trg = root_batch.transpose(0, 1)\n",
    "\n",
    "            trg = trg[1:].reshape(-1)\n",
    "\n",
    "            loss = self.criterion(output, trg)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_loss+=loss.item()\n",
    "\n",
    "            n+=1\n",
    "\n",
    "            print('the loss of the train batch ', n ,' is : ', loss.item())\n",
    "    \n",
    "        return epoch_loss/n\n",
    "\n",
    "    def evaluate_model(self, batches, teacher_forcing_bool):\n",
    "        '''\n",
    "        this method evaluates our model :=)\n",
    "        will be similar to train but without the teacher forcing/ using an optimizer \n",
    "        '''          \n",
    "        self.eval()\n",
    "\n",
    "        val_batches = batches\n",
    "\n",
    "        n = 0\n",
    "\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        with torch.no_grad() :\n",
    "\n",
    "            for batch in val_batches :\n",
    "\n",
    "                root_batch, encoder_states = self.encode(batch)\n",
    "\n",
    "                outputs = self.decode(encoder_states, root_batch, teacher_forcing_bool)\n",
    "\n",
    "                a = [torch.squeeze(item, 1) for item in outputs]\n",
    "                a = [torch.unsqueeze(item, 0) for item in a]\n",
    "\n",
    "                output = torch.cat(a, dim = 0)\n",
    "\n",
    "                output_dim = output.shape[-1]\n",
    "\n",
    "                output = output[1:].view(-1, output_dim)\n",
    "\n",
    "                trg = root_batch.transpose(0, 1)\n",
    "\n",
    "                trg = trg[1:].reshape(-1)\n",
    "\n",
    "                loss = self.criterion(output, trg)\n",
    "\n",
    "                epoch_loss+=loss.item()\n",
    "\n",
    "                n+=1\n",
    "\n",
    "                print('the loss of the val batch ', n ,' is : ', loss.item())\n",
    "\n",
    "        return epoch_loss / n\n",
    "\n",
    "    def fit(self, num_epochs):\n",
    "        \n",
    "        \"\"\"\n",
    "        let's first prepare our data\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        data = self.data\n",
    "        \n",
    "        data = random.sample(data, len(data))\n",
    "        data_size = len(data)\n",
    "        middle_index = int(data_size * self.ratio)        \n",
    "        train_data , val_data = data[:middle_index], data[middle_index:]\n",
    "        \n",
    "        train_batches, voc, dic = self.prepare_data(train_data)\n",
    "        val_batches ,voc , dic = self.prepare_data(val_data)\n",
    "        \n",
    "        epochs = list(range(num_epochs))\n",
    "        \n",
    "        best_val_loss = 1000\n",
    "        best_model_par = 0\n",
    "        \n",
    "        losses =[]\n",
    "        \n",
    "        for epoch in epochs : \n",
    "            \n",
    "            print('epoch num : ', epoch) \n",
    "            \n",
    "            t1 = time.time()\n",
    "            \n",
    "            #train_batches = random.sample(train_batches , len(train_batches))\n",
    "            #val_batches = random.sample(val_batches, len(val_batches))\n",
    "                        \n",
    "            train_loss = self.train_model(train_batches, 1)\n",
    "            val_loss = self.evaluate_model(val_batches, 0) # we set the teacher forcing to false\n",
    "            tmp = [train_loss , val_loss]\n",
    "            losses.append(tmp)\n",
    "            \n",
    "            t2 = time.time()\n",
    "            \n",
    "            tmp = [train_loss, val_loss]\n",
    "            losses.append(tmp)\n",
    "            \n",
    "            print('the training loss : ', train_loss , 'the val loss :', val_loss)\n",
    "            print('epoch num : ' ,epoch , ' lasted : ', t2 - t1 , 'seconds')\n",
    "            \n",
    "            if val_loss < best_val_loss :\n",
    "                \n",
    "                best_val_loss = val_loss \n",
    "                best_model_par = self.state_dict()\n",
    "            \n",
    "            torch.save(best_model_par, 'best_model.pt')\n",
    "            \n",
    "        return losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "46c9388d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1916736936.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_5887/1916736936.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    res = mod.fit(10)*\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mod = model(data_root, 1000, 39, 8, 0.1 ,0.3, 0.01)\n",
    "res = mod.fit(10)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7e80414e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = np.array(res)\n",
    "plt.figure()\n",
    "x = list(range(10))\n",
    "plt.plot(x, res[:,0])\n",
    "#plt.plot(x, res[:,1])\n",
    "plt.title(\"evolution of loss with epochs\")\n",
    "plt.xlabel('epochs ')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df568b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b231012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849be848",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''temp_soft = tuple(temp_soft)\n",
    "        \n",
    "        res = torch.tensor(res)\n",
    "        \n",
    "        tst = torch.cat(temp_soft, dim = 1)      \n",
    "        \n",
    "        predicted_sequences = []\n",
    "\n",
    "        for instance in tst :\n",
    "\n",
    "            best_char_indexes = [torch.argmax(item).item() for item in instance]\n",
    "\n",
    "            predicted_sequences.append(best_char_indexes)\n",
    "\n",
    "        #print(torch.tensor(predicted_sequences).size(), '*'*100)\n",
    "        \n",
    "        char_batch = torch.tensor(predicted_sequences)\n",
    "        \n",
    "        #print(char_batch)\n",
    "                \n",
    "        #print('predicted : ', predicted_roots.size(), 'original :', root_batch.size())           \n",
    "        \n",
    "        key_list = list(self.char_index_dic.keys())\n",
    "        val_list = list(self.char_index_dic.values())\n",
    "\n",
    "        pos_all = []\n",
    "\n",
    "        for seq in char_batch : \n",
    "\n",
    "            positions = [val_list.index(item) for item in seq]\n",
    "            result_chars = [key_list[pos] for pos in positions]\n",
    "            char = ''.join(result_chars)\n",
    "\n",
    "            pos_all.append(char)\n",
    "\n",
    "        #print(pos_all)\n",
    "                \n",
    "        return root_batch , res , pos_all\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c16f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68cfd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5231259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of a paper : \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch \n",
    "import string\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "\n",
    "class model(nn.Module): \n",
    "    \n",
    "    def __init__(self, data, batch_size ,embedding_size, hidden_size, num_layers, teacher_forcing_ratio):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''\n",
    "        our input data are arabic words with their roots, with the hypothesis that each word has it's own root.\n",
    "        \n",
    "        (our dataset for this is named root_data)\n",
    "        '''\n",
    "        self.sow = '$' # the start of root character \n",
    "        self.eow = '£' # the end of root character\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab = self.extract_dict()\n",
    "        self.char_index_dic = self.char_to_index()\n",
    "        print(self.char_index_dic)\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(num_embeddings = len(self.vocab), embedding_dim = self.embedding_size) \n",
    "        self.bigru = nn.GRU(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=self.num_layers, bidirectional=True)\n",
    "        self.gru = nn.GRU(input_size= self.embedding_size ,hidden_size = self.hidden_size, num_layers = self.num_layers)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.Linear = nn.Linear(self.hidden_size,len(self.vocab))\n",
    "        self.optimizer = optim.Adam([*self.bigru.parameters(), *self.gru.parameters(), *self.Linear.parameters()], lr = 0.1)\n",
    "\n",
    "    \n",
    "    def extract_dict(self) :\n",
    "        '''\n",
    "        this function extracts all the unique characters from the given dataset\n",
    "        '''\n",
    "        dictt = []\n",
    "        for word in self.data :\n",
    "            for item in word : \n",
    "                tmp = set(item)\n",
    "                for k in tmp : \n",
    "                    if k not in dictt : \n",
    "                        dictt.append(k)\n",
    "        return dictt \n",
    "    \n",
    "    \n",
    "    # Let's now construct our model : \n",
    "    \n",
    "    # we should think about character embeddings in order to create an embeded matrix for each word\n",
    "    \n",
    "    def char_to_index(self):\n",
    "        '''\n",
    "        this function creates unique indexes of each character\n",
    "        '''\n",
    "        char_to_idx_map = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        return char_to_idx_map  \n",
    "    \n",
    "    def data_batches(self): \n",
    "        size = self.batch_size\n",
    "        \n",
    "        batches = [self.data[i:i + size] for i in range(0, len(self.data), size)]\n",
    "        \n",
    "        return batches\n",
    "        \n",
    "    \n",
    "    def word_to_idx_seq(self, word):\n",
    "        '''\n",
    "        this function returns a sequence of the unique indexes for the given word \n",
    "        (sequence is tensor that can be changed using a .tolist() )\n",
    "        '''\n",
    "        word_char_idx_tensor = torch.tensor([self.char_index_dic[char] for char in word])        \n",
    "        return word_char_idx_tensor # word sequence\n",
    "    \n",
    "    def word_to_seq(self, word):\n",
    "        '''\n",
    "        this function returns a sequence of the unique indexes for the given word \n",
    "        (sequence is tensor that can be changed using a .tolist() )\n",
    "        '''\n",
    "        word_char_idx_tensor =[self.char_index_dic[char] for char in word]    \n",
    "        return word_char_idx_tensor # word sequence\n",
    "    \n",
    "    \n",
    "    def encode_word(self, word):\n",
    "        \n",
    "            \n",
    "        # we create embedding of the sequence : \n",
    "        word_seq = self.word_to_idx_seq(word)\n",
    "        embedded_vec = self.embedding(word_seq)\n",
    "        \n",
    "        outputs, hidden = self.bigru(embedded_vec) # we pass the emebedded vector through the bi-GRU \n",
    "\n",
    "        \n",
    "        '''\n",
    "        kaynin two cases : \n",
    "        \n",
    "             case1 :  we work on the outputs  ==> chosen\n",
    "             case2 :  we work on the final hidden state ==> discarted \n",
    "            \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        '''test_hidden = torch.flatten(hidden)\n",
    "        final_hidden = torch.unsqueeze(test_hidden, 0)'''\n",
    "        \n",
    "        \n",
    "        # can also be outputs.\n",
    "        \n",
    "        encoder_output = torch.mean(hidden , dim=0)  # Average the hidden vectors across all time steps. Shape: (hidden_size*2,) if bidirectional, else (hidden_size,)\n",
    "        \n",
    "        final_output = torch.unsqueeze(encoder_output, 0)\n",
    "               \n",
    "        \n",
    "        return final_output\n",
    "    \n",
    "    \n",
    "    def decode_word(self, encoding, character):\n",
    "\n",
    "        \n",
    "        '''\n",
    "        encoding : output of the encoder network. \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        seq = self.word_to_idx_seq(self.sow)\n",
    "        embedded_sow= self.embedding(self.word_to_idx_seq(character))   # starts with self.sow\n",
    "\n",
    "        input_size = embedded_sow.size(1)\n",
    "\n",
    "        hidden_size = encoding.size(1)\n",
    "                 \n",
    "        dec_out , dec_hidden = self.gru(embedded_sow,encoding)\n",
    "                \n",
    "        a = self.Linear(dec_out)\n",
    "        m = nn.Softmax(dim = 1)\n",
    "        \n",
    "        output = m(a)\n",
    "        \n",
    "        top1 = output.argmax(1)[0].item()\n",
    "        \n",
    "        key_list = list(self.char_index_dic.keys())\n",
    "        val_list = list(self.char_index_dic.values())\n",
    "\n",
    "        position = val_list.index(top1)\n",
    "        \n",
    "        result_char = key_list[position]\n",
    "\n",
    "        return result_char, dec_hidden , top1\n",
    "        \n",
    "        \n",
    "    def train_batch(self, batch):\n",
    "                \n",
    "        \n",
    "        for instance in batch: \n",
    "            '''\n",
    "            word = instance[0] : the word to extract root from\n",
    "            target_word = instance[1] : the root of the word\n",
    "            '''\n",
    "            word = instance[0]\n",
    "            target_root = instance[1]\n",
    "            \n",
    "            hidd = self.encode_word(word)\n",
    "            predicted_root = []\n",
    "            original_root_seq = self.word_to_idx_seq(target_root)\n",
    "            predicted_root_seq = []\n",
    "            res_char = target_root[0]\n",
    "            for char in target_root : \n",
    "                if len(predicted_root) == len(target_root):\n",
    "                    break\n",
    "                \n",
    "                \n",
    "                if random.random() < self.teacher_forcing_ratio : \n",
    "                    res_char = char\n",
    "                \n",
    "                res_char , hidd , idx = self.decode_word(hidd, res_char)\n",
    "                predicted_root_seq.append(idx)\n",
    "                test = hidd\n",
    "                predicted_root.append(res_char)\n",
    "\n",
    "            predicted_root = ''.join(predicted_root)\n",
    "\n",
    "            predicted_root_seq = torch.tensor(predicted_root_seq)\n",
    "\n",
    "            org = self.embedding(original_root_seq)\n",
    "            pred = self.embedding(predicted_root_seq)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(org, pred)\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            print('target root :', target_root)\n",
    "            print('predicted root : ', predicted_root)\n",
    "\n",
    "            print('the loss : ', loss.item())        \n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        batches = self.data_batches()\n",
    "        \n",
    "        for batch in batches : \n",
    "            \n",
    "            self.train_batch(batch)\n",
    "\n",
    "        #print(em1)\n",
    "        \n",
    "        \n",
    "\n",
    "mod = model(data_root, 100, 50, 100, 1, 0.9)\n",
    "mod.train()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b751a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3769b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c8969b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''l = 0\n",
    "for item in root_data:\n",
    "    if len(item[1]) > 3:\n",
    "        print(item[1])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc375018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ad99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def decode_word(self, word):\n",
    "        \n",
    "        hidden_size = 32  # Number of hidden units in the GRU\n",
    "        num_layers = 1  # Number of layers in the GRU\n",
    "        dropout = 0.2  # Dropout rate\n",
    "        \n",
    "        gru = nn.GRU(input_size=self.embedding_size, hidden_size=hidden_size, batch_first=True)\n",
    "        \n",
    "        e = self.encode_word(word)\n",
    "        # Initialize the hidden state h0\n",
    "        \n",
    "        h0 = torch.zeros(1, hidden_size)\n",
    "\n",
    "        \n",
    "        # Concatenate the encoder representation and start-of-word character\n",
    "        print(e.size())\n",
    "        print(self.hsowi.unsqueeze(1).size())\n",
    "        \n",
    "        x = torch.cat([self.hsowi.unsqueeze(0).float(), e.unsqueeze(0)], dim=1)\n",
    "\n",
    "        print(x.size())\n",
    "        print(h0.size())\n",
    "        \n",
    "        # Apply the GRU layer\n",
    "        out, h = gru(x)\n",
    "        \n",
    "        # Apply the output layer\n",
    "        out_l = nn.Linear(hidden_size, len(self.vocab))\n",
    "        \n",
    "        out = out_l(out)\n",
    "        \n",
    "        softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "        # Apply the softmax activation function\n",
    "        out = softmax(out)\n",
    "        \n",
    "        return out\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b8531",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class model:\n",
    "    def __init__(self, data, embedding_size):\n",
    "        self.sow = '$'  # the start of word character\n",
    "        self.eow = '£'  # the end of word character\n",
    "        self.data = data\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab = self.extract_dict()\n",
    "        self.char_index_dic = self.char_to_index()\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(self.vocab), embedding_dim=self.embedding_size)\n",
    "\n",
    "    def extract_dict(self):\n",
    "        dictt = []\n",
    "        for word in self.data:\n",
    "            for item in word:\n",
    "                tmp = set(item)\n",
    "                for k in tmp:\n",
    "                    if k not in dictt:\n",
    "                        dictt.append(k)\n",
    "        dictt.append(self.sow)\n",
    "        dictt.append(self.eow)\n",
    "        return dictt\n",
    "\n",
    "    def char_to_index(self):\n",
    "        char_to_idx_map = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        #print(char_to_idx_map)\n",
    "        return char_to_idx_map\n",
    "\n",
    "    def word_to_idx_seq(self, word):\n",
    "        word_char_idx_tensor = torch.tensor([self.char_index_dic[char] for char in word])\n",
    "        return word_char_idx_tensor\n",
    "\n",
    "    def encode_word(self, word):\n",
    "        \n",
    "        hidden_size = 32\n",
    "        bidirectional = True\n",
    "        num_layers = 1\n",
    "        \n",
    "        \n",
    "        gru = nn.GRU(input_size=self.embedding_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                     bidirectional=bidirectional, batch_first=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        word_seq = self.word_to_idx_seq(word)\n",
    "        embedded_vec = self.embedding(word_seq)\n",
    "        outputs, hidden = gru(embedded_vec)\n",
    "\n",
    "        h1 = torch.reshape(hidden[-1], (1, hidden_size))\n",
    "        h2 = torch.reshape(hidden[-2], (1, hidden_size))\n",
    "        hidden = torch.cat((h2, h1), 1)\n",
    "\n",
    "        encoder_output = torch.mean(hidden, dim=0)\n",
    "\n",
    "        return encoder_output\n",
    "\n",
    "    def decode_word(self, encoding):\n",
    "        input_size = encoding.shape[-1] + len(self.vocab)\n",
    "        hidden_size = 64\n",
    "        num_layers = 1\n",
    "\n",
    "        embedded_sow = self.embedding(torch.tensor(self.char_index_dic[self.sow]))\n",
    "        decoder_input = torch.cat((encoding, embedded_sow), dim=1)\n",
    "\n",
    "        gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "        outputs, hidden = gru(decoder_input.unsqueeze(0))\n",
    "        decoder_output = torch.mean(hidden, dim=0)\n",
    "\n",
    "        decoded = []\n",
    "        for i in range(self.max_word_length):\n",
    "            output = self.output_layer(decoder_output)\n",
    "            _, argmax = torch.max(output, dim=1)\n",
    "            decoded.append(argmax.item())\n",
    "            decoder_input = self.embedding(argmax)\n",
    "            decoder_input = torch.cat((decoder_input, encoding), dim=1)\n",
    "            decoder_output, hidden = gru(decoder_input.unsqueeze(0), hidden)\n",
    "            decoder_output = torch.mean(hidden, dim=0)\n",
    "\n",
    "            if argmax == self.eow_index:\n",
    "                break\n",
    "\n",
    "        decoded_word = ' '.join([self.index_char_dic[i] for i in decoded])\n",
    "        return decoded_word\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        #for instance in self.data:\n",
    "        word = self.data[0]\n",
    "        encoding = self.encode_word(word)\n",
    "        print(f'Encoding for {word}: {encoding}')\n",
    "\n",
    "        decoding = self.decode_word(encoding)\n",
    "        print(f'Decoding for {word}: {decoding}')\n",
    "        break\n",
    "\n",
    "mod = model(root_data, 10)\n",
    "mod.train()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# create an embedding layer with 50 input dimensions and 100 output dimensions\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=43, output_dim=100)\n",
    "sequence = [5 , 10 , 10, 20 , 7, 1]\n",
    "# get the embedding vectors for the sequence of character indices\n",
    "embedding_vectors = embedding_layer(np.array(sequence))\n",
    "embedding_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e94deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 32  # Number of hidden units in the GRU\n",
    "bidirectional = True  # Whether to use a bidirectional GRU or not\n",
    "num_layers = 1  # Number of layers in the GRU\n",
    "dropout = 0.2  # Dropout rate\n",
    "gru = nn.GRU(input_size=16, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee58704c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaad5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d5ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx_map = {char: idx for idx, char in enumerate(dic)}\n",
    "print(f\"Character to Index Mapping:\\n{char_to_idx_map}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_characters = torch.eye(n=len(dic))  # using the eye method for identity matrix\n",
    "print(f\"One hot encoded characters:\\n{ohe_characters}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb81318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_characters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c788cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_repr_a = ohe_characters[char_to_idx_map['ح']]\n",
    "print(f\"One hot encoded representation of 1:\\n{ohe_repr_a}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word, char_to_idx_map):\n",
    "    \n",
    "    word_char_idx_tensor = torch.tensor([char_to_idx_map[char] for char in word])\n",
    "    word_ohe_repr = ohe_characters[word_char_idx_tensor].T\n",
    "    \n",
    "    return word_ohe_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0bd253",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_char_idx_tensor = torch.tensor([char_to_idx_map[char] for char in \"مَرْحَبًا\"])\n",
    "print(word_char_idx_tensor.tolist())\n",
    "word_ohe_repr = ohe_characters[word_char_idx_tensor].T\n",
    "word_ohe_repr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b448e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "convolution_layer = nn.Conv1d(in_channels=43, out_channels=128, kernel_size=3, bias=False)\n",
    "conv_out = convolution_layer(word_ohe_repr.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of convolution output: {conv_out.shape}\")\n",
    "print(f\"Convolution Output:\\n{conv_out}\\n\")\n",
    "\n",
    "# adding an activation layer so as to add non linearity to the output\n",
    "activation_layer = nn.Tanh()\n",
    "activation_out = activation_layer(conv_out)\n",
    "activation_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd7ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of activation output: {activation_out.shape}\")\n",
    "print(f\"Activation Output:\\n{activation_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling Layer\n",
    "max_pooling_layer = nn.MaxPool1d(kernel_size=conv_out.shape[2])\n",
    "\n",
    "# finally here is our word embedding of size -> 256\n",
    "word_embedding = max_pooling_layer(activation_out).squeeze()\n",
    "\n",
    "print(f\"Final word embedding shape: {word_embedding.shape}\")\n",
    "print(f\"Final word embedding:\\n{word_embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed2aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_embedding():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2619f608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bea57e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e852de90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = ''.join(dic)\n",
    "vowel_pattern = r'[\\u064e\\u064b\\u0650\\u064d\\u064f\\u064c\\u064f\\u0652\\u0651]'\n",
    "vowels = re.findall(vowel_pattern, st)\n",
    "l = []\n",
    "for char in st :\n",
    "    if char not in vowels : \n",
    "        l.append(char)\n",
    "        \n",
    "l = ''.join(l)\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c27d12",
   "metadata": {},
   "source": [
    "### Let's create a data structure for each word of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ef987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = work_list[2]\n",
    "tmp.split(\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b25293",
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel_pattern = r'[\\u064b\\u064e\\u064f\\u0650\\u0651\\u0652]'\n",
    "word = \"مَرْحَبًا\"\n",
    "vowels = re.findall(vowel_pattern, word)\n",
    "print(f\"Vowels in '{word}': {vowels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f63a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''filtered_split_list = []\n",
    "for item in split_list : \n",
    "    temp_item_list = []\n",
    "    for subitem in item : \n",
    "        if 'لا توجد نتائج لتحليل هذه الكلمة' not in item : \n",
    "            temp_item_list.append(item)\n",
    "    filtered_split_list.append(temp_item_list)\n",
    "filtered_split_list[0]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a7be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Check the data type of each element in texts\n",
    "texts = pd.DataFrame(data_1[:, -1])\n",
    "texts = texts.iloc[:, 1:]\n",
    "for i, text in enumerate(texts):\n",
    "    if not isinstance(text, str):\n",
    "        print(f\"Element {i} is not a string: {text}\")'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
