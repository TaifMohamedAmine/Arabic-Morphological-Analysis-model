{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ea0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c89e9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 05:41:23.308434: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-17 05:41:23.308471: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense , Activation , Dropout, Embedding, SpatialDropout1D, LSTM , TextVectorization, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1643560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb1349c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50137a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'corpus_morphological_analysis'\n",
    "file_paths = []\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    file_paths.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c5f5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file_paths = file_paths[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2602e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "#i = 0\n",
    "for filepath in temp_file_paths :\n",
    "    #print(i)\n",
    "    with open(filepath, encoding='utf-8') as f :\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()  \n",
    "    text = soup.get_text()\n",
    "    content.append(text)\n",
    "    #i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d7aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "104b5ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35a75b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_list = []\n",
    "for item in content : \n",
    "    tmp = item.splitlines()\n",
    "    split_list.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3544ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_list = []\n",
    "for k in split_list :\n",
    "    l = [item for item in k if 'لا توجد نتائج لتحليل هذه الكلمة' not in item]\n",
    "    tmp_l = [item.replace(\"#\",'') for item in l]\n",
    "    work_list.append(tmp_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "429730b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_list = []\n",
    "for k in work_list :\n",
    "    tst = [item.split(':') for item in k]\n",
    "    final_list.append(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9135fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifie le prefixe, racine et le suffixe et les placent dans un dictionnaire\n",
    "def identify(word_l):\n",
    "    if len(word_l) < 4 :\n",
    "        return None\n",
    "    dictt = {}\n",
    "    dictt['word'] = word_l[0]\n",
    "    # le cas s'il existe un préfixe\n",
    "    if word_l[2] != '' and word_l[2] != ' ' :   \n",
    "        if word_l[4] not in word_l[0]: \n",
    "            if word_l[5] in word_l[0] and word_l[5] != '': \n",
    "                dictt['prefixe'] = word_l[2]\n",
    "                dictt['root'] = word_l[8]\n",
    "                dictt['suffixe'] = word_l[9]\n",
    "            elif word_l[3] in word_l[0] and word_l[3] != '':\n",
    "                dictt['prefixe'] = word_l[2]\n",
    "                dictt['root'] = word_l[3]\n",
    "                dictt['suffixe'] = ''\n",
    "        else :\n",
    "            dictt['prefixe'] = word_l[2]\n",
    "            dictt['root'] = word_l[7]\n",
    "            dictt['suffixe'] = word_l[8]\n",
    "    # s'il n'existe pas un préfixe\n",
    "    else : \n",
    "        if word_l[2] == '' : \n",
    "            dictt['prefixe'] = word_l[2]\n",
    "            dictt['root'] = word_l[6]\n",
    "            dictt['suffixe'] = word_l[7]\n",
    "        elif  word_l[2] == ' ' :\n",
    "            dictt['prefixe'] = ''\n",
    "            dictt['root'] = word_l[3]\n",
    "            dictt['suffixe'] = ''    \n",
    "    return dictt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807dc08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69a52da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtre la liste de mots en liste de dictionnaires\n",
    "def word_to_dict_list(wordlist):\n",
    "    dictlist = []\n",
    "    for k in wordlist : \n",
    "        dictlist.append(identify(k))\n",
    "    return dictlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "897b24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for k in final_list: \n",
    "    for j in k :\n",
    "        s = identify(j)\n",
    "        if s == None :\n",
    "            continue\n",
    "        final.append(identify(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cce4d9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'ستقام', 'prefixe': 'سَ', 'root': 'قوم', 'suffixe': ''}\n"
     ]
    }
   ],
   "source": [
    "print(final[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0fe335f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dic_to_list(listt):\n",
    "    L = []\n",
    "    for k in listt : \n",
    "        tmp = []\n",
    "        #print(k)\n",
    "        if len(k) == 4 : \n",
    "            tmp.append(k['word'])\n",
    "            tmp.append(k['prefixe'])\n",
    "            tmp.append(k['root'])\n",
    "            tmp.append(k['suffixe'])\n",
    "            L.append(tmp)\n",
    "    return L\n",
    "data = dic_to_list(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad205283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f81f4b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['مسقط', '', 'سقط', '']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_l = dic_to_list(final)\n",
    "final_l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22118e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimenting with our first neural network to predict the root of a word.\n",
    "\n",
    "data_1 = []\n",
    "\n",
    "for word in final_l:\n",
    "    tmp = []\n",
    "    word[0] = word[0].replace('+', ' ')\n",
    "    word[2] = word[2].replace('+', ' ')\n",
    "    tmp.append(word[0])\n",
    "    tmp.append(word[2])\n",
    "    data_1.append(tmp)\n",
    "    #print(tmp)\n",
    "#data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f8ebb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we extract our dictionnary from the our dataset \n",
    "def extract_dict(listt) :\n",
    "    dictt = []\n",
    "    for word in listt :\n",
    "        for item in word : \n",
    "            tmp = set(item)\n",
    "            for k in tmp : \n",
    "                if k not in dictt : \n",
    "                    dictt.append(k)\n",
    "    return dictt       \n",
    "dic = extract_dict(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247cf020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc451ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data = []\n",
    "for word in data : \n",
    "    tmp =[]\n",
    "    tmp.append(word[0])\n",
    "    tmp.append(word[2])\n",
    "    root_data.append(tmp)\n",
    "#root_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51d4a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encode :\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def code_sequence(self):\n",
    "        L = []\n",
    "        for word in self.data : \n",
    "            tmp_string = code_word(word)\n",
    "            L.append(tmp_string)\n",
    "        final_string = '#'.join(map(str,L))\n",
    "        return final_string \n",
    "    \n",
    "    def code_normal_text(self):\n",
    "        L = []\n",
    "        for word in self.data : \n",
    "            L.append(word[1])\n",
    "        final_string = ' '.join(map(str,L))\n",
    "        return final_string\n",
    "        \n",
    "final_str = encode(root_data).code_normal_text()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d08082",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3dbc01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90b091db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = []\n",
    "for item in root_data : \n",
    "    tmp = []\n",
    "    if len(item[1]) <= 3 and len(item[1]) != 0:\n",
    "        tmp.append('$'+item[0]+'£')\n",
    "        tmp.append('$'+item[1]+'£')\n",
    "        data_root.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d85cf4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32e29629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_padding(dat, padding_char):\n",
    "    #Le'ts create a padding character : \n",
    "    pad_char = padding_char\n",
    "    padded_data = []\n",
    "    \n",
    "    ls_words = []\n",
    "    ls_roots = []\n",
    "\n",
    "    for instance in dat : \n",
    "        ls_words.append(instance[0])\n",
    "        ls_roots.append(instance[1])\n",
    "    max_len_words = max([len(item) for item in ls_words])\n",
    "    max_len_roots = max([len(item) for item in ls_roots])\n",
    "    \n",
    "    for instance in dat: \n",
    "        tmp = []\n",
    "        word,root = instance[0], instance[1]\n",
    "        while(len(word) != max_len_words):\n",
    "            word += pad_char\n",
    "        tmp.append(word)\n",
    "        while(len(root) != max_len_roots):\n",
    "            root += pad_char\n",
    "        tmp.append(root)\n",
    "        padded_data.append(tmp)\n",
    "        \n",
    "    return padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11f4f468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['$مسقط£%%%%%%%%', '$سقط£'],\n",
       " ['$يقام£%%%%%%%%', '$قمي£'],\n",
       " ['$الرابع£%%%%%%', '$ربع£'],\n",
       " ['$والعشرين£%%%%', '$عشر£'],\n",
       " ['$من£%%%%%%%%%%', '$من£%'],\n",
       " ['$شهر£%%%%%%%%%', '$شهر£'],\n",
       " ['$فبراير£%%%%%%', '$رير£'],\n",
       " ['$المقبل£%%%%%%', '$قبل£'],\n",
       " ['$قصر£%%%%%%%%%', '$قصر£'],\n",
       " ['$البستان£%%%%%', '$بسس£']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_padding(data_root,'%')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7246f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    def train_batch(self, batch):\n",
    "                \n",
    "        self.optimizer.zero_grad()\n",
    "        for instance in batch: \n",
    "            '''\n",
    "            word = instance[0] : the word to extract root from\n",
    "            target_word = instance[1] : the root of the word\n",
    "            '''\n",
    "            word = instance[0]\n",
    "            target_root = instance[1]\n",
    "            \n",
    "            res1 = self.encode_word(word)\n",
    "            predicted_root = []\n",
    "            original_root_seq = self.word_to_idx_seq(target_root)\n",
    "            predicted_root_seq = []\n",
    "            res_char = target_root[0]\n",
    "            \n",
    "            i = 0\n",
    "            for char in target_root : \n",
    "                if len(predicted_root) == len(target_root):\n",
    "                    break\n",
    "                \n",
    "                \n",
    "                if random.random() < self.teacher_forcing_ratio and i!=0 : \n",
    "                    res_char = char\n",
    "                    i+= 1\n",
    "                \n",
    "                res_char , hidd , idx = self.decode_word(res1, res_char)\n",
    "                predicted_root_seq.append(idx)\n",
    "                test = hidd\n",
    "                predicted_root.append(res_char)\n",
    "\n",
    "            predicted_root = ''.join(predicted_root)\n",
    "\n",
    "            predicted_root_seq = torch.tensor(predicted_root_seq)\n",
    "\n",
    "            org = self.embedding(original_root_seq)\n",
    "            pred = self.embedding(predicted_root_seq)\n",
    "            \n",
    "            \n",
    "            loss = self.criterion(org, pred)\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            print('target root :', target_root)\n",
    "            print('predicted root : ', predicted_root)\n",
    "\n",
    "            print('the loss : ', loss.item())  \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return\n",
    "    \n",
    "            \n",
    "            \n",
    "        #print(em1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95150815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(self):\n",
    "    \n",
    "    #Le'ts create a padding for ouriinstances : \n",
    "    \n",
    "    pad_char = ''\n",
    "    padded_data = []\n",
    "    ls_words = []\n",
    "    ls_roots = []\n",
    "    for instance in self.data : \n",
    "        ls_words.append(instance[0])\n",
    "        ls_roots.append(instance[1])\n",
    "    max_len_words = max([len(item) for item in ls_words])\n",
    "    max_len_roots = max([len(item) for item in ls_roots])\n",
    "    \n",
    "    for instance in self.data: \n",
    "        tmp = []\n",
    "        word,root = instance[0], instance[1]\n",
    "        while(len(word) != max_len_words):\n",
    "            word += pad_char\n",
    "        tmp.append(word)\n",
    "        while(len(root) != max_len_roots):\n",
    "            root += pad_char\n",
    "        tmp.append(root)\n",
    "        padded_data.append(tmp)\n",
    "    \n",
    "\n",
    "    # let's create our vocab : \n",
    "    \n",
    "    vocab = []\n",
    "    for word in padded_data :\n",
    "        for item in word : \n",
    "            tmp = set(item)\n",
    "            for k in tmp : \n",
    "                if k not in vocab : \n",
    "                    vocab.append(k)\n",
    "    \n",
    "    \n",
    "    # Let's create our dictionnary with unique indexes\n",
    "    \n",
    "    char_to_idx_map = {char: idx for idx, char in enumerate(dictt)}\n",
    "    \n",
    "    # Let's now split our data to batches\n",
    "   \n",
    "    final_data = []\n",
    "    for instance in padded_data : \n",
    "        tmp = []\n",
    "        word = self.word_to_seq(instance[0])\n",
    "        root = self.word_to_seq(instance[1])\n",
    "        tmp.append(word)\n",
    "        tmp.append(root)\n",
    "        final_data.append(tmp)\n",
    "        \n",
    "    size= self.batch_size \n",
    "    batches = [final_data[i:i + size] for i in range(0, len(final_data), size)]\n",
    "    \n",
    "    return batches , vocab , char_to_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d695d5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c250db0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of the embbeded word batch : torch.Size([100, 14, 50])\n",
      "the final hidden layers size before concatenation :  torch.Size([2, 100, 39])\n",
      "the final hidden layers after concatenation :  torch.Size([1, 100, 78])\n",
      "the size of the root batch :  torch.Size([100, 5])\n",
      "before emb  torch.Size([100, 1])\n",
      "the size of the embbeded  0  word batch : torch.Size([100, 1, 50])\n",
      "the size of the decoder output right after the gru : torch.Size([100, 1, 78])\n",
      "after applying dense on the output : torch.Size([100, 1, 39])\n",
      "After using the softmax :  torch.Size([100, 1, 39])\n",
      "before emb  torch.Size([100, 1])\n",
      "the size of the embbeded  1  word batch : torch.Size([100, 1, 50])\n",
      "the size of the decoder output right after the gru : torch.Size([100, 1, 78])\n",
      "after applying dense on the output : torch.Size([100, 1, 39])\n",
      "After using the softmax :  torch.Size([100, 1, 39])\n",
      "before emb  torch.Size([100, 1])\n",
      "the size of the embbeded  2  word batch : torch.Size([100, 1, 50])\n",
      "the size of the decoder output right after the gru : torch.Size([100, 1, 78])\n",
      "after applying dense on the output : torch.Size([100, 1, 39])\n",
      "After using the softmax :  torch.Size([100, 1, 39])\n",
      "before emb  torch.Size([100, 1])\n",
      "the size of the embbeded  3  word batch : torch.Size([100, 1, 50])\n",
      "the size of the decoder output right after the gru : torch.Size([100, 1, 78])\n",
      "after applying dense on the output : torch.Size([100, 1, 39])\n",
      "After using the softmax :  torch.Size([100, 1, 39])\n",
      "before emb  torch.Size([100, 1])\n",
      "the size of the embbeded  4  word batch : torch.Size([100, 1, 50])\n",
      "the size of the decoder output right after the gru : torch.Size([100, 1, 78])\n",
      "after applying dense on the output : torch.Size([100, 1, 39])\n",
      "After using the softmax :  torch.Size([100, 1, 39])\n",
      "predicted :  torch.Size([100, 5]) original : torch.Size([100, 5])\n",
      "tensor([[ 3,  5,  2,  6,  1],\n",
      "        [ 3,  2,  0,  7,  1],\n",
      "        [ 3, 11, 10, 12,  1],\n",
      "        [ 3, 12, 13, 11,  1],\n",
      "        [ 3,  0, 14,  1,  4],\n",
      "        [ 3, 13, 16, 11,  1],\n",
      "        [ 3, 11,  7, 11,  1],\n",
      "        [ 3,  2, 10,  9,  1],\n",
      "        [ 3,  2, 18, 11,  1],\n",
      "        [ 3, 10,  5,  5,  1],\n",
      "        [ 3, 21,  0, 11,  1],\n",
      "        [ 3, 17, 11, 18,  1],\n",
      "        [ 3, 22,  0, 11,  1],\n",
      "        [ 3, 19, 24, 11,  1],\n",
      "        [ 3, 14, 25,  0,  1],\n",
      "        [ 3,  0, 24,  9,  1],\n",
      "        [ 3,  0,  7, 26,  1],\n",
      "        [ 3,  0, 24,  9,  1],\n",
      "        [ 3, 13, 11,  2,  1],\n",
      "        [ 3, 12, 15, 14,  1],\n",
      "        [ 3,  0, 12,  1,  4],\n",
      "        [ 3, 11, 27, 28,  1],\n",
      "        [ 3, 11, 15, 24,  1],\n",
      "        [ 3, 22,  0, 11,  1],\n",
      "        [ 3, 14,  0,  7,  1],\n",
      "        [ 3, 18, 26, 11,  1],\n",
      "        [ 3,  0, 11, 11,  1],\n",
      "        [ 3,  0, 26, 26,  1],\n",
      "        [ 3,  7, 15,  0,  1],\n",
      "        [ 3, 13,  0,  9,  1],\n",
      "        [ 3, 21,  0, 11,  1],\n",
      "        [ 3, 29, 19, 15,  1],\n",
      "        [ 3, 17, 11, 18,  1],\n",
      "        [ 3, 27, 10, 11,  1],\n",
      "        [ 3, 22,  0, 11,  1],\n",
      "        [ 3, 12,  9, 30,  1],\n",
      "        [ 3, 18, 15, 17,  1],\n",
      "        [ 3, 13,  0,  9,  1],\n",
      "        [ 3,  2, 18, 26,  1],\n",
      "        [ 3, 14, 15, 12,  1],\n",
      "        [ 3,  0, 11,  7,  1],\n",
      "        [ 3, 10, 16, 16,  1],\n",
      "        [ 3, 29, 15,  9,  1],\n",
      "        [ 3,  2,  6, 12,  1],\n",
      "        [ 3, 12,  9,  7,  1],\n",
      "        [ 3, 14,  0, 15,  1],\n",
      "        [ 3,  0, 12,  1,  4],\n",
      "        [ 3, 27, 10, 11,  1],\n",
      "        [ 3, 15, 17, 11,  1],\n",
      "        [ 3, 17, 11, 18,  1],\n",
      "        [ 3, 12,  0,  9,  1],\n",
      "        [ 3,  2, 26,  0,  1],\n",
      "        [ 3, 31,  9,  9,  1],\n",
      "        [ 3, 21,  0, 11,  1],\n",
      "        [ 3, 12,  0,  9,  1],\n",
      "        [ 3, 32,  6, 15,  1],\n",
      "        [ 3, 31,  9, 17,  1],\n",
      "        [ 3,  2,  6, 12,  1],\n",
      "        [ 3, 14, 17,  6,  1],\n",
      "        [ 3, 32, 28, 28,  1],\n",
      "        [ 3, 10, 22,  2,  1],\n",
      "        [ 3, 12, 14,  1,  4],\n",
      "        [ 3,  0,  8,  1,  4],\n",
      "        [ 3, 17,  7,  1,  4],\n",
      "        [ 3, 13, 11, 12,  1],\n",
      "        [ 3, 13, 11, 27,  1],\n",
      "        [ 3,  9, 32, 28,  1],\n",
      "        [ 3,  5,  7,  9,  1],\n",
      "        [ 3, 18, 29, 11,  1],\n",
      "        [ 3, 18,  9,  9,  1],\n",
      "        [ 3,  6,  2,  2,  1],\n",
      "        [ 3,  0,  7, 16,  1],\n",
      "        [ 3, 15, 18,  9,  1],\n",
      "        [ 3,  0,  6, 11,  1],\n",
      "        [ 3,  5,  7, 29,  1],\n",
      "        [ 3, 11,  0, 24,  1],\n",
      "        [ 3, 14,  0,  7,  1],\n",
      "        [ 3, 15, 11, 26,  1],\n",
      "        [ 3, 10, 13, 11,  1],\n",
      "        [ 3,  0,  8,  1,  4],\n",
      "        [ 3,  6, 11,  2,  1],\n",
      "        [ 3, 29, 26, 22,  1],\n",
      "        [ 3, 18, 26, 11,  1],\n",
      "        [ 3,  0,  7,  9,  1],\n",
      "        [ 3,  0,  8,  1,  4],\n",
      "        [ 3, 33,  0, 14,  1],\n",
      "        [ 3, 21,  0, 11,  1],\n",
      "        [ 3, 14, 26, 15,  1],\n",
      "        [ 3, 29, 15, 11,  1],\n",
      "        [ 3, 29, 15,  9,  1],\n",
      "        [ 3, 17, 33,  9,  1],\n",
      "        [ 3,  5, 10,  9,  1],\n",
      "        [ 3, 15, 18,  9,  1],\n",
      "        [ 3, 14,  5, 10,  1],\n",
      "        [ 3, 22,  0, 11,  1],\n",
      "        [ 3,  0,  8,  1,  4],\n",
      "        [ 3,  2, 15,  0,  1],\n",
      "        [ 3, 24,  9,  5,  1],\n",
      "        [ 3, 31, 18, 18,  1],\n",
      "        [ 3, 14,  2, 13,  1]]) tensor([[26,  0, 10, 26, 33],\n",
      "        [26,  0, 14, 34, 33],\n",
      "        [26,  0,  9,  8, 33],\n",
      "        [26,  0, 38,  1, 33],\n",
      "        [26, 35, 28, 35, 33],\n",
      "        [26, 21, 16, 33, 33],\n",
      "        [26,  0, 32, 28, 33],\n",
      "        [26,  0, 12, 33, 24],\n",
      "        [26,  0, 36, 26, 33],\n",
      "        [26, 21, 16, 33, 16],\n",
      "        [26,  0, 14, 20, 33],\n",
      "        [26, 15, 10, 27, 33],\n",
      "        [26, 32, 14, 20, 33],\n",
      "        [26,  0, 14, 36, 33],\n",
      "        [26,  0, 32,  0, 33],\n",
      "        [26, 35, 14, 36, 33],\n",
      "        [26, 35, 14, 34, 33],\n",
      "        [26, 30, 14, 36, 33],\n",
      "        [26, 21, 10, 26, 33],\n",
      "        [26,  0, 12, 30, 24],\n",
      "        [26, 35, 28, 33, 24],\n",
      "        [26,  0, 19, 12, 33],\n",
      "        [26,  0, 14, 36, 33],\n",
      "        [26, 32, 14, 20, 33],\n",
      "        [26,  0, 14, 20, 33],\n",
      "        [26, 20, 21, 20, 33],\n",
      "        [26, 30, 14, 28, 33],\n",
      "        [26, 32, 14, 20, 33],\n",
      "        [26,  0, 14, 36, 33],\n",
      "        [26, 21, 14, 28, 33],\n",
      "        [26,  0, 14, 20, 33],\n",
      "        [26, 12, 27, 28, 24],\n",
      "        [26, 15, 10, 27, 33],\n",
      "        [26,  9, 25, 18, 33],\n",
      "        [26, 34, 14, 20, 33],\n",
      "        [26,  0, 26,  8, 33],\n",
      "        [26, 20, 12, 30, 33],\n",
      "        [26, 21, 14, 20, 33],\n",
      "        [26,  0, 36, 26, 33],\n",
      "        [26,  0, 12, 30, 33],\n",
      "        [26, 30, 14, 28, 33],\n",
      "        [26, 21, 16, 33, 33],\n",
      "        [26, 12, 14, 35, 10],\n",
      "        [26,  0, 34, 12, 24],\n",
      "        [26,  0, 26,  8, 33],\n",
      "        [26,  0, 14, 20, 33],\n",
      "        [26, 35, 28, 33, 24],\n",
      "        [26,  9, 25, 18, 33],\n",
      "        [26, 25, 14, 36, 33],\n",
      "        [13, 15, 10, 27, 33],\n",
      "        [26,  0, 14, 20, 33],\n",
      "        [26,  0, 21, 20, 33],\n",
      "        [26, 15, 26,  8, 33],\n",
      "        [26,  0, 14, 20, 33],\n",
      "        [26,  0, 14, 34, 33],\n",
      "        [26, 12, 29, 12, 31],\n",
      "        [26, 15, 26, 18, 33],\n",
      "        [26,  0, 34, 12, 24],\n",
      "        [26,  0, 32, 28, 33],\n",
      "        [26, 12, 19, 19, 33],\n",
      "        [26, 21, 12, 12, 24],\n",
      "        [26,  0, 28, 35, 19],\n",
      "        [26, 35, 14, 20, 10],\n",
      "        [26, 15, 32, 28, 31],\n",
      "        [26, 21, 10, 26, 33],\n",
      "        [26, 21, 10, 26, 33],\n",
      "        [26, 15, 11, 29, 33],\n",
      "        [26,  0, 28, 11, 33],\n",
      "        [26, 20, 25, 12, 33],\n",
      "        [26, 20, 21, 33, 33],\n",
      "        [26,  0, 34, 12, 24],\n",
      "        [26, 30, 14, 34, 33],\n",
      "        [26, 25, 28, 35, 33],\n",
      "        [26, 30, 14, 20, 33],\n",
      "        [26,  0, 28, 11, 33],\n",
      "        [26,  0, 14, 20, 33],\n",
      "        [26,  0, 14, 20, 33],\n",
      "        [26, 25, 14, 29, 33],\n",
      "        [26, 21, 25, 18, 33],\n",
      "        [26, 30, 14, 20, 10],\n",
      "        [26,  0, 27, 12, 24],\n",
      "        [26, 12, 21, 20, 33],\n",
      "        [26, 20, 21, 20, 33],\n",
      "        [26, 35, 14, 34, 33],\n",
      "        [26, 30, 14, 20, 10],\n",
      "        [26, 15, 14, 34, 33],\n",
      "        [26,  0, 14, 20, 33],\n",
      "        [13,  0, 28, 35, 33],\n",
      "        [26, 12, 14, 35, 10],\n",
      "        [26, 12, 14, 35, 33],\n",
      "        [13, 15, 14, 36, 33],\n",
      "        [26,  0, 16, 33, 16],\n",
      "        [26, 25, 28, 35, 33],\n",
      "        [26,  0, 10, 26, 33],\n",
      "        [26, 32, 14, 20, 33],\n",
      "        [26, 30, 14, 20, 10],\n",
      "        [26,  0, 12, 30, 33],\n",
      "        [26,  3, 26,  8, 33],\n",
      "        [26, 15, 36, 35, 33],\n",
      "        [26,  0, 34, 12, 24]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15975/1469777602.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_15975/1469777602.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_roots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_roots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_roots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_roots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "# implementation of a paper : \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch \n",
    "import string\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "\n",
    "class model(nn.Module): \n",
    "    \n",
    "    def __init__(self, data, batch_size ,embedding_size, hidden_size, num_layers, teacher_forcing_ratio):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''\n",
    "        our input data are arabic words with their roots, with the hypothesis that each word has it's own root.\n",
    "        \n",
    "        (our dataset for this is named root_data)\n",
    "        '''\n",
    "        self.sow = '$' # the start of root character \n",
    "        self.eow = '£' # the end of root character\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.batches, self.vocab, self.char_index_dic = self.prepare_data()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = len(self.vocab)\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(num_embeddings = len(self.vocab), embedding_dim = self.embedding_size, padding_idx = self.char_index_dic['%']) \n",
    "        \n",
    "        \n",
    "        self.bigru = nn.GRU(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=self.num_layers, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(input_size= self.embedding_size ,hidden_size = self.hidden_size * 2, num_layers = self.num_layers, batch_first = True)\n",
    "        \n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index =self.char_index_dic['%'])\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.Linear = nn.Linear(self.hidden_size,len(self.vocab))\n",
    "        \n",
    "        self.optimizer = optim.Adam([*self.bigru.parameters(), *self.gru.parameters()], lr = 0.1)\n",
    "\n",
    "    \n",
    "    def prepare_data(self):\n",
    "    \n",
    "        #Le'ts create a padding for ouriinstances : \n",
    "\n",
    "        pad_char = '%'\n",
    "        padded_data = []\n",
    "        ls_words = []\n",
    "        ls_roots = []\n",
    "        for instance in self.data : \n",
    "            ls_words.append(instance[0])\n",
    "            ls_roots.append(instance[1])\n",
    "        \n",
    "        # Let's calculate the biggest length\n",
    "        max_len_words = max([len(item) for item in ls_words])\n",
    "        max_len_roots = max([len(item) for item in ls_roots])\n",
    "\n",
    "        # Now we pad the word until we reach the max length\n",
    "        for instance in self.data: \n",
    "            tmp = []\n",
    "            word,root = instance[0], instance[1]\n",
    "            while(len(word) != max_len_words):\n",
    "                word += pad_char\n",
    "            tmp.append(word)\n",
    "            while(len(root) != max_len_roots):\n",
    "                root += pad_char\n",
    "            tmp.append(root)\n",
    "            padded_data.append(tmp)\n",
    "\n",
    "        # let's create our vocab : \n",
    "\n",
    "        vocab = []\n",
    "        for word in padded_data :\n",
    "            for item in word : \n",
    "                tmp = set(item)\n",
    "                for k in tmp : \n",
    "                    if k not in vocab : \n",
    "                        vocab.append(k)\n",
    "\n",
    "        # Let's create our dictionnary with unique indexes\n",
    "\n",
    "        char_to_idx_map = {char: idx for idx, char in enumerate(vocab)}\n",
    "\n",
    "        # Let's now split our data to batches\n",
    "\n",
    "        final_data = []\n",
    "        for instance in padded_data : \n",
    "            tmp = []\n",
    "            word = [char_to_idx_map[char] for char in instance[0]]\n",
    "            root = [char_to_idx_map[char] for char in instance[1]]\n",
    "            tmp.append(word)\n",
    "            tmp.append(root)\n",
    "            final_data.append(tmp)\n",
    "\n",
    "        size= self.batch_size \n",
    "        batches = [final_data[i:i + size] for i in range(0, len(final_data), size)]\n",
    "        \n",
    "        return batches , vocab , char_to_idx_map\n",
    "    \n",
    "    \n",
    "    def word_to_seq(self, word):\n",
    "        '''\n",
    "        this function returns a sequence of the unique indexes for the given word \n",
    "        (sequence is tensor that can be changed using a .tolist() )\n",
    "        '''\n",
    "        word_char_idx_seq =[self.char_index_dic[char] for char in word]    \n",
    "        return word_char_idx_seq # word sequence\n",
    "    \n",
    "    \n",
    "    # Let's now construct our model : \n",
    "    \n",
    "    # we should think about character embeddings in order to create an embeded matrix for each word\n",
    "        \n",
    "    def train(self, num_epochs):\n",
    "        \n",
    "        epochs = list(range(num_epochs))\n",
    "        \n",
    "        final_batches = self.batches\n",
    "        \n",
    "        for epoch in epochs :\n",
    "\n",
    "            for batch in final_batches :\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                encoder_hidden, root_batch = self.encode(batch)\n",
    "                org_roots , predicted_roots  = self.decode(encoder_hidden, root_batch)\n",
    "                \n",
    "                print(org_roots, predicted_roots)\n",
    "                loss = self.criterion(org_roots, predicted_roots)\n",
    "                \n",
    "                print(loss)\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                \n",
    "            return\n",
    "                \n",
    "    \n",
    "    \n",
    "    \n",
    "    def encode(self, batch):    \n",
    "        '''\n",
    "        input : a batch of sequences of instances : [word_seq , root_seq] * batch_size\n",
    "                input_size : (input_size,2)\n",
    "        '''\n",
    "        word_batch = [] # list of words in the batch\n",
    "        root_batch = [] # list of roots in the batch\n",
    "        \n",
    "        for instance in batch : \n",
    "            word_batch.append(instance[0])\n",
    "            root_batch.append(instance[1])\n",
    "            \n",
    "        word_batch = torch.tensor(word_batch)\n",
    "        root_batch = torch.tensor(root_batch)\n",
    "        \n",
    "        # we create embedding of the word batch : \n",
    "        \n",
    "        embedded_word_batch = self.embedding(word_batch)\n",
    "        \n",
    "        print('the size of the embbeded word batch :', embedded_word_batch.size())\n",
    "        \n",
    "        outputs, hidden = self.bigru(embedded_word_batch) # we pass the emebedded vector through the bi-GRU \n",
    "        \n",
    "        print('the final hidden layers size before concatenation : ', hidden.size())\n",
    "        \n",
    "        test_hidden = torch.cat((hidden[0], hidden[1]), dim = 1)\n",
    "        \n",
    "        test_hidden = torch.unsqueeze(test_hidden, 0)\n",
    "        print('the final hidden layers after concatenation : ', test_hidden.size())\n",
    "        \n",
    "        \n",
    "        return test_hidden , root_batch\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def decode(self, encoding_hidden_layer , batch):\n",
    "        \n",
    "        '''\n",
    "        input : encoding_hidden_layer => corresponds to the concatenation of the final hidden layers \n",
    "                                        of the bidirectionnal gru in our encoder\n",
    "                \n",
    "                batch : subset of data that contains the roots of the words we encoded.\n",
    "                \n",
    "        output : we'll see :) \n",
    "        \n",
    "        '''\n",
    "\n",
    "        root_batch = batch\n",
    "        \n",
    "        hidden_layer = encoding_hidden_layer\n",
    "        \n",
    "        print('the size of the root batch : ', root_batch.size())\n",
    "\n",
    "        results = []\n",
    "        \n",
    "        char_batch = torch.unsqueeze(root_batch[:, 0], 1)\n",
    "        \n",
    "        for i in range(root_batch.size(1)): \n",
    "            \n",
    "            if random.random() < self.teacher_forcing_ratio : \n",
    "            \n",
    "                char_batch = torch.unsqueeze(root_batch[:, i], 1)\n",
    "                           \n",
    "            print('before emb ' , char_batch.size())\n",
    "        \n",
    "            embedded_char = self.embedding(char_batch)\n",
    "            \n",
    "            print('the size of the embbeded ', i, ' word batch :', embedded_char.size())\n",
    "            \n",
    "            decoder_output , decoder_final_hidden = self.gru(embedded_char, hidden_layer)\n",
    "            \n",
    "            hidden_layer = decoder_final_hidden\n",
    "            \n",
    "            print('the size of the decoder output right after the gru :', decoder_output.size() )\n",
    "        \n",
    "            Dense = nn.Linear(decoder_output.size(2), len(self.vocab))\n",
    "        \n",
    "            out_layer = Dense(decoder_output)\n",
    "                \n",
    "            print('after applying dense on the output :' , out_layer.size())\n",
    "        \n",
    "            softmax_layer = nn.Softmax(dim = 2)\n",
    "\n",
    "            final_res = softmax_layer(out_layer)\n",
    "\n",
    "            print('After using the softmax : ', final_res.size())\n",
    "\n",
    "            predicted_sequences = []\n",
    "\n",
    "            for instance in final_res : \n",
    "\n",
    "                best_char_indexes = [torch.argmax(item).item() for item in instance]\n",
    "\n",
    "                predicted_sequences.append(best_char_indexes)\n",
    "            \n",
    "            results.append(predicted_sequences)\n",
    "            \n",
    "            #print(torch.tensor(predicted_sequences).size(), '*'*100)\n",
    "            \n",
    "            char_batch = torch.tensor(predicted_sequences)\n",
    "        \n",
    "        results = torch.tensor(results)\n",
    "         \n",
    "        predicted_roots = []\n",
    "        \n",
    "        res = torch.squeeze(results, 2)\n",
    "            \n",
    "        for k in range(res.size(1)): \n",
    "            \n",
    "            predicted_roots.append(res[:, k].tolist())\n",
    "        \n",
    "        predicted_roots = torch.tensor(predicted_roots)\n",
    "        \n",
    "        print('predicted : ', predicted_roots.size(), 'original :', root_batch.size())\n",
    "        \n",
    "        return root_batch , predicted_roots\n",
    "                    \n",
    "        \n",
    "        '''key_list = list(self.char_index_dic.keys())\n",
    "        val_list = list(self.char_index_dic.values())\n",
    "\n",
    "        pos_all = []\n",
    "\n",
    "        for seq in predicted_sequences : \n",
    "\n",
    "            positions = [val_list.index(item) for item in seq]\n",
    "            result_chars = [key_list[pos] for pos in positions]\n",
    "            char = ''.join(result_chars)\n",
    "\n",
    "            pos_all.append(char)\n",
    "\n",
    "        print(pos_all)'''\n",
    "                        \n",
    "            \n",
    "            \n",
    "        '''embedded_root_batch = self.embedding(root_batch) \n",
    "        \n",
    "        print('the size of the embbeded root batch :', embedded_root_batch.size())\n",
    "            \n",
    "        print(embedded_root_batch.size(), encoding_hidden_layer.size())\n",
    "        \n",
    "        decoder_output , decoder_final_hidden = self.gru(embedded_root_batch ,encoding_hidden_layer)\n",
    "        \n",
    "        print('the size of the decoder output right after the gru :', decoder_output.size() )\n",
    "        \n",
    "        Dense = nn.Linear(decoder_output.size(2), len(self.vocab))\n",
    "        \n",
    "        out_layer = Dense(decoder_output)\n",
    "        \n",
    "        print('after applying dense on the output :' , out_layer.size())\n",
    "        \n",
    "        softmax_layer = nn.Softmax(dim = 2)\n",
    "        \n",
    "        final_res = softmax_layer(out_layer)\n",
    "        \n",
    "        print('After using the softmax : ', final_res.size())\n",
    "        \n",
    "        predicted_sequences = []\n",
    "        \n",
    "        for instance in final_res : \n",
    "            \n",
    "            best_char_indexes = [torch.argmax(item).item() for item in instance]\n",
    "            \n",
    "            predicted_sequences.append(best_char_indexes)\n",
    "            \n",
    "            \n",
    "        key_list = list(self.char_index_dic.keys())\n",
    "        val_list = list(self.char_index_dic.values())\n",
    "        \n",
    "        pos_all = []\n",
    "        \n",
    "        for seq in predicted_sequences : \n",
    "            \n",
    "            positions = [val_list.index(item) for item in seq]\n",
    "            result_chars = [key_list[pos] for pos in positions]\n",
    "            char = ''.join(result_chars)\n",
    "            \n",
    "            pos_all.append(char)\n",
    "        print(pos_all)\n",
    "        \n",
    "        \n",
    "            '''\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "mod = model(data_root, 100, 50, 100, 1, 0.5)\n",
    "mod.train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c9388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = nn.CrossEntropyLoss()\n",
    "a = torch.tensor([1.0 ,2.0 ,3.0 , 4.0])\n",
    "b = torch.tensor([1.0 ,2.0 ,3.0, 4.0])\n",
    "loss = crit(b, a)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849be848",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1 ,2 ,3 , 4]).to(torch.float)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c16f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5231259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of a paper : \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch \n",
    "import string\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "\n",
    "class model(nn.Module): \n",
    "    \n",
    "    def __init__(self, data, batch_size ,embedding_size, hidden_size, num_layers, teacher_forcing_ratio):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''\n",
    "        our input data are arabic words with their roots, with the hypothesis that each word has it's own root.\n",
    "        \n",
    "        (our dataset for this is named root_data)\n",
    "        '''\n",
    "        self.sow = '$' # the start of root character \n",
    "        self.eow = '£' # the end of root character\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab = self.extract_dict()\n",
    "        self.char_index_dic = self.char_to_index()\n",
    "        print(self.char_index_dic)\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(num_embeddings = len(self.vocab), embedding_dim = self.embedding_size) \n",
    "        self.bigru = nn.GRU(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=self.num_layers, bidirectional=True)\n",
    "        self.gru = nn.GRU(input_size= self.embedding_size ,hidden_size = self.hidden_size, num_layers = self.num_layers)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.Linear = nn.Linear(self.hidden_size,len(self.vocab))\n",
    "        self.optimizer = optim.Adam([*self.bigru.parameters(), *self.gru.parameters(), *self.Linear.parameters()], lr = 0.1)\n",
    "\n",
    "    \n",
    "    def extract_dict(self) :\n",
    "        '''\n",
    "        this function extracts all the unique characters from the given dataset\n",
    "        '''\n",
    "        dictt = []\n",
    "        for word in self.data :\n",
    "            for item in word : \n",
    "                tmp = set(item)\n",
    "                for k in tmp : \n",
    "                    if k not in dictt : \n",
    "                        dictt.append(k)\n",
    "        return dictt \n",
    "    \n",
    "    \n",
    "    # Let's now construct our model : \n",
    "    \n",
    "    # we should think about character embeddings in order to create an embeded matrix for each word\n",
    "    \n",
    "    def char_to_index(self):\n",
    "        '''\n",
    "        this function creates unique indexes of each character\n",
    "        '''\n",
    "        char_to_idx_map = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        return char_to_idx_map  \n",
    "    \n",
    "    def data_batches(self): \n",
    "        size = self.batch_size\n",
    "        \n",
    "        batches = [self.data[i:i + size] for i in range(0, len(self.data), size)]\n",
    "        \n",
    "        return batches\n",
    "        \n",
    "    \n",
    "    def word_to_idx_seq(self, word):\n",
    "        '''\n",
    "        this function returns a sequence of the unique indexes for the given word \n",
    "        (sequence is tensor that can be changed using a .tolist() )\n",
    "        '''\n",
    "        word_char_idx_tensor = torch.tensor([self.char_index_dic[char] for char in word])        \n",
    "        return word_char_idx_tensor # word sequence\n",
    "    \n",
    "    def word_to_seq(self, word):\n",
    "        '''\n",
    "        this function returns a sequence of the unique indexes for the given word \n",
    "        (sequence is tensor that can be changed using a .tolist() )\n",
    "        '''\n",
    "        word_char_idx_tensor =[self.char_index_dic[char] for char in word]    \n",
    "        return word_char_idx_tensor # word sequence\n",
    "    \n",
    "    \n",
    "    def encode_word(self, word):\n",
    "        \n",
    "            \n",
    "        # we create embedding of the sequence : \n",
    "        word_seq = self.word_to_idx_seq(word)\n",
    "        embedded_vec = self.embedding(word_seq)\n",
    "        \n",
    "        outputs, hidden = self.bigru(embedded_vec) # we pass the emebedded vector through the bi-GRU \n",
    "\n",
    "        \n",
    "        '''\n",
    "        kaynin two cases : \n",
    "        \n",
    "             case1 :  we work on the outputs  ==> chosen\n",
    "             case2 :  we work on the final hidden state ==> discarted \n",
    "            \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        '''test_hidden = torch.flatten(hidden)\n",
    "        final_hidden = torch.unsqueeze(test_hidden, 0)'''\n",
    "        \n",
    "        \n",
    "        # can also be outputs.\n",
    "        \n",
    "        encoder_output = torch.mean(hidden , dim=0)  # Average the hidden vectors across all time steps. Shape: (hidden_size*2,) if bidirectional, else (hidden_size,)\n",
    "        \n",
    "        final_output = torch.unsqueeze(encoder_output, 0)\n",
    "               \n",
    "        \n",
    "        return final_output\n",
    "    \n",
    "    \n",
    "    def decode_word(self, encoding, character):\n",
    "\n",
    "        \n",
    "        '''\n",
    "        encoding : output of the encoder network. \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        seq = self.word_to_idx_seq(self.sow)\n",
    "        embedded_sow= self.embedding(self.word_to_idx_seq(character))   # starts with self.sow\n",
    "\n",
    "        input_size = embedded_sow.size(1)\n",
    "\n",
    "        hidden_size = encoding.size(1)\n",
    "                 \n",
    "        dec_out , dec_hidden = self.gru(embedded_sow,encoding)\n",
    "                \n",
    "        a = self.Linear(dec_out)\n",
    "        m = nn.Softmax(dim = 1)\n",
    "        \n",
    "        output = m(a)\n",
    "        \n",
    "        top1 = output.argmax(1)[0].item()\n",
    "        \n",
    "        key_list = list(self.char_index_dic.keys())\n",
    "        val_list = list(self.char_index_dic.values())\n",
    "\n",
    "        position = val_list.index(top1)\n",
    "        \n",
    "        result_char = key_list[position]\n",
    "\n",
    "        return result_char, dec_hidden , top1\n",
    "        \n",
    "        \n",
    "    def train_batch(self, batch):\n",
    "                \n",
    "        \n",
    "        for instance in batch: \n",
    "            '''\n",
    "            word = instance[0] : the word to extract root from\n",
    "            target_word = instance[1] : the root of the word\n",
    "            '''\n",
    "            word = instance[0]\n",
    "            target_root = instance[1]\n",
    "            \n",
    "            res1 = self.encode_word(word)\n",
    "            predicted_root = []\n",
    "            original_root_seq = self.word_to_idx_seq(target_root)\n",
    "            predicted_root_seq = []\n",
    "            res_char = target_root[0]\n",
    "            for char in target_root : \n",
    "                if len(predicted_root) == len(target_root):\n",
    "                    break\n",
    "                \n",
    "                \n",
    "                if random.random() < self.teacher_forcing_ratio : \n",
    "                    res_char = char\n",
    "                \n",
    "                res_char , hidd , idx = self.decode_word(res1, res_char)\n",
    "                predicted_root_seq.append(idx)\n",
    "                test = hidd\n",
    "                predicted_root.append(res_char)\n",
    "\n",
    "            predicted_root = ''.join(predicted_root)\n",
    "\n",
    "            predicted_root_seq = torch.tensor(predicted_root_seq)\n",
    "\n",
    "            org = self.embedding(original_root_seq)\n",
    "            pred = self.embedding(predicted_root_seq)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(org, pred)\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            print('target root :', target_root)\n",
    "            print('predicted root : ', predicted_root)\n",
    "\n",
    "            print('the loss : ', loss.item())        \n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        batches = self.data_batches()\n",
    "        \n",
    "        for batch in batches : \n",
    "            \n",
    "            self.train_batch(batch)\n",
    "\n",
    "        #print(em1)\n",
    "        \n",
    "        \n",
    "\n",
    "mod = model(data_root, 100, 50, 100, 1, 0.9)\n",
    "mod.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b751a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3769b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c8969b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''l = 0\n",
    "for item in root_data:\n",
    "    if len(item[1]) > 3:\n",
    "        print(item[1])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc375018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ad99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def decode_word(self, word):\n",
    "        \n",
    "        hidden_size = 32  # Number of hidden units in the GRU\n",
    "        num_layers = 1  # Number of layers in the GRU\n",
    "        dropout = 0.2  # Dropout rate\n",
    "        \n",
    "        gru = nn.GRU(input_size=self.embedding_size, hidden_size=hidden_size, batch_first=True)\n",
    "        \n",
    "        e = self.encode_word(word)\n",
    "        # Initialize the hidden state h0\n",
    "        \n",
    "        h0 = torch.zeros(1, hidden_size)\n",
    "\n",
    "        \n",
    "        # Concatenate the encoder representation and start-of-word character\n",
    "        print(e.size())\n",
    "        print(self.hsowi.unsqueeze(1).size())\n",
    "        \n",
    "        x = torch.cat([self.hsowi.unsqueeze(0).float(), e.unsqueeze(0)], dim=1)\n",
    "\n",
    "        print(x.size())\n",
    "        print(h0.size())\n",
    "        \n",
    "        # Apply the GRU layer\n",
    "        out, h = gru(x)\n",
    "        \n",
    "        # Apply the output layer\n",
    "        out_l = nn.Linear(hidden_size, len(self.vocab))\n",
    "        \n",
    "        out = out_l(out)\n",
    "        \n",
    "        softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "        # Apply the softmax activation function\n",
    "        out = softmax(out)\n",
    "        \n",
    "        return out\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b8531",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class model:\n",
    "    def __init__(self, data, embedding_size):\n",
    "        self.sow = '$'  # the start of word character\n",
    "        self.eow = '£'  # the end of word character\n",
    "        self.data = data\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab = self.extract_dict()\n",
    "        self.char_index_dic = self.char_to_index()\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(self.vocab), embedding_dim=self.embedding_size)\n",
    "\n",
    "    def extract_dict(self):\n",
    "        dictt = []\n",
    "        for word in self.data:\n",
    "            for item in word:\n",
    "                tmp = set(item)\n",
    "                for k in tmp:\n",
    "                    if k not in dictt:\n",
    "                        dictt.append(k)\n",
    "        dictt.append(self.sow)\n",
    "        dictt.append(self.eow)\n",
    "        return dictt\n",
    "\n",
    "    def char_to_index(self):\n",
    "        char_to_idx_map = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        #print(char_to_idx_map)\n",
    "        return char_to_idx_map\n",
    "\n",
    "    def word_to_idx_seq(self, word):\n",
    "        word_char_idx_tensor = torch.tensor([self.char_index_dic[char] for char in word])\n",
    "        return word_char_idx_tensor\n",
    "\n",
    "    def encode_word(self, word):\n",
    "        \n",
    "        hidden_size = 32\n",
    "        bidirectional = True\n",
    "        num_layers = 1\n",
    "        \n",
    "        \n",
    "        gru = nn.GRU(input_size=self.embedding_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                     bidirectional=bidirectional, batch_first=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        word_seq = self.word_to_idx_seq(word)\n",
    "        embedded_vec = self.embedding(word_seq)\n",
    "        outputs, hidden = gru(embedded_vec)\n",
    "\n",
    "        h1 = torch.reshape(hidden[-1], (1, hidden_size))\n",
    "        h2 = torch.reshape(hidden[-2], (1, hidden_size))\n",
    "        hidden = torch.cat((h2, h1), 1)\n",
    "\n",
    "        encoder_output = torch.mean(hidden, dim=0)\n",
    "\n",
    "        return encoder_output\n",
    "\n",
    "    def decode_word(self, encoding):\n",
    "        input_size = encoding.shape[-1] + len(self.vocab)\n",
    "        hidden_size = 64\n",
    "        num_layers = 1\n",
    "\n",
    "        embedded_sow = self.embedding(torch.tensor(self.char_index_dic[self.sow]))\n",
    "        decoder_input = torch.cat((encoding, embedded_sow), dim=1)\n",
    "\n",
    "        gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "        outputs, hidden = gru(decoder_input.unsqueeze(0))\n",
    "        decoder_output = torch.mean(hidden, dim=0)\n",
    "\n",
    "        decoded = []\n",
    "        for i in range(self.max_word_length):\n",
    "            output = self.output_layer(decoder_output)\n",
    "            _, argmax = torch.max(output, dim=1)\n",
    "            decoded.append(argmax.item())\n",
    "            decoder_input = self.embedding(argmax)\n",
    "            decoder_input = torch.cat((decoder_input, encoding), dim=1)\n",
    "            decoder_output, hidden = gru(decoder_input.unsqueeze(0), hidden)\n",
    "            decoder_output = torch.mean(hidden, dim=0)\n",
    "\n",
    "            if argmax == self.eow_index:\n",
    "                break\n",
    "\n",
    "        decoded_word = ' '.join([self.index_char_dic[i] for i in decoded])\n",
    "        return decoded_word\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        #for instance in self.data:\n",
    "        word = self.data[0]\n",
    "        encoding = self.encode_word(word)\n",
    "        print(f'Encoding for {word}: {encoding}')\n",
    "\n",
    "        decoding = self.decode_word(encoding)\n",
    "        print(f'Decoding for {word}: {decoding}')\n",
    "        break\n",
    "\n",
    "mod = model(root_data, 10)\n",
    "mod.train()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# create an embedding layer with 50 input dimensions and 100 output dimensions\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=43, output_dim=100)\n",
    "sequence = [5 , 10 , 10, 20 , 7, 1]\n",
    "# get the embedding vectors for the sequence of character indices\n",
    "embedding_vectors = embedding_layer(np.array(sequence))\n",
    "embedding_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e94deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 32  # Number of hidden units in the GRU\n",
    "bidirectional = True  # Whether to use a bidirectional GRU or not\n",
    "num_layers = 1  # Number of layers in the GRU\n",
    "dropout = 0.2  # Dropout rate\n",
    "gru = nn.GRU(input_size=16, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee58704c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaad5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d5ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx_map = {char: idx for idx, char in enumerate(dic)}\n",
    "print(f\"Character to Index Mapping:\\n{char_to_idx_map}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_characters = torch.eye(n=len(dic))  # using the eye method for identity matrix\n",
    "print(f\"One hot encoded characters:\\n{ohe_characters}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb81318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_characters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c788cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_repr_a = ohe_characters[char_to_idx_map['ح']]\n",
    "print(f\"One hot encoded representation of 1:\\n{ohe_repr_a}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word, char_to_idx_map):\n",
    "    \n",
    "    word_char_idx_tensor = torch.tensor([char_to_idx_map[char] for char in word])\n",
    "    word_ohe_repr = ohe_characters[word_char_idx_tensor].T\n",
    "    \n",
    "    return word_ohe_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0bd253",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_char_idx_tensor = torch.tensor([char_to_idx_map[char] for char in \"مَرْحَبًا\"])\n",
    "print(word_char_idx_tensor.tolist())\n",
    "word_ohe_repr = ohe_characters[word_char_idx_tensor].T\n",
    "word_ohe_repr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b448e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "convolution_layer = nn.Conv1d(in_channels=43, out_channels=128, kernel_size=3, bias=False)\n",
    "conv_out = convolution_layer(word_ohe_repr.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of convolution output: {conv_out.shape}\")\n",
    "print(f\"Convolution Output:\\n{conv_out}\\n\")\n",
    "\n",
    "# adding an activation layer so as to add non linearity to the output\n",
    "activation_layer = nn.Tanh()\n",
    "activation_out = activation_layer(conv_out)\n",
    "activation_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd7ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of activation output: {activation_out.shape}\")\n",
    "print(f\"Activation Output:\\n{activation_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling Layer\n",
    "max_pooling_layer = nn.MaxPool1d(kernel_size=conv_out.shape[2])\n",
    "\n",
    "# finally here is our word embedding of size -> 256\n",
    "word_embedding = max_pooling_layer(activation_out).squeeze()\n",
    "\n",
    "print(f\"Final word embedding shape: {word_embedding.shape}\")\n",
    "print(f\"Final word embedding:\\n{word_embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed2aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_embedding():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2619f608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bea57e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e852de90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = ''.join(dic)\n",
    "vowel_pattern = r'[\\u064e\\u064b\\u0650\\u064d\\u064f\\u064c\\u064f\\u0652\\u0651]'\n",
    "vowels = re.findall(vowel_pattern, st)\n",
    "l = []\n",
    "for char in st :\n",
    "    if char not in vowels : \n",
    "        l.append(char)\n",
    "        \n",
    "l = ''.join(l)\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c27d12",
   "metadata": {},
   "source": [
    "### Let's create a data structure for each word of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ef987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = work_list[2]\n",
    "tmp.split(\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b25293",
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel_pattern = r'[\\u064b\\u064e\\u064f\\u0650\\u0651\\u0652]'\n",
    "word = \"مَرْحَبًا\"\n",
    "vowels = re.findall(vowel_pattern, word)\n",
    "print(f\"Vowels in '{word}': {vowels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f63a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''filtered_split_list = []\n",
    "for item in split_list : \n",
    "    temp_item_list = []\n",
    "    for subitem in item : \n",
    "        if 'لا توجد نتائج لتحليل هذه الكلمة' not in item : \n",
    "            temp_item_list.append(item)\n",
    "    filtered_split_list.append(temp_item_list)\n",
    "filtered_split_list[0]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a7be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Check the data type of each element in texts\n",
    "texts = pd.DataFrame(data_1[:, -1])\n",
    "texts = texts.iloc[:, 1:]\n",
    "for i, text in enumerate(texts):\n",
    "    if not isinstance(text, str):\n",
    "        print(f\"Element {i} is not a string: {text}\")'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
